{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug Code\n",
    "```\n",
    "def debug(self,X):\n",
    "        print('---------------1--------------')\n",
    "        X1 = self.layer1(X)\n",
    "        print(self.layer1.__class__.__name__, \"output shape: \\t\", X1.shape)\n",
    "        print('---------------2--------------')\n",
    "        X2 = self.layer2(X1)\n",
    "        print(self.layer2.__class__.__name__, \"output shape: \\t\", X2.shape)\n",
    "        print('----------------3-------------')\n",
    "        X3 = self.layer3(X2)\n",
    "        print(self.layer3.__class__.__name__, \"output shape: \\t\", X3.shape)\n",
    "        print('---------------4--------------')\n",
    "        X4 = self.layer4(X3)\n",
    "        print(self.layer4.__class__.__name__, \"output shape: \\t\", X4.shape)\n",
    "        print('----------------5-------------')\n",
    "        X5 = self.layer5(X4)\n",
    "        print(self.layer5.__class__.__name__, \"output shape: \\t\", X5.shape)\n",
    "        print('----------------6-------------')\n",
    "        X5_with_upscale = self.crop_and_cat(X4,self.upCov1(X5),crop_by=4)\n",
    "        X6 = self.layer6(X5_with_upscale)  \n",
    "        print(self.layer6.__class__.__name__, \"output shape: \\t\", X6.shape)\n",
    "        print('-----------------7------------')\n",
    "        X6_with_upscale = self.crop_and_cat(X3,self.upCov2(X6),crop_by=16)\n",
    "        X7 = self.layer7(X6_with_upscale)  \n",
    "        print(self.layer7.__class__.__name__, \"output shape: \\t\", X7.shape)\n",
    "        print('-----------------8------------')\n",
    "        X7_with_upscale = self.crop_and_cat(X2,self.upCov3(X7),crop_by=40)\n",
    "        X8 = self.layer8(X7_with_upscale)  \n",
    "        print(self.layer8.__class__.__name__, \"output shape: \\t\", X8.shape)\n",
    "        print('-----------------9------------')\n",
    "        X8_with_upscale = self.crop_and_cat(X1,self.upCov4(X8),crop_by=88)\n",
    "        X9 = self.layer9(X8_with_upscale)  \n",
    "        print(self.layer9.__class__.__name__, \"output shape: \\t\", X9.shape)\n",
    "        return X9\n",
    "```\n",
    "Experimental code\n",
    "```\n",
    "# input = torch.randn(1, 16, 12, 12)\n",
    "# downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "# upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "# h = downsample(input)\n",
    "# print(h.size())\n",
    "# torch.Size([1, 16, 6, 6])\n",
    "# output = upsample(h, output_size=input.size())\n",
    "# print(output.size())\n",
    "\n",
    "input = torch.randn(1, 512, 64, 64)\n",
    "downsample = nn.Sequential(\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(512,1024,kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(1024,1024,kernel_size=3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "upsample = nn.Sequential(\n",
    "    nn.ConvTranspose2d(1024, 512, 2,stride=2)\n",
    ")\n",
    "h = downsample(input)\n",
    "print(h.size())\n",
    "output = upsample(h)\n",
    "# output = upsample(h, output_size=(-1,512,32,32))\n",
    "print(output.size())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (2 x 2). Kernel size: (3 x 3). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 119\u001b[0m\n\u001b[1;32m    117\u001b[0m model \u001b[38;5;241m=\u001b[39m UNetModel(output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    118\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 119\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 105\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    103\u001b[0m X2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(X1)\n\u001b[1;32m    104\u001b[0m X3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(X2)\n\u001b[0;32m--> 105\u001b[0m X4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m X5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer5(X4)\n\u001b[1;32m    107\u001b[0m X5_with_upscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_and_cat(X4,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupCov1(X5),crop_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (2 x 2). Kernel size: (3 x 3). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "class UNetModel(nn.Module):\n",
    "\n",
    "    def __init__(self,output_channels=3):\n",
    "        super().__init__()\n",
    "        self.layer1 = self.defineFirstLayer()\n",
    "        self.layer2 = self.defineSecondLayer()\n",
    "        self.layer3 = self.defineThirdLayer()\n",
    "        self.layer4 = self.defineFourthLayer()\n",
    "        self.layer5 = self.defineFifthLayer()\n",
    "        self.upCov1 = nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2)\n",
    "        self.layer6 = self.defineSixthLayer()\n",
    "        self.upCov2 = nn.ConvTranspose2d(512,256,kernel_size=2,stride=2)\n",
    "        self.layer7 = self.defineSeventhLayer()\n",
    "        self.upCov3 = nn.ConvTranspose2d(256,128,kernel_size=2,stride=2)\n",
    "        self.layer8 = self.defineEighthLayer()\n",
    "        self.upCov4 = nn.ConvTranspose2d(128,64,kernel_size=2,stride=2)\n",
    "        self.layer9 = self.defineNinthLayer(output_channels)\n",
    "    \n",
    "    def defineFirstLayer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(3,64,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def defineSecondLayer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(64,128,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def defineThirdLayer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(128,256,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256,kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def defineFourthLayer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(256,512,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,512,kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def defineFifthLayer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(512,1024,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024,1024,kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def defineSixthLayer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(1024,512,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,512,kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def defineSeventhLayer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(512,256,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256,kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def defineEighthLayer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(256,128,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def defineNinthLayer(self,output_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(128,64,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,output_channels,kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def crop_and_cat(self,cropTensor,catTensor,crop_by=4):\n",
    "        cropTensor = cropTensor[:,:,crop_by:-crop_by,crop_by:-crop_by]  \n",
    "        return torch.cat((cropTensor, catTensor),dim=1)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X1 = self.layer1(X)\n",
    "        X2 = self.layer2(X1)\n",
    "        X3 = self.layer3(X2)\n",
    "        X4 = self.layer4(X3)\n",
    "        X5 = self.layer5(X4)\n",
    "        X5_with_upscale = self.crop_and_cat(X4,self.upCov1(X5),crop_by=4)\n",
    "        X6 = self.layer6(X5_with_upscale)  \n",
    "        X6_with_upscale = self.crop_and_cat(X3,self.upCov2(X6),crop_by=16)\n",
    "        X7 = self.layer7(X6_with_upscale)  \n",
    "        X7_with_upscale = self.crop_and_cat(X2,self.upCov3(X7),crop_by=40)\n",
    "        X8 = self.layer8(X7_with_upscale)  \n",
    "        X8_with_upscale = self.crop_and_cat(X1,self.upCov4(X8),crop_by=88)\n",
    "        X9 = self.layer9(X8_with_upscale)  \n",
    "        return X9\n",
    "\n",
    "model = UNetModel(output_channels=3)\n",
    "X = torch.rand(size=(1, 3, 572, 572), dtype=torch.float32)\n",
    "model(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(batch_size, resize=None):\n",
    "  trans = [transforms.ToTensor()]\n",
    "  if resize:\n",
    "    trans.insert(0, transforms.Resize(resize))\n",
    "  trans = transforms.Compose(trans)\n",
    "  train = torchvision.datasets.CelebA(root=\"../data\", split='train', transform=trans, download=True)\n",
    "  test = torchvision.datasets.CelebA(root=\"../data\", split='test', transform=trans, download=True)\n",
    "  return (\n",
    "    data.DataLoader(train, batch_size, shuffle=True, num_workers=4),\n",
    "    data.DataLoader(test, batch_size, shuffle=False, num_workers=4)\n",
    "  )\n",
    "batch_size = 10\n",
    "train_iter, test_iter = load_dataset(batch_size=batch_size,resize=(572,572))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "\n",
    "# i=0\n",
    "# for images,labels in train_iter: \n",
    "#     index = random.randint(0,batch_size-1)\n",
    "#     img = images[index] \n",
    "#     plt.subplot(3, 3, i + 1)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(img.permute(1,2,0))\n",
    "#     # plt.title(labels[index])\n",
    "#     i += 1\n",
    "#     if i >= 6:\n",
    "#        break\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "  \"\"\"Set the axes for matlabplots.\"\"\"\n",
    "  axes.set_xlabel(xlabel)\n",
    "  axes.set_ylabel(ylabel)\n",
    "  axes.set_xlim(xlim)\n",
    "  axes.set_ylim(ylim)\n",
    "  axes.set_xscale(xscale)\n",
    "  axes.set_yscale(yscale)\n",
    "  if legend:\n",
    "    axes.legend(legend)\n",
    "  axes.grid()\n",
    "\n",
    "class Animator:\n",
    "  \"\"\"For plotting data in animation\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      xlabel=None,\n",
    "      ylabel=None,\n",
    "      legend=None,\n",
    "      xlim=None,\n",
    "      ylim=None,\n",
    "      xscale=\"linear\",\n",
    "      yscale=\"linear\",\n",
    "      fmts=(\"-\", \"m--\", \"g-\", \"r:\"),\n",
    "      nrows=1,\n",
    "      ncols=1,\n",
    "      figsize=(3.5, 2.5),\n",
    "  ):\n",
    "      #Incrementally plot multiple lines\n",
    "      if legend is None:\n",
    "          legend = []\n",
    "      display.set_matplotlib_formats(\"svg\")\n",
    "      self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "      if nrows*ncols == 1:\n",
    "        self.axes = [\n",
    "            self.axes,\n",
    "        ]\n",
    "      #Use lamda function to capture arguments\n",
    "      self.config_axes = lambda: set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "      self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "  def add(self, x, y):\n",
    "    # Add multiple data points into the figure\n",
    "    if not hasattr(y, \"__len__\"):\n",
    "      y = [y]\n",
    "    n = len(y)\n",
    "    if not hasattr(x, \"__len__\"):\n",
    "      x = [x] * n\n",
    "    if not self.X:\n",
    "      self.X = [[] for _ in range(n)]\n",
    "    if not self.Y:\n",
    "      self.Y = [[] for _ in range(n)]\n",
    "\n",
    "    for i ,(a,b) in enumerate(zip(x, y)):\n",
    "      if a is not None and b is not None:\n",
    "        self.X[i].append(a)\n",
    "        self.Y[i].append(b)\n",
    "    self.axes[0].cla()\n",
    "    for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "      self.axes[0].plot(x, y, fmt)\n",
    "    self.config_axes()\n",
    "    display.display(self.fig)\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "class Timer:\n",
    "  \"\"\"Record multiple running times.\"\"\"\n",
    "  def __init__(self):\n",
    "    self.times = []\n",
    "    self.start()\n",
    "\n",
    "  def start(self):\n",
    "    \"\"\"Start the timer.\"\"\"\n",
    "    self.tik = time.time()\n",
    "\n",
    "  def stop(self):\n",
    "    \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "    self.times.append(time.time() - self.tik)\n",
    "    return self.times[-1]\n",
    "\n",
    "  def avg(self):\n",
    "    \"\"\"Returns the average time\"\"\"\n",
    "    return sum(self.times) / len(self.times)\n",
    "\n",
    "  def sum(self):\n",
    "    \"\"\"Returns the sum of time.\"\"\"\n",
    "    return sum(self.times)\n",
    "\n",
    "  def cumsum(self):\n",
    "    \"\"\"Return the accumulated time.\"\"\"\n",
    "    return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "class Accumulator:\n",
    "  \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "\n",
    "  def __init__(self, n):\n",
    "    self.data = [0.0] * n\n",
    "\n",
    "  def add(self, *args):\n",
    "    self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "  def reset(self):\n",
    "    self.data = [0.0] * len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx]\n",
    "\n",
    "def try_gpu(i=0):\n",
    "  \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "  if torch.cuda.device_count() >= i+1:\n",
    "    return torch.device(f\"cuda:{i}\")\n",
    "  return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute classification accuracy on the current batch\n",
    "# and add this metric to the accumulator, for plotting purposes\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "  \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "  if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "    y_hat = torch.argmax(y_hat, axis=1)\n",
    "  cmp_ = y_hat.type(y.dtype) == y\n",
    "  return float(cmp_.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "  \"\"\"Compute the accuracy of the model on the dataset using GPU\"\"\"\n",
    "  if isinstance(net, torch.nn.Module):\n",
    "    net.eval() # Set the model to evaluation mode\n",
    "    if not device:\n",
    "      device = next(iter(net.parameters())).device\n",
    "  # No. of correct predictions, no. of predictions\n",
    "  metric = Accumulator(2)\n",
    "  for X, y in data_iter:\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    metric.add(accuracy(net(X), y), y.numel())\n",
    "  return metric[0] / metric[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "  def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "      nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "  net.apply(init_weights)\n",
    "  print(\"training on\", device)\n",
    "  net.to(device)\n",
    "  optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "  loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "  x_resize = torchvision.transforms.Resize((388,388))\n",
    "  animator = Animator(xlabel=\"epoch\",xlim=[1, num_epochs], legend = [\"train loss\",\"train acc\", \"test acc\"])\n",
    "  timer, num_batches = Timer(), len(train_iter)\n",
    "  for epoch in range(num_epochs):\n",
    "    # Sum of training loss, sum of training accuracy, no of examples\n",
    "    metric = Accumulator(3)\n",
    "    net.train()\n",
    "    for i, (X,_) in enumerate(train_iter):\n",
    "      timer.start()\n",
    "      optimizer.zero_grad()\n",
    "      X = X.to(device)\n",
    "      y_hat = net(X)\n",
    "      y = x_resize(X)\n",
    "      l = loss(y_hat,y)\n",
    "      l.backward()\n",
    "      optimizer.step()\n",
    "      with torch.no_grad():\n",
    "        metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "      timer.stop()\n",
    "      train_l = metric[0] / metric[2]\n",
    "      train_acc = metric[1] / metric[2]\n",
    "      # if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "        # animator.add(epoch + (i + 1) / num_batches, (train_l, train_acc, None))\n",
    "      print(epoch + (i + 1) / num_batches, (train_l, train_acc, None))\n",
    "    test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "    # animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(epoch + 1, (None, None, test_acc))\n",
    "  print(f\"loss {train_l:.3f}, train_acc:{train_acc:.3f}, \" f\"test acc {test_acc:.3f}\")\n",
    "  print(f\"{metric[2] * num_epochs / timer.sum():.1f} examples/sec \" f\"on {str(device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/75/_0h54l8n6ys6k7klg1v8zggh0000gn/T/ipykernel_3851/2540042151.py:36: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  display.set_matplotlib_formats(\"svg\")\n",
      "/Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.9, 2\n",
    "train(model, train_iter, test_iter, num_epochs, lr, try_gpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
