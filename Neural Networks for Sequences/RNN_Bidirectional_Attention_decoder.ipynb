{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural machine translation (NMT) using RNN bidirectional with Attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: figures: File exists\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from IPython import display\n",
    "try:\n",
    "    import torch\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq torch\n",
    "    import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "torch.manual_seed(1)\n",
    "!mkdir figures # for saving plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Standard Download Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required functions for downloading data\n",
    "\n",
    "\n",
    "def download(name, cache_dir=os.path.join(\"..\", \"data\")):\n",
    "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split(\"/\")[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, \"rb\") as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # Hit cache\n",
    "    print(f\"Downloading {fname} from {url}...\")\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def download_extract(name, folder=None):\n",
    "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == \".zip\":\n",
    "        fp = zipfile.ZipFile(fname, \"r\")\n",
    "    elif ext in (\".tar\", \".gz\"):\n",
    "        fp = tarfile.open(fname, \"r\")\n",
    "    else:\n",
    "        assert False, \"Only zip/tar files can be extracted.\"\n",
    "    fp.extractall(base_dir)\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download preprocessor and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_nmt():\n",
    "    \"\"\"Load the English-French dataset.\"\"\"\n",
    "    data_dir = download_extract(\"fra-eng\")\n",
    "    with open(os.path.join(data_dir, \"fra.txt\"), \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def preprocess_nmt(text):\n",
    "    \"\"\"Preprocess the English-French dataset.\"\"\"\n",
    "    # print('Dataset : ',text[:100])\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(\",.!?\") and prev_char != \" \"\n",
    "\n",
    "    # Replace non-breaking space with space, and convert uppercase letters to\n",
    "    # lowercase ones\n",
    "    text = text.replace(\"\\u202f\", \" \").replace(\"\\xa0\", \" \").lower()\n",
    "    # Insert space between words and punctuation marks\n",
    "    out = [\" \" + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
    "    # print(out[:100])\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"Tokenize the English-French dataset.\"\"\"\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split(\"\\n\")):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(\" \"))\n",
    "            target.append(parts[1].split(\" \"))\n",
    "    # print(source[:10])\n",
    "    # print(target[:10])\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.unk, uniq_tokens = 0, [\"<unk>\"] + reserved_tokens\n",
    "        uniq_tokens += [token for token, freq in self.token_freqs if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "\n",
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab[\"<eos>\"]] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(l, num_steps, vocab[\"<pad>\"]) for l in lines])\n",
    "    # print('As Type : ',astype(array[:1] != vocab[\"<pad>\"], torch.int32))\n",
    "    # print('Reduce Sum', reduce_sum(astype(array[:1] != vocab[\"<pad>\"], torch.int32), 1))\n",
    "    valid_len = reduce_sum(astype(array != vocab[\"<pad>\"], torch.int32), 1)\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))\n",
    "\n",
    "\n",
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = Vocab(source, min_freq=2, reserved_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    tgt_vocab = Vocab(target, min_freq=2, reserved_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = load_array(data_arrays, batch_size)\n",
    "    return data_iter, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "Preprocess Input format : \n",
    "```\n",
    "Go.\tVa !\n",
    "Hi.\tSalut !\n",
    "Run!\tCoursâ€¯!\n",
    "```\n",
    "Preprocess output format : \n",
    "```\n",
    "['g', 'o', ' .', '\\t', 'v', 'a', ' ', '!', '\\n', 'h', 'i', ' .', '\\t', 's', 'a', 'l', 'u', 't', ' ', '!', '\\n', 'r', 'u', 'n', ' !', '\\t', 'c', 'o', 'u', 'r', 's', ' ', '!', '\\n', ...]\n",
    "```\n",
    "This function extracts all the data and create train_itr with source and target vocab\n",
    "```\n",
    "load_data_nmt(batch_size, num_steps)\n",
    "```\n",
    "It also has a default parameter `num_examples` which decides no. of examples we consider for vocab done by `tokenize_nmt` which creates output something like this :\n",
    "```\n",
    "[['go', '.'], ['hi', '.'], ['run', '!'], ['run', '!'], ['who', '?'], ['wow', '!'], ['fire', '!'], ['help', '!']]\n",
    "[['va', '!'], ['salut', '!'], ['cours', '!'], ['courez', '!'], ['qui', '?'], ['Ã§a', 'alors', '!'], ['au', 'feu', '!'], ['Ã ', \"l'aide\", '!']]\n",
    "```\n",
    "Here few words in english have been broken down into 2 words of french\n",
    "\n",
    "Next we create `vocab` of the data(source and target) without considering the order, just the words and there frequency. Now `build_array_nmt` is used to mark each line of `<eos>(end of sentence)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUB = dict()\n",
    "DATA_URL = \"http://d2l-data.s3-accelerate.amazonaws.com/\"\n",
    "\n",
    "DATA_HUB[\"fra-eng\"] = (DATA_URL + \"fra-eng.zip\", \"94646ad1522d915e7b0f9296181140edcf86a4f5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common classes for analysis and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xlabel=None,\n",
    "        ylabel=None,\n",
    "        legend=None,\n",
    "        xlim=None,\n",
    "        ylim=None,\n",
    "        xscale=\"linear\",\n",
    "        yscale=\"linear\",\n",
    "        fmts=(\"-\", \"m--\", \"g-.\", \"r:\"),\n",
    "        nrows=1,\n",
    "        ncols=1,\n",
    "        figsize=(3.5, 2.5),\n",
    "    ):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        display.set_matplotlib_formats(\"svg\")\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [\n",
    "                self.axes,\n",
    "            ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "\n",
    "\n",
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f\"cuda:{i}\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The base encoder interface for the encoder-decoder architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(Decoder):\n",
    "    \"\"\"The base attention-based decoder interface.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "Lets talk little about `nn.Embedding()`\n",
    "```\n",
    "torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0,..)\n",
    "```\n",
    "So based on the model we define we have `vocab_size` as no. of embeddings and its dimension is `embed_size`. \n",
    "\n",
    "`No. of hiddens = no. of layers * embed size (2*8)= 16`\n",
    "\n",
    "`output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
    "\n",
    "`state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "\n",
    "### Decoder \n",
    "\n",
    "`X = self.embedding(X).permute(1, 0, 2)` : changes the dimension of the tensor so (A x B x C) => (B x A x C)\n",
    "\n",
    "`context = state[-1].repeat(X.shape[0], 1, 1)` : copies the data into whatever format needed here state(2 x 4 x 16) and X.shape[0](7) : \n",
    "\n",
    "shape = (4 x 16).repeat(7, 1, 1) = (7 x 4 x 16) \n",
    "\n",
    "Here we only need state information of the last hidden layer so we use `state[-1]` of the 2 layered model.\n",
    "\n",
    "`X_and_context = torch.cat((X, context), 2)` : we concat the values of context (7 x 4 x 16) with X (7 x 4 x 8), to get (7 x 4 x 24) as we do it against `inner dimension 2`\n",
    "\n",
    "`output, state = self.rnn(X_and_context, state)` : shapes => output  :  torch.Size([7, 4, 16]) state :  torch.Size([2, 4, 16])\n",
    "\n",
    "`output = self.dense(output).permute(1, 0, 2)` : make output linear into vocab size so output shape is (4 x 7 x 10)\n",
    "\n",
    "### Create mask of different sizes : \n",
    "```\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    print('maxlen : ', maxlen)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    print('mask : ', mask)\n",
    "    # apply that mask on X\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "```\n",
    "\n",
    "`sequence_mask(X, torch.tensor([1, 1, 2]))`\n",
    "Mask created : \n",
    "```\n",
    "mask :  tensor(\n",
    "    [[ True, False, False],\n",
    "    [ True, False, False],\n",
    "    [ True,  True, False]])\n",
    "    \n",
    "and apply on X\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # print('forward : before embedding : ',X.shape)\n",
    "        # The input `X` shape: (`batch_size`, `num_steps`)\n",
    "        X = self.embedding(X)\n",
    "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
    "        # print('forward : after embedding : ',X.shape)\n",
    "        # In RNN models, the first axis corresponds to time steps\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # When state is not mentioned, it defaults to zeros\n",
    "        output, state = self.rnn(X)\n",
    "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
    "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 7])\n",
      "torch.Size([7, 4, 16])\n",
      "torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.eval()\n",
    "batch_size = 4\n",
    "num_steps = 7\n",
    "X = torch.zeros((batch_size, num_steps), dtype=torch.long)\n",
    "print(X.shape)\n",
    "output, state = encoder(X)\n",
    "print(output.shape)\n",
    "print(state.shape)\n",
    "# test_tensor = torch.tensor([1, 2, 3])\n",
    "# test_tensor.repeat(1, 1)\n",
    "# print(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaskedSoftmaxCELoss\n",
    "Note that this case is equivalent to applying LogSoftmax on an input, followed by NLLLoss.\n",
    "```\n",
    "unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "```\n",
    "As we have vocab_size a list of 10 size of 1's so we get` ln 10 => 2.3026`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # `X`: 3D tensor, `valid_lens`: 1D or 2D tensor\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "    \n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    # print('maxlen : ', maxlen)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    # print('mask : ', mask)\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# print(torch.arange((10), dtype=torch.float32, device=X.device))\n",
    "# print(torch.arange((3), dtype=torch.float32, device=X.device)[None,:])\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
    "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, shape of `queries`: (`batch_size`, no. of\n",
    "        # queries, 1, `num_hiddens`) and shape of `keys`: (`batch_size`, 1,\n",
    "        # no. of key-value pairs, `num_hiddens`). Sum them up with\n",
    "        # broadcasting\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # There is only one output of `self.w_v`, so we remove the last\n",
    "        # one-dimensional entry from the shape. Shape of `scores`:\n",
    "        # (`batch_size`, no. of queries, no. of key-value pairs)\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
    "        # dimension)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "    \n",
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n",
    "        self.attention = AdditiveAttention(num_hiddens, num_hiddens, num_hiddens, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # Shape of `enc_outputs`: (`batch_size`, `num_steps`, `num_hiddens`).\n",
    "        # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\n",
    "        # `num_hiddens`)\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
    "        # print('Before embedding : ',X.shape)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        outputs, self._attention_weights = [], []\n",
    "        for x in X:\n",
    "            # Shape of `query`: (`batch_size`, 1, `num_hiddens`)\n",
    "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
    "            # Shape of `context`: (`batch_size`, 1, `num_hiddens`)\n",
    "            context = self.attention(query, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "            # Concatenate on the feature dimension\n",
    "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
    "            # Reshape `x` as (1, `batch_size`, `embed_size` + `num_hiddens`)\n",
    "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
    "            outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        # After fully-connected layer transformation, shape of `outputs`:\n",
    "        # (`num_steps`, `batch_size`, `vocab_size`)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/debashisdas/Projects/DeepLearning/Neural Networks for Sequences/RNN_Bidirectional_Attention_decoder.ipynb Cell 27\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m decoder\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m state \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39minit_state(encoder(X))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m output, state \u001b[39m=\u001b[39m decoder(X, state)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)  \u001b[39m# (batch size, number of time steps, vocabulary size)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(state\u001b[39m.\u001b[39mshape)  \u001b[39m# (num layers, batch size, num hiddens)\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/debashisdas/Projects/DeepLearning/Neural Networks for Sequences/RNN_Bidirectional_Attention_decoder.ipynb Cell 27\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X, state):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# Shape of `enc_outputs`: (`batch_size`, `num_steps`, `num_hiddens`).\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# `num_hiddens`)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     enc_outputs, hidden_state, enc_valid_lens \u001b[39m=\u001b[39m state\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# print('Before embedding : ',X.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/debashisdas/Projects/DeepLearning/Neural%20Networks%20for%20Sequences/RNN_Bidirectional_Attention_decoder.ipynb#X35sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(X)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "# decoder.eval()\n",
    "# state = decoder.init_state(encoder(X))\n",
    "# output, state = decoder(X, state)\n",
    "# print(output.shape)  # (batch size, number of time steps, vocabulary size)\n",
    "# print(state.shape)  # (num layers, batch size, num hiddens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
    "\n",
    "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` shape: (`batch_size`, `num_steps`)\n",
    "    # `valid_len` shape: (`batch_size`,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        # print(weights)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        # print(weights)\n",
    "        self.reduction = \"none\"\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "        # print(unweighted_loss)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss\n",
    "\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long), torch.tensor([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis\n",
    "```\n",
    "net.train()\n",
    "```\n",
    "It helps the model to understand that what is the current mode of exection. This helps the model perform differently during training and test.\n",
    "This helps majorly in cases where we use dropout with a porpose of regularization.This is opposite of `net.eval()`\n",
    "```\n",
    "# Y[:, :-1] : represents all elements of outer dimension and all except last of inner dimension\n",
    "dec_input = torch.cat([bos, Y[:, :-1]], 1)\n",
    "\n",
    "Y         = [ 10,   0,  23, 199,  32, 200,   4,   3,   1]\n",
    "Y[:, :-1] = [ 10,   0,  23, 199,  32, 200,   4,   3,   1,   1]\n",
    "dec_input = [  2,  10,   0,  23, 199,  32, 200,   4,   3,   1]\n",
    "\n",
    "```\n",
    "`Y_hat, _ = net(X, dec_input, X_valid_len)` passes input elements and dec_input here intial state output is given as `<bos>` ==> 2  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
    "\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    animator = Animator(xlabel=\"epoch\", ylabel=\"loss\", xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = Timer()\n",
    "        metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "        for batch in data_iter:\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab[\"<bos>\"]] * Y.shape[0], device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()  # Make the loss scalar for `backward`\n",
    "            grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f\"loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} \" f\"tokens/sec on {str(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "lr, num_epochs, device = 0.005, 300, try_gpu()\n",
    "batch_size, num_steps = 64, 10\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)\n",
    "\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Here during prediction we use the following code\n",
    "```\n",
    "enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "```\n",
    "Here you can see that we pass enc_outputs to the decoder as input which as \n",
    "```\n",
    "dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "```  \n",
    "as we used `Teacher forcing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps, device, save_attention_weights=False):\n",
    "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
    "    # Set `net` to eval mode for inference\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(\" \")] + [src_vocab[\"<eos>\"]]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab[\"<pad>\"])\n",
    "    # Add the batch axis\n",
    "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # Add the batch axis\n",
    "    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab[\"<bos>\"]], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Save attention weights (to be covered later)\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # Once the end-of-sequence token is predicted, the generation of the\n",
    "        # output sequence is complete\n",
    "        if pred == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return \" \".join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU (bilingual evaluation understudy) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):\n",
    "    \"\"\"Compute the BLEU.\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(\" \"), label_seq.split(\" \")\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[\"\".join(label_tokens[i : i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[\"\".join(pred_tokens[i : i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[\"\".join(pred_tokens[i : i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "engs = [\"go .\", \"i lost .\", \"he's calm .\", \"i'm home .\"]\n",
    "fras = [\"va !\", \"j'ai perdu .\", \"il est calme .\", \"je suis chez moi .\"]\n",
    "\n",
    "data = []\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    score = bleu(translation, fra, k=2)\n",
    "    data.append((eng, fra, translation, score))\n",
    "print(f'English : {eng}, Truth : {fra}, Prediction : {translation}, Bleu : {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = engs[-1]  # length 3+1\n",
    "fra = fras[-1]  # length 5+1\n",
    "\n",
    "translation, dec_attention_weight_seq = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "\n",
    "attention_weights = torch.cat([step[0][0][0] for step in dec_attention_weight_seq], 0).reshape((1, 1, -1, num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5), cmap=\"Reds\"):\n",
    "    display.set_matplotlib_formats(\"svg\")\n",
    "    num_rows, num_cols = matrices.shape[0], matrices.shape[1]\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize, sharex=True, sharey=True, squeeze=False)\n",
    "    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\n",
    "        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\n",
    "            pcm = ax.imshow(matrix.detach(), cmap=cmap)\n",
    "            if i == num_rows - 1:\n",
    "                ax.set_xlabel(xlabel)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(ylabel)\n",
    "            if titles:\n",
    "                ax.set_title(titles[j])\n",
    "    fig.colorbar(pcm, ax=axes, shrink=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plus one to include the end-of-sequence token\n",
    "show_heatmaps(\n",
    "    attention_weights[:, :, :, : len(engs[-1].split()) + 1].cpu(), xlabel=\"Key posistions\", ylabel=\"Query posistions\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
