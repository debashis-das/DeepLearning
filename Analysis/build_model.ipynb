{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey, randint, uniform, permutation, split\n",
    "import jax.numpy as jnp\n",
    "import jax.dlpack\n",
    "from jax.lax import scan\n",
    "from jax import vmap, jit, value_and_grad\n",
    "from jax.nn import softmax\n",
    "from jax.scipy.special import expit, logit\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import distrax\n",
    "from distrax._src.utils import jittable\n",
    "\n",
    "import itertools\n",
    "import haiku as hk\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Any, Iterator, Mapping, NamedTuple, Sequence, Tuple\n",
    "Batch = Mapping[str, np.ndarray]\n",
    "\n",
    "def load_dataset(split: str, batch_size: int) -> Iterator[Batch]:\n",
    "  ds = tfds.load(\"binarized_mnist\", split=split, shuffle_files=True)\n",
    "  ds = ds.shuffle(buffer_size=10 * batch_size)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(buffer_size=5)\n",
    "  ds = ds.repeat()\n",
    "  data = tfds.as_numpy(ds)\n",
    "  return iter(tfds.as_numpy(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class Encoder(hk.Module):\n",
    "  \"\"\"Encoder model.\"\"\"\n",
    "\n",
    "  def __init__(self, hidden_size: int = 512, latent_size: int = 10):\n",
    "    super().__init__()\n",
    "    self._hidden_size = hidden_size\n",
    "    self._latent_size = latent_size\n",
    "    \n",
    "\n",
    "  def __call__(self, x: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    x = hk.Flatten()(x)\n",
    "    x = hk.Linear(self._hidden_size)(x)\n",
    "    x = jax.nn.relu(x)\n",
    "    x = hk.Linear(self._latent_size)(x)\n",
    "    x = jax.nn.sigmoid(x)\n",
    "    mean = hk.Linear(self._latent_size)(x)\n",
    "    log_stddev = hk.Linear(self._latent_size)(x)\n",
    "    stddev = jnp.exp(log_stddev)\n",
    "    return mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages/haiku/_src/initializers.py:127: UserWarning: Explicitly requested dtype float64  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  unscaled = jax.random.truncated_normal(\n",
      "/Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages/haiku/_src/base.py:658: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n",
      "2023-12-19 19:40:28.714288: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "/Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:733: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return getattr(self.aval, name).fun(self, *args, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`hk.next_rng_key` must be used as part of an `hk.transform`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m test_data \u001b[38;5;241m=\u001b[39m load_dataset(tfds\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTEST, batch_size)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_of_steps):\n\u001b[0;32m---> 52\u001b[0m     params, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrng_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     55\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(params,\u001b[38;5;28mnext\u001b[39m(rng_seq),\u001b[38;5;28mnext\u001b[39m(test_data))\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[16], line 23\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(params, rng_key, opt_state, batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\n\u001b[1;32m     17\u001b[0m     params: hk\u001b[38;5;241m.\u001b[39mParams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     batch: Batch,\n\u001b[1;32m     21\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[hk\u001b[38;5;241m.\u001b[39mParams, OptState]:\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Single SGD update step.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     updates, new_opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state)\n\u001b[1;32m     25\u001b[0m     new_params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 22 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[16], line 35\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, rng_key, batch)\u001b[0m\n\u001b[1;32m     31\u001b[0m mean,std \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(params, rng_key, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     32\u001b[0m likelihood_distrib \u001b[38;5;241m=\u001b[39m distrax\u001b[38;5;241m.\u001b[39mIndependent(\n\u001b[1;32m     33\u001b[0m     distrax\u001b[38;5;241m.\u001b[39mBernoulli(logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m---> 35\u001b[0m z \u001b[38;5;241m=\u001b[39m likelihood_distrib\u001b[38;5;241m.\u001b[39msample(seed\u001b[38;5;241m=\u001b[39m\u001b[43mhk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_rng_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz : \u001b[39m\u001b[38;5;124m'\u001b[39m,z\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# p(z) = N(0, I)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# prior_z = distrax.MultivariateNormalDiag(\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     loc=jnp.zeros((latent_size,)),\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#     scale_diag=jnp.ones((latent_size,)))\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/haiku/_src/base.py:1144\u001b[0m, in \u001b[0;36mnext_rng_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_rng_key\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PRNGKey:\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a unique JAX random key split from the current global key.\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \n\u001b[1;32m   1137\u001b[0m \u001b[38;5;124;03m  >>> key = hk.next_rng_key()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;124;03m    used with APIs such as :func:`jax.random.uniform`.\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1144\u001b[0m   \u001b[43massert_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext_rng_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1145\u001b[0m   assert_jax_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_rng_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m next_rng_key_internal()\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/haiku/_src/base.py:364\u001b[0m, in \u001b[0;36massert_context\u001b[0;34m(public_symbol_name)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_context\u001b[39m(public_symbol_name):\n\u001b[1;32m    363\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m frame_stack:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`hk.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` must be used as part of an `hk.transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    366\u001b[0m             public_symbol_name))\n",
      "\u001b[0;31mValueError\u001b[0m: `hk.next_rng_key` must be used as part of an `hk.transform`"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "OptState = Any\n",
    "no_of_steps = 5000\n",
    "batch_size = 128\n",
    "no_of_batches = 10\n",
    "latent_size = 10\n",
    "batch_size = 10\n",
    "\n",
    "model = hk.transform(\n",
    "    lambda x: Encoder(latent_size)(x),  # pylint: disable=unnecessary-lambda\n",
    "    apply_rng=True)\n",
    "optimizer = optax.adam(0.001)\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params,\n",
    "    rng_key: PRNGKey,\n",
    "    opt_state: OptState,\n",
    "    batch: Batch,\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(loss_fn)(params, rng_key, batch)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params: hk.Params, rng_key: PRNGKey, batch: Batch) -> jnp.ndarray:\n",
    "    \"\"\"Loss = -ELBO, where ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z)).\"\"\"\n",
    "    mean,std = model.apply(params, rng_key, batch[\"image\"])\n",
    "    likelihood_distrib = distrax.Independent(\n",
    "        distrax.Bernoulli(logits=10))\n",
    "    \n",
    "    z = likelihood_distrib.sample(seed=hk.next_rng_key())\n",
    "    print('z : ',z.shape)\n",
    "    # p(z) = N(0, I)\n",
    "    # prior_z = distrax.MultivariateNormalDiag(\n",
    "    #     loc=jnp.zeros((latent_size,)),\n",
    "    #     scale_diag=jnp.ones((latent_size,)))\n",
    "\n",
    "    log_likelihood = likelihood_distrib.log_prob(batch[\"image\"])\n",
    "    return -jnp.mean(log_likelihood)\n",
    "rng_seq = hk.PRNGSequence(42)\n",
    "params = model.init(next(rng_seq), np.zeros((1, *(1,28,28))))\n",
    "opt_state = optimizer.init(params)\n",
    "train_data = load_dataset(tfds.Split.TRAIN, batch_size)\n",
    "test_data = load_dataset(tfds.Split.TEST, batch_size)\n",
    "\n",
    "\n",
    "for step in range(no_of_steps):\n",
    "    params, opt_state = update(params, next(rng_seq), opt_state, next(train_data))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_fn(params,next(rng_seq),next(test_data))\n",
    "        print(f' Step: {step}; Validation loss : {-loss}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
