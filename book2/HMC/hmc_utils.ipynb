{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "-476.06305\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_probability.substrates import jax as tfp\n",
    "import jax\n",
    "rng_key = jax.random.PRNGKey(10)\n",
    "tfd = tfp.distributions\n",
    "# logits = [[0,1],[1,0]]\n",
    "logits = tfd.Normal(0,10).sample(seed=rng_key, sample_shape=(50,2))\n",
    "outputs = [1,0]\n",
    "log_lik = tfd.Bernoulli(logits=logits).log_prob(outputs)\n",
    "print(log_lik.shape)\n",
    "print(log_lik.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchedTensor(lvl=1, bdim=0, value=\n",
      "    tensor([1, 2, 3])\n",
      ") BatchedTensor(lvl=1, bdim=0, value=\n",
      "    tensor([0.1000, 0.2000, 0.3000])\n",
      ") BatchedTensor(lvl=1, bdim=0, value=\n",
      "    tensor([0.0100, 0.0200, 0.0300])\n",
      ")\n",
      "tensor([1.1100, 2.2200, 3.3300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def sum_all_valus(a,b,c):\n",
    "    print(a,b,c)\n",
    "    return a+b+c\n",
    "\n",
    "apply_data1 =  torch.vmap(sum_all_valus)\n",
    "# loglikelihood of y\n",
    "# print(n_of_positives.shape,group_size.shape,thetas.shape)\n",
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([0.1,0.2,0.3])\n",
    "c = torch.tensor([0.01,0.02,0.03])\n",
    "# logprob_y = torch.sum(\n",
    "#     apply_data(a, b, c)\n",
    "# )\n",
    "print(apply_data1(a, b, c))\n",
    "# print(logprob_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71]) torch.Size([71]) torch.Size([71])\n",
      "tensor(-11.5149)\n",
      "tensor(0.) tensor(20.) tensor(0.4377)\n",
      "tensor(-11.5149)\n",
      "BatchedTensor(lvl=1, bdim=0, value=\n",
      "    tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "             1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
      "             2.,  2.,  2.,  1.,  5.,  2.,  5.,  3.,  2.,  7.,  7.,  3.,  3.,  2.,\n",
      "             9., 10.,  4.,  4.,  4.,  4.,  4.,  4.,  4., 10.,  4.,  4.,  4.,  5.,\n",
      "            11., 12.,  5.,  5.,  6.,  5.,  6.,  6.,  6.,  6., 16., 15., 15.,  9.,\n",
      "             4.])\n",
      ") BatchedTensor(lvl=1, bdim=0, value=\n",
      "    tensor([20., 20., 20., 20., 20., 20., 20., 19., 19., 19., 19., 18., 18., 17.,\n",
      "            20., 20., 20., 20., 19., 19., 18., 18., 25., 24., 23., 20., 20., 20.,\n",
      "            20., 20., 20., 10., 49., 19., 46., 27., 17., 49., 47., 20., 20., 13.,\n",
      "            48., 50., 20., 20., 20., 20., 20., 20., 20., 48., 19., 19., 19., 22.,\n",
      "            46., 49., 20., 20., 23., 19., 22., 20., 20., 20., 52., 46., 47., 24.,\n",
      "            14.])\n",
      ") BatchedTensor(lvl=1, bdim=0, value=\n",
      "    tensor([0.4377, 0.1193, 0.2001, 0.5459, 0.2212, 0.2250, 0.8657, 0.1380, 0.3748,\n",
      "            0.1874, 0.8802, 0.2390, 0.1310, 0.2534, 0.7051, 0.1230, 0.6963, 0.7879,\n",
      "            0.5084, 0.4580, 0.6189, 0.5963, 0.6736, 0.4422, 0.7612, 0.4317, 0.5968,\n",
      "            0.3210, 0.6284, 0.3095, 0.7718, 0.2882, 0.6099, 0.3075, 0.2314, 0.4812,\n",
      "            0.6365, 0.4115, 0.8474, 0.6031, 0.2507, 0.6785, 0.6496, 0.3439, 0.4170,\n",
      "            0.8274, 0.2050, 0.6992, 0.7804, 0.7777, 0.4196, 0.4549, 0.3584, 0.6596,\n",
      "            0.4202, 0.1592, 0.7908, 0.7017, 0.6118, 0.2985, 0.8264, 0.2664, 0.7433,\n",
      "            0.8648, 0.7175, 0.1562, 0.7463, 0.8282, 0.4917, 0.8678, 0.6730])\n",
      ")\n",
      "tensor([-11.5149,  -2.5402,  -4.4655, -15.7880,  -4.9988,  -5.0978, -40.1584,\n",
      "         -2.8224,  -8.9249,  -3.9434, -40.3112,  -4.9166,  -2.5283,  -4.9682,\n",
      "        -20.5552,  -1.5933, -20.0068, -26.7098, -10.5144,  -8.8610, -13.9872,\n",
      "        -13.0461, -20.8349,  -8.8538, -25.0901,  -6.6038, -12.1332,  -3.9947,\n",
      "        -13.5016,  -3.7647, -21.8642,  -2.0008, -29.4324,  -3.4630,  -3.9769,\n",
      "         -9.9648, -11.1703, -10.2170, -58.3958, -10.1886,  -2.0184,  -8.9022,\n",
      "        -23.5431,  -4.4799,  -3.6456, -20.3825,  -1.5239, -12.1645, -16.7629,\n",
      "        -16.5787,  -3.6926,  -8.3336,  -2.4992,  -9.5652,  -3.3810,  -1.9571,\n",
      "        -34.0277, -23.7640,  -7.0006,  -1.7142, -19.3846,  -1.5897, -12.3188,\n",
      "        -18.3176,  -9.1253,  -2.9528, -24.0865, -30.4651,  -4.9556, -17.5461,\n",
      "         -5.8528])\n"
     ]
    }
   ],
   "source": [
    "n_of_positives = torch.tensor( [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 5, 2, 5, 3, 2, 7, 7, 3, 3, 2, 9, 10, 4, 4, 4, 4, 4, 4, 4, 10, 4, 4, 4, 5, 11, 12, 5, 5, 6, 5, 6, 6, 6, 6, 16, 15, 15, 9, 4, ], dtype=torch.float32)\n",
    "group_size = torch.tensor( [ 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18, 18, 17, 20, 20, 20, 20, 19, 19, 18, 18, 25, 24, 23, 20, 20, 20, 20, 20, 20, 10, 49, 19, 46, 27, 17, 49, 47, 20, 20, 13, 48, 50, 20, 20, 20, 20, 20, 20, 20, 48, 19, 19, 19, 22, 46, 49, 20, 20, 23, 19, 22, 20, 20, 20, 52, 46, 47, 24, 14, ], dtype=torch.float32)\n",
    "logits_shape = torch.tensor([n_of_positives.shape])\n",
    "logits = torch.distributions.uniform.Uniform(-2, 2).sample(logits_shape)\n",
    "thetas = torch.nn.functional.sigmoid(logits)\n",
    "print(n_of_positives.shape,group_size.shape,thetas.shape)\n",
    "def binomial(y, N, theta):\n",
    "    print(y,N,theta)\n",
    "    # return y+N+theta\n",
    "    return torch.distributions.binomial.Binomial(N, probs=theta, validate_args=False).log_prob(y)\n",
    "    # return y+N+theta\n",
    "print(torch.distributions.binomial.Binomial(group_size[0], probs=thetas[0]).log_prob((n_of_positives[0])))\n",
    "print(binomial(n_of_positives[0],group_size[0],thetas[0]))\n",
    "apply_data2 =  torch.vmap(binomial)\n",
    "\n",
    "# apply_data =  torch.vmap(lambda y, N, theta: torch.distributions.binomial.Binomial(N, probs=theta).log_prob(y))\n",
    "print(apply_data2(n_of_positives,group_size,thetas))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5 0.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0.  0. ]\n",
      " [0.  0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  0.  0.5 0.  0.5]\n",
      " [0.  0.  0.  0.  0.5 0.5]]\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "[[0.5 0.5 0.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0.  0. ]\n",
      " [0.  0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  0.  0.5 0.  0.5]\n",
      " [0.  0.  0.  0.  0.5 0.5]]\n",
      "[[0.5 0.5 0.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0.  0. ]\n",
      " [0.  0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  0.  0.5 0.  0.5]\n",
      " [0.  0.  0.  0.  0.5 0.5]]\n",
      "[[0.5 0.5 0.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0.  0. ]\n",
      " [0.  0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  0.  0.5 0.  0.5]\n",
      " [0.  0.  0.  0.  0.5 0.5]]\n",
      "[[0.5  0.25 0.25 0.   0.   0.  ]\n",
      " [0.25 0.5  0.   0.25 0.   0.  ]\n",
      " [0.25 0.   0.5  0.   0.25 0.  ]\n",
      " [0.   0.25 0.   0.5  0.   0.25]\n",
      " [0.   0.   0.25 0.   0.5  0.25]\n",
      " [0.   0.   0.   0.25 0.25 0.5 ]]\n",
      "[[0.5 0.5 0.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0.  0. ]\n",
      " [0.  0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  0.  0.5 0.  0.5]\n",
      " [0.  0.  0.  0.  0.5 0.5]]\n",
      "[[0.375 0.375 0.125 0.125 0.    0.   ]\n",
      " [0.375 0.125 0.375 0.    0.125 0.   ]\n",
      " [0.125 0.375 0.    0.375 0.    0.125]\n",
      " [0.125 0.    0.375 0.    0.375 0.125]\n",
      " [0.    0.125 0.    0.375 0.125 0.375]\n",
      " [0.    0.    0.125 0.125 0.375 0.375]]\n",
      "[[0.5 0.5 0.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0.  0. ]\n",
      " [0.  0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  0.  0.5 0.  0.5]\n",
      " [0.  0.  0.  0.  0.5 0.5]]\n",
      "[[0.24609375 0.20605469 0.20605469 0.12695312 0.12695312 0.08789062]\n",
      " [0.20605469 0.24609375 0.12695312 0.20605469 0.08789062 0.12695312]\n",
      " [0.20605469 0.12695312 0.24609375 0.08789062 0.20605469 0.12695312]\n",
      " [0.12695312 0.20605469 0.08789062 0.24609375 0.12695312 0.20605469]\n",
      " [0.12695312 0.08789062 0.20605469 0.12695312 0.24609375 0.20605469]\n",
      " [0.08789062 0.12695312 0.12695312 0.20605469 0.20605469 0.24609375]]\n",
      "[[0.5 0.5 0.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0.  0. ]\n",
      " [0.  0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  0.  0.5 0.  0.5]\n",
      " [0.  0.  0.  0.  0.5 0.5]]\n",
      "[[0.16666685 0.16666675 0.16666675 0.16666657 0.16666657 0.16666646]\n",
      " [0.16666675 0.16666684 0.16666655 0.16666676 0.16666646 0.16666657]\n",
      " [0.16666675 0.16666657 0.16666684 0.16666648 0.16666676 0.16666657]\n",
      " [0.16666657 0.16666675 0.16666646 0.16666685 0.16666658 0.16666676]\n",
      " [0.16666657 0.16666648 0.16666675 0.16666657 0.16666687 0.16666676]\n",
      " [0.16666648 0.16666658 0.16666657 0.16666675 0.16666676 0.16666685]]\n",
      "[[0.5 0.5 0.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0.  0. ]\n",
      " [0.  0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  0.  0.5 0.  0.5]\n",
      " [0.  0.  0.  0.  0.5 0.5]]\n",
      "[[0.16666666 0.16666664 0.16666664 0.16666666 0.16666666 0.16666667]\n",
      " [0.16666666 0.16666666 0.16666666 0.16666667 0.16666666 0.16666666]\n",
      " [0.16666666 0.16666666 0.16666666 0.16666666 0.16666666 0.16666666]\n",
      " [0.16666666 0.16666666 0.16666666 0.16666666 0.16666666 0.16666664]\n",
      " [0.16666666 0.16666666 0.16666666 0.16666666 0.16666666 0.16666666]\n",
      " [0.16666664 0.16666664 0.16666664 0.16666667 0.16666666 0.16666664]]\n",
      "[[0.5 0.5 0.  0.  0.  0. ]\n",
      " [0.5 0.  0.5 0.  0.  0. ]\n",
      " [0.  0.5 0.  0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.  0.5 0. ]\n",
      " [0.  0.  0.  0.5 0.  0.5]\n",
      " [0.  0.  0.  0.  0.5 0.5]]\n",
      "[[0.16666663 0.16666663 0.16666663 0.16666663 0.16666664 0.16666663]\n",
      " [0.16666664 0.16666664 0.16666664 0.16666664 0.16666664 0.16666663]\n",
      " [0.16666664 0.16666664 0.16666664 0.16666664 0.16666666 0.16666664]\n",
      " [0.16666664 0.16666664 0.16666664 0.16666664 0.16666666 0.16666664]\n",
      " [0.16666664 0.16666664 0.16666664 0.16666664 0.16666664 0.16666663]\n",
      " [0.16666663 0.16666663 0.16666663 0.16666663 0.16666664 0.16666663]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "# create transition matrix\n",
    "prob_vect = jnp.repeat(0.5, 5)\n",
    "# print(prob_vect)\n",
    "transition_mat = jnp.diag(prob_vect, -1) + jnp.diag(prob_vect, 1)\n",
    "transition_mat = transition_mat.at[0, 0].set(0.5)\n",
    "transition_mat = transition_mat.at[5, 5].set(0.5)\n",
    "\n",
    "def calculate_q_at_state_no(state_no, p_initial, transition_mat):\n",
    "    transition_mat_at_state_no = jnp.linalg.matrix_power(transition_mat, state_no)\n",
    "    print(transition_mat_at_state_no)\n",
    "    q = jnp.matmul(transition_mat_at_state_no, p_initial)  # q = p_initial * T^n = transpose(T)^n * p_initial\n",
    "    return q\n",
    "\n",
    "x_0 = 2  # initial state\n",
    "p_initial = [0] * 6\n",
    "p_initial[x_0] = 1\n",
    "p_initial = jnp.array(p_initial)\n",
    "p_initial\n",
    "states = [0, 1, 2, 3, 10, 100, 200, 400]\n",
    "for ind, state_no in enumerate(states):\n",
    "    print(transition_mat)\n",
    "    q = calculate_q_at_state_no(\n",
    "        state_no, p_initial, transition_mat\n",
    "    )  # q is proposed probabilities for p(x) on particular markov state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5448514   0.2774427  -0.29255113 -0.91421574 -0.62452507 -0.24748124\n",
      " -0.85743254 -0.7823266   0.7682733   0.59566766]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "@jax.jit\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jax.numpy.where(x > 0, x, alpha * jax.numpy.exp(x) - alpha)\n",
    "\n",
    "key = jax.random.key(0)\n",
    "x = jax.random.normal(key, (10,))\n",
    "print(selu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import jit,lax\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "rng_key = jax.random.PRNGKey(82)\n",
    "tfd = tfp.distributions\n",
    "n_rat_tumors = 71\n",
    "\n",
    "jacobian_fn = jax.jacfwd(jax.nn.sigmoid)\n",
    "jacobian_fn_softplus = jax.jacfwd(jax.nn.softplus)\n",
    "\n",
    "def joint_log_prob(params):\n",
    "    a, b, logits = params[\"a\"], params[\"b\"], params[\"logits\"]\n",
    "\n",
    "    # HMC requires unconstrained sample space, thus it samples logits and we apply change of variable to\n",
    "    # get theta from logits\n",
    "    thetas = jax.nn.sigmoid(logits)\n",
    "    a = jax.nn.softplus(a)\n",
    "    b = jax.nn.softplus(b)\n",
    "    log_det_jacob = jnp.sum(\n",
    "        jax.vmap(lambda logit: jnp.log(jnp.abs(jnp.linalg.det(jacobian_fn(logit.reshape(1, 1))))))(logits)\n",
    "    )\n",
    "\n",
    "    log_det_jacob_priors = jnp.log(jnp.abs(jacobian_fn(a))) + jnp.log(jnp.abs(jacobian_fn(b)))\n",
    "\n",
    "    # improper prior for a,b\n",
    "    logprob_ab = jnp.log(jnp.power(a + b, -2.5))\n",
    "\n",
    "    # logprob prior of theta\n",
    "    logprob_thetas = tfd.Beta(a, b).log_prob(thetas).sum()\n",
    "\n",
    "    # loglikelihood of y\n",
    "    logprob_y = jnp.sum(\n",
    "        jax.vmap(lambda y, N, theta: tfd.Binomial(N, probs=theta).log_prob(y))(n_of_positives, group_size, thetas)\n",
    "    )\n",
    "    print('joint_log_prob')\n",
    "    return logprob_ab + logprob_thetas + logprob_y + log_det_jacob + log_det_jacob_priors\n",
    "\n",
    "    # return logprob_ab + logprob_thetas + logprob_y + log_det_jacob\n",
    "\n",
    "def init_param_fn(seed):\n",
    "    \"\"\"\n",
    "    initialize a, b & logits\n",
    "    \"\"\"\n",
    "    key1, key2 = jax.random.split(seed)\n",
    "    return {\n",
    "        \"a\": tfd.Uniform(0, 3).sample(seed=key1),\n",
    "        \"b\": tfd.Uniform(0, 3).sample(seed=key2),\n",
    "        \"logits\": tfd.Uniform(-2, 2).sample(n_rat_tumors, seed),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kernel(key, state):\n",
    "    output = [jax.numpy.where(state > 0, 1,-1),key]\n",
    "    if output == 1:\n",
    "        return output, 'greater than 1'\n",
    "    else:\n",
    "        return output, 'less than 1'\n",
    "\n",
    "n_chains = 4\n",
    "keys = jax.random.split(rng_key, n_chains)\n",
    "initial_states = jax.vmap(lambda seed: kernel(init_param_fn(seed)))(keys)\n",
    "\n",
    "\n",
    "\n",
    "def abc(num_samples,num_chains):\n",
    "    @jit\n",
    "    def xyz(states):\n",
    "        keys = jax.random.split(rng_key, num_chains)\n",
    "        states, infos = jax.vmap(kernel)(keys,states)\n",
    "        return states, (states,infos)\n",
    "    \n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, (states, infos) = lax.scan(xyz, 2.4, keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'a': Array([2.8442535 , 2.4914398 , 1.910634  , 0.39569843], dtype=float32)}, Array([[4194276336, 3483324415],\n",
      "       [ 652449214,  703397549],\n",
      "       [1471379998, 3506494469],\n",
      "       [2265643003,  811380549]], dtype=uint32))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_probability.substrates.jax as tfp\n",
    "import jax\n",
    "key = jax.random.PRNGKey(82)\n",
    "tfd = tfp.distributions\n",
    "\n",
    "def uniform_data(seed):\n",
    "    return {'a':tfd.Uniform(0, 3).sample(seed=seed)}\n",
    "\n",
    "def states(position,seed):\n",
    "    return (position,seed)\n",
    "\n",
    "keys = jax.random.split(key,4)\n",
    "initial_states = jax.vmap(lambda seed: states(uniform_data(seed),seed))(keys)\n",
    "print(initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.dot(torch.tensor([1,2,3]),torch.tensor([1,2,3]))\n",
    "# batch_dot = torch.func.vmap(torch.dot)\n",
    "# batch_dot(torch.tensor([[1,2,3]]),torch.tensor([[1,2,3]]))\n",
    "# x,y = torch.randn(2,5), torch.randn(2,5)\n",
    "# print(x)\n",
    "# print(y)\n",
    "# print(batch_dot(x,y))\n",
    "\n",
    "# f = lambda dict: torch.dot(dict['x'], dict['y'])\n",
    "# input = [{'x': torch.randn(2, 5), 'y': torch.randn(5)} for _ in range(4)]\n",
    "# batched_dot = torch.vmap(f, in_dims=0)\n",
    "# batched_dot(input)\n",
    "\n",
    "abc = torch.Tensor([[1,2,3,4],[5,6,7,8]])\n",
    "print(abc.dim())\n",
    "print(abc.shape[:1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': Array(2.4621992, dtype=float32), 'b': Array(1.5081836, dtype=float32), 'logits': Array([-0.5830965 ,  0.9119353 , -1.0459356 ,  0.6607442 ,  0.05115604,\n",
      "        1.9180942 , -0.8123336 , -0.44906664, -0.7647896 ,  1.3639178 ,\n",
      "       -1.2365346 , -0.5110321 , -0.36780643, -0.30413723,  1.486865  ,\n",
      "       -1.1990237 , -0.10980558,  1.2190585 ,  0.11574888, -0.76580906,\n",
      "       -1.3776202 ,  1.7598639 ,  0.20172453,  0.601172  ,  1.3165002 ,\n",
      "       -0.50377035,  0.03355122,  0.59856844,  0.6224303 , -0.5276737 ,\n",
      "        1.7546191 , -0.20318699, -1.401957  ,  0.3111601 ,  1.4075761 ,\n",
      "       -0.18194914,  1.5353355 , -1.3714151 , -0.9289584 ,  1.684566  ,\n",
      "        0.27385235, -0.04773569, -1.4597545 ,  1.8086128 , -0.39681387,\n",
      "       -1.822547  , -0.00653982,  1.7390656 ,  1.6817617 ,  1.7226138 ,\n",
      "       -0.88208246,  1.4745278 , -0.20355034,  1.624649  , -0.10385036,\n",
      "       -0.49547052, -1.1517582 , -0.5216894 ,  0.3182559 ,  1.5142751 ,\n",
      "       -0.41060638,  0.3635683 , -0.2908492 ,  1.0384064 ,  0.21553755,\n",
      "        0.36332464,  1.1165876 ,  1.7729998 ,  0.59590054,  1.1304469 ,\n",
      "        0.36041832], dtype=float32)}\n",
      "[ 2.4621992   1.5081836  -0.5830965   0.9119353  -1.0459356   0.6607442\n",
      "  0.05115604  1.9180942  -0.8123336  -0.44906664 -0.7647896   1.3639178\n",
      " -1.2365346  -0.5110321  -0.36780643 -0.30413723  1.486865   -1.1990237\n",
      " -0.10980558  1.2190585   0.11574888 -0.76580906 -1.3776202   1.7598639\n",
      "  0.20172453  0.601172    1.3165002  -0.50377035  0.03355122  0.59856844\n",
      "  0.6224303  -0.5276737   1.7546191  -0.20318699 -1.401957    0.3111601\n",
      "  1.4075761  -0.18194914  1.5353355  -1.3714151  -0.9289584   1.684566\n",
      "  0.27385235 -0.04773569 -1.4597545   1.8086128  -0.39681387 -1.822547\n",
      " -0.00653982  1.7390656   1.6817617   1.7226138  -0.88208246  1.4745278\n",
      " -0.20355034  1.624649   -0.10385036 -0.49547052 -1.1517582  -0.5216894\n",
      "  0.3182559   1.5142751  -0.41060638  0.3635683  -0.2908492   1.0384064\n",
      "  0.21553755  0.36332464  1.1165876   1.7729998   0.59590054  1.1304469\n",
      "  0.36041832]\n",
      "<jax._src.util.HashablePartial object at 0x3371d9910>\n",
      "(73,)\n"
     ]
    }
   ],
   "source": [
    "from jax.flatten_util import ravel_pytree\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "import jax\n",
    "\n",
    "tfd = tfp.distributions\n",
    "rng_key = jax.random.PRNGKey(82)\n",
    "key1, key2 = jax.random.split(rng_key)\n",
    "position =  {\n",
    "        'a'  : tfd.Uniform(0, 3, validate_args=False).sample(seed=key1),\n",
    "        'b'  : tfd.Uniform(0, 3, validate_args=False).sample(seed=key2),\n",
    "        'logits' :  tfd.Uniform(-2, 2, validate_args=False).sample(sample_shape=(71,),seed=rng_key)}\n",
    "p, unravel_fn = ravel_pytree(position)\n",
    "print(position)\n",
    "print(p)\n",
    "print(unravel_fn)\n",
    "print(p.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': Array(-0.07475591, dtype=float32), 'b': Array(-0.08839476, dtype=float32), 'logits': Array([-0.04861826, -1.8656483 ,  0.34041414,  0.18445395,  0.5349512 ,\n",
      "        0.95408654, -3.0757098 , -1.1467196 ,  1.19135   , -0.2820422 ,\n",
      "       -1.685374  ,  1.500075  , -0.13415635,  0.14802201,  1.5196137 ,\n",
      "       -0.00603148, -0.7331637 , -1.5049448 , -0.73264414, -2.600379  ,\n",
      "        1.4972322 ,  0.01504064,  1.7474204 , -2.1307764 , -1.1750268 ,\n",
      "       -2.8646846 , -2.4126124 ,  0.9970587 ,  1.4150826 , -1.564853  ,\n",
      "        0.9212672 ,  1.4154911 , -0.6433712 , -0.2716634 , -1.8312318 ,\n",
      "       -1.8919958 , -1.7231661 , -0.6976004 , -0.23184645,  0.37453178,\n",
      "        2.238924  ,  2.4347796 , -4.047677  ,  0.7205491 , -4.6773086 ,\n",
      "        0.37422583, -0.48042172, -0.38190788, -1.366039  , -0.40215185,\n",
      "       -0.14671317, -0.3983269 , -0.50269324,  0.30281428,  2.2282526 ,\n",
      "       -2.3805358 , -1.4691638 ,  0.09317751, -1.2908531 ,  1.7081944 ,\n",
      "       -1.8053311 , -2.2225502 ,  1.6838262 ,  0.11178557,  1.5792245 ,\n",
      "       -0.63167435, -0.42817688, -0.8882376 , -0.36921385, -0.34640792,\n",
      "        0.6582328 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import normal\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from functools import partial\n",
    "from jax import jit,lax\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "n_rat_tumors = 71\n",
    "# inverse_mass_matrix = jnp.array([0.5] * (73))\n",
    "# print(inverse_mass_matrix)\n",
    "# print(jnp.sqrt(jnp.reciprocal(inverse_mass_matrix)))\n",
    "@partial(jit, static_argnames=(\"precision\",), inline=True)\n",
    "def linear_map(diag_or_dense_a, b, *, precision=\"highest\"):\n",
    "    dtype = jnp.result_type(diag_or_dense_a.dtype, b.dtype)\n",
    "    diag_or_dense_a = diag_or_dense_a.astype(dtype)\n",
    "    b = b.astype(dtype)\n",
    "    ndim = jnp.ndim(diag_or_dense_a)\n",
    "\n",
    "    if ndim <= 1:\n",
    "        return lax.mul(diag_or_dense_a, b)\n",
    "    else:\n",
    "        return lax.dot(diag_or_dense_a, b, precision=precision)\n",
    "    \n",
    "def init_param_fn(seed):\n",
    "    \"\"\"\n",
    "    initialize a, b & logits\n",
    "    \"\"\"\n",
    "    key1, key2 = jax.random.split(seed)\n",
    "    return {\n",
    "        \"a\": tfd.Uniform(0, 3).sample(seed=key1),\n",
    "        \"b\": tfd.Uniform(0, 3).sample(seed=key2),\n",
    "        \"logits\": tfd.Uniform(-2, 2).sample(n_rat_tumors, seed),\n",
    "    }\n",
    "\n",
    "rng_key = jax.random.PRNGKey(82)\n",
    "position = init_param_fn(rng_key)\n",
    "inverse_mass_matrix = jnp.array([0.5] * (73))\n",
    "sigma = jnp.sqrt(jnp.reciprocal(inverse_mass_matrix))\n",
    "p, unravel_fn = ravel_pytree(position)\n",
    "sample = normal(rng_key, shape=p.shape, dtype=p.dtype)\n",
    "print(unravel_fn(0.0 + linear_map(sigma, sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5718,  0.2643,  1.0009,  0.0569, -0.5621, -0.6192, -0.5267,  0.2028,\n",
      "        -0.9483, -0.1262,  1.1488, -0.5866,  0.4012,  0.2000,  0.4527,  0.7122,\n",
      "        -0.0744, -0.6050, -0.1402, -0.0629, -0.0578,  0.2841, -0.0971, -0.6210,\n",
      "         0.3933, -0.6801, -0.1018, -0.0279,  0.2880,  0.1497, -1.1574, -0.1302,\n",
      "        -0.3572,  0.4622,  0.0133,  0.3708,  0.0758, -1.0541, -0.4030,  0.0652,\n",
      "         0.2437,  0.3255,  0.3924, -1.2665, -0.5735, -0.0120,  0.2569, -0.2013,\n",
      "        -0.1668,  0.9001,  0.7111, -0.0732, -0.7182,  0.1098,  0.7383,  0.2254,\n",
      "        -0.4604,  0.5400, -0.4769, -0.0024, -0.1629,  0.2353,  0.0550,  0.7600,\n",
      "        -0.3215,  0.1037,  0.3392, -0.1784, -0.6881,  0.5868,  0.4467])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# print(f'means : {torch.zeros(71)}')\n",
    "# print(f'covariance : {torch.eye(71)}')\n",
    "dist = torch.distributions.normal.Normal(loc=torch.zeros(71),scale=0.5*torch.ones(71))\n",
    "print(dist.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1056.9024, dtype=torch.float64)\n",
      "tensor(-956.7878, dtype=torch.float64)\n",
      "tensor(100.1146, dtype=torch.float64)\n",
      "(tensor([100.1146, 100.1146], dtype=torch.float64),)\n"
     ]
    }
   ],
   "source": [
    "# theta\n",
    "from typing import NamedTuple\n",
    "import torch\n",
    "\n",
    "n_of_positives = torch.tensor( [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 5, 2, 5, 3, 2, 7, 7, 3, 3, 2, 9, 10, 4, 4, 4, 4, 4, 4, 4, 10, 4, 4, 4, 5, 11, 12, 5, 5, 6, 5, 6, 6, 6, 6, 16, 15, 15, 9, 4, ], dtype=torch.float32)\n",
    "group_size = torch.tensor( [ 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18, 18, 17, 20, 20, 20, 20, 19, 19, 18, 18, 25, 24, 23, 20, 20, 20, 20, 20, 20, 10, 49, 19, 46, 27, 17, 49, 47, 20, 20, 13, 48, 50, 20, 20, 20, 20, 20, 20, 20, 48, 19, 19, 19, 22, 46, 49, 20, 20, 23, 19, 22, 20, 20, 20, 52, 46, 47, 24, 14, ], dtype=torch.float32)\n",
    "\n",
    "jacobian_fn = torch.func.jacfwd(torch.nn.functional.sigmoid)\n",
    "jacobian_fn_softplus = torch.func.jacfwd(torch.nn.functional.softplus)\n",
    "\n",
    "class Position(NamedTuple):\n",
    "    a: torch.Tensor\n",
    "    b: torch.Tensor\n",
    "    logits: torch.Tensor\n",
    "\n",
    "def joint_log_prob(params: Position) -> torch.Tensor:\n",
    "    a, b, logits = params.a, params.b, params.logits\n",
    "\n",
    "    thetas = torch.nn.functional.sigmoid(logits)\n",
    "    a = torch.nn.functional.softplus(a)\n",
    "    b = torch.nn.functional.softplus(b)\n",
    "\n",
    "    log_det_jacob = torch.sum(\n",
    "        torch.vmap(lambda logit: torch.log(torch.abs(torch.linalg.det(jacobian_fn(logit.reshape(1, 1))))))(logits)\n",
    "    )\n",
    "\n",
    "    # improper prior for a,b\n",
    "    logprob_ab = torch.log(torch.float_power(a + b, -2.5))\n",
    "\n",
    "    # logprob prior of theta\n",
    "    logprob_thetas = torch.distributions.beta.Beta(a, b).log_prob(thetas).sum()\n",
    "\n",
    "    apply_data =  torch.vmap(lambda y, N, theta: torch.distributions.binomial.Binomial(N, probs=theta, validate_args=False).log_prob(y))\n",
    "    # loglikelihood of y\n",
    "    logprob_y = torch.sum(\n",
    "       apply_data(n_of_positives, group_size, thetas)\n",
    "    )\n",
    "    return logprob_ab + logprob_thetas + logprob_y + log_det_jacob\n",
    "\n",
    "\n",
    "# def update_fn(current_tensor:torch.Tensor):\n",
    "#     return current_tensor*torch.distributions.uniform.Uniform(0, 3, validate_args=False).sample(sample_shape=(current_tensor.shape[-1],))\n",
    "\n",
    "pos1 = Position(\n",
    "        a = torch.distributions.uniform.Uniform(0, 3, validate_args=False).sample(),\n",
    "        b = torch.distributions.uniform.Uniform(0, 3, validate_args=False).sample(),\n",
    "        logits =  torch.distributions.uniform.Uniform(-2, 2, validate_args=False).sample(sample_shape=(71,)),\n",
    "    )\n",
    "pos2 = Position(\n",
    "        a = torch.distributions.uniform.Uniform(0, 3, validate_args=False).sample(),\n",
    "        b = torch.distributions.uniform.Uniform(0, 3, validate_args=False).sample(),\n",
    "        logits =  torch.distributions.uniform.Uniform(-2, 2, validate_args=False).sample(sample_shape=(71,)),\n",
    "    )\n",
    "print(joint_log_prob(pos1))\n",
    "print(joint_log_prob(pos2))\n",
    "print(torch.subtract(joint_log_prob(pos2),joint_log_prob(pos1)))\n",
    "print(torch.gradient(torch.tensor([joint_log_prob(pos1),joint_log_prob(pos2)])))\n",
    "# # print(len(pos.a.shape))\n",
    "# print(pos.a.unsqueeze(0).shape[-1])\n",
    "# print(pos.logits.unsqueeze(0).shape[-1])\n",
    "# pos_update_dict = {}\n",
    "# for name,value in zip(pos._fields,pos):\n",
    "#     pos_update_dict[name] = update_fn(value)\n",
    "\n",
    "# new_pos = Position(**pos_update_dict)\n",
    "# print(new_pos)\n",
    "# torch.gradient(torch.tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.9589e-01, -1.8276e+00,  1.3651e+00, -1.9453e+00],\n",
      "        [-6.9609e-01, -1.9410e+00, -1.8210e+00, -1.9413e+00],\n",
      "        [-6.2078e-01, -6.7969e-01, -5.5754e-01,  1.3908e+00],\n",
      "        [ 1.9284e+00, -3.4854e-01, -3.5880e-01, -1.4026e+00],\n",
      "        [ 1.5286e+00, -5.2742e-01,  8.7124e-01,  1.0254e+00],\n",
      "        [-8.8449e-01, -1.3673e+00, -3.7122e-01, -8.2427e-01],\n",
      "        [ 1.1585e+00,  3.3126e-02,  1.4649e+00, -8.7469e-01],\n",
      "        [-1.5747e+00, -9.2794e-01,  2.5389e-01,  1.0795e+00],\n",
      "        [-7.1296e-01,  1.5220e+00,  2.6638e-01,  1.3141e+00],\n",
      "        [ 6.8450e-01, -1.1606e+00, -3.6868e-01, -1.9550e+00],\n",
      "        [ 1.5599e+00,  1.2254e+00, -3.9266e-01, -1.8518e+00],\n",
      "        [-1.6191e+00,  1.0354e+00,  6.1860e-01,  1.0131e+00],\n",
      "        [-7.7582e-01, -3.2102e-01,  1.0088e+00,  1.5622e+00],\n",
      "        [-1.8697e+00, -6.8815e-01, -1.2677e+00, -1.9908e+00],\n",
      "        [-1.2291e+00, -1.9938e+00, -1.8720e+00, -1.7855e+00],\n",
      "        [-1.8059e+00, -1.8125e+00,  6.5918e-01,  1.2546e+00],\n",
      "        [-1.1305e+00, -3.2323e-01, -3.3371e-01,  1.7146e+00],\n",
      "        [ 1.9200e+00,  5.1808e-02,  1.0032e-01,  4.7163e-01],\n",
      "        [-1.3234e+00,  3.1799e-01, -1.3308e+00, -1.3969e-01],\n",
      "        [ 6.6497e-01,  1.9508e+00, -7.2743e-01, -1.6435e+00],\n",
      "        [-9.0217e-01,  4.5630e-01, -7.0328e-02,  1.1141e-01],\n",
      "        [ 8.5492e-01, -9.0318e-01,  8.0467e-01, -1.1725e-01],\n",
      "        [-1.6606e+00,  1.2812e+00, -1.4094e+00, -3.5982e-01],\n",
      "        [-3.6593e-01, -1.6380e+00, -6.7990e-01,  7.9696e-01],\n",
      "        [ 1.0974e+00, -5.5500e-02,  1.4130e+00, -3.6699e-01],\n",
      "        [ 8.1803e-01,  1.2773e+00, -2.7077e-01,  1.6408e+00],\n",
      "        [-1.8748e+00,  1.2015e+00, -1.6114e+00,  1.0954e+00],\n",
      "        [ 1.0385e+00, -1.0341e+00,  1.9760e+00, -1.8518e+00],\n",
      "        [ 1.3183e+00, -8.8570e-01,  1.7360e+00,  1.7186e+00],\n",
      "        [ 1.2268e+00, -2.4467e-01, -1.8061e+00,  3.4373e-01],\n",
      "        [ 6.0397e-01,  8.7452e-01,  6.1984e-01, -1.5058e+00],\n",
      "        [-1.0374e+00, -3.2566e-02,  1.1671e-03, -1.2466e+00],\n",
      "        [ 3.8532e-01,  1.8450e+00, -8.4118e-01,  1.9111e+00],\n",
      "        [-1.3229e+00, -1.9913e+00,  1.5309e+00, -3.2402e-01],\n",
      "        [-4.5192e-01,  7.0829e-02,  1.9121e+00, -3.7713e-01],\n",
      "        [ 1.4556e+00,  3.2675e-01,  9.3339e-01, -4.1498e-01],\n",
      "        [-9.9575e-02, -1.4261e+00,  4.2460e-01,  1.8036e+00],\n",
      "        [-1.1263e+00,  9.2121e-01,  1.7222e+00,  1.0963e-01],\n",
      "        [ 8.9710e-01,  5.3391e-01,  6.8080e-01, -4.6928e-01],\n",
      "        [ 3.4966e-01, -1.3840e+00,  1.0336e+00,  7.1825e-01],\n",
      "        [-1.5809e+00, -1.6423e+00,  9.0337e-01, -1.3377e+00],\n",
      "        [ 1.0839e+00,  1.6428e+00,  5.6497e-01,  1.1056e+00],\n",
      "        [ 7.4999e-01,  5.8101e-01,  1.1429e-01, -1.8015e+00],\n",
      "        [ 5.4812e-01, -1.5355e+00,  1.0007e+00,  2.2632e-01],\n",
      "        [ 1.2350e+00, -3.6905e-01, -1.3904e+00, -4.9020e-01],\n",
      "        [ 1.9930e+00, -1.7899e+00, -7.1350e-01, -2.2428e-01],\n",
      "        [-9.7094e-01,  1.2718e+00, -2.1099e-02, -5.7751e-01],\n",
      "        [-6.3016e-01, -5.5190e-02,  9.7935e-01,  1.8956e+00],\n",
      "        [-2.0358e-01, -2.3577e-01, -1.9093e-01,  1.2568e+00],\n",
      "        [-9.4008e-03, -1.1978e+00,  4.2146e-01,  2.5392e-01],\n",
      "        [-1.6869e+00,  1.2281e+00, -1.6893e+00,  1.2618e+00],\n",
      "        [-4.3140e-01,  8.3579e-02,  1.0661e+00,  1.0338e+00],\n",
      "        [-5.0206e-01, -3.9187e-01, -3.4800e-01, -1.5294e+00],\n",
      "        [ 1.8177e+00,  7.9007e-01,  7.2044e-01, -2.9033e-02],\n",
      "        [-3.6852e-01,  3.3575e-01, -9.2255e-01,  1.4975e+00],\n",
      "        [-1.3112e+00, -8.2847e-01,  1.5941e+00, -3.7182e-02],\n",
      "        [-1.6500e-01,  1.1588e+00,  6.0250e-01, -1.6590e+00],\n",
      "        [ 1.2016e+00,  8.0772e-01, -1.9629e+00,  5.3791e-01],\n",
      "        [-4.1537e-01, -1.3049e+00,  1.5328e+00,  1.8944e+00],\n",
      "        [ 1.1718e+00,  1.8739e+00, -1.4040e+00,  1.5151e+00],\n",
      "        [-4.9890e-01,  1.4360e+00,  8.9140e-01, -1.0450e+00],\n",
      "        [ 2.4117e-01, -1.3007e+00,  1.4566e+00,  1.5929e+00],\n",
      "        [-1.2395e+00, -1.6702e+00,  1.3483e+00, -1.8245e+00],\n",
      "        [-1.5572e+00,  5.2986e-01, -7.9787e-01,  9.2562e-01],\n",
      "        [-1.5927e+00, -1.5516e+00,  7.3995e-01,  1.2645e+00],\n",
      "        [-1.6217e+00, -3.5842e-01,  1.8103e+00, -3.4048e-01],\n",
      "        [-1.4839e+00, -1.2224e+00,  9.7781e-01,  6.3765e-01],\n",
      "        [-1.4487e-01,  7.1531e-01, -1.5433e+00,  7.2147e-01],\n",
      "        [-3.2007e-01,  1.0792e-01,  1.0265e-01,  1.8516e+00],\n",
      "        [ 1.0430e-01, -1.1805e+00, -1.6971e+00,  1.6305e+00],\n",
      "        [-6.8942e-01, -9.0859e-03,  1.3343e+00, -1.5019e+00]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from torch.func import grad\n",
    "\n",
    "# x = torch.randn([])\n",
    "# cos_x = grad(lambda x: torch.sin(x))(x)\n",
    "# assert torch.allclose(cos_x, x.cos())\n",
    "# print(x,x.cos(),cos_x)\n",
    "# torch.distributions.uniform.Uniform(0, 3, validate_args=False).sample(sample_shape=(4,))\n",
    "sample = torch.distributions.uniform.Uniform(-2, 2, validate_args=False).sample(sample_shape=(71,4))\n",
    "print(sample)\n",
    "                                                                                \n",
    "                                                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 71)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reshape total size must be unchanged, got new_sizes (4, 1, 1) for shape (4, 71).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng_key, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     61\u001b[0m initial_states \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(\u001b[38;5;28;01mlambda\u001b[39;00m seed: init(init_param_fn(seed)))(keys)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjoint_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_states\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mjoint_log_prob\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     21\u001b[0m b \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftplus(b)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m log_det_jacob \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjacobian_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(log_det_jacob\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m log_det_jacob_priors \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39mabs(jacobian_fn(a))) \u001b[38;5;241m+\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39mabs(jacobian_fn(b)))\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mjoint_log_prob.<locals>.<lambda>\u001b[0;34m(logit)\u001b[0m\n\u001b[1;32m     21\u001b[0m b \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftplus(b)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m log_det_jacob \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m---> 24\u001b[0m     jax\u001b[38;5;241m.\u001b[39mvmap(\u001b[38;5;28;01mlambda\u001b[39;00m logit: jnp\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39mabs(jnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mdet(jacobian_fn(\u001b[43mlogit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)))))(logits)\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(log_det_jacob\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m log_det_jacob_priors \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39mabs(jacobian_fn(a))) \u001b[38;5;241m+\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39mabs(jacobian_fn(b)))\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:733\u001b[0m, in \u001b[0;36m_forward_method_to_aval.<locals>.meth\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeth\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 733\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:143\u001b[0m, in \u001b[0;36m_reshape\u001b[0;34m(a, order, *args)\u001b[0m\n\u001b[1;32m    141\u001b[0m newshape \u001b[38;5;241m=\u001b[39m _compute_newshape(a, args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m args)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m order \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m order \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    145\u001b[0m   dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(a\u001b[38;5;241m.\u001b[39mndim)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "    \u001b[0;31m[... skipping hidden 31 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/lax/lax.py:3378\u001b[0m, in \u001b[0;36m_reshape_shape_rule\u001b[0;34m(operand, new_sizes, dimensions)\u001b[0m\n\u001b[1;32m   3375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdynamic_shapes\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   3376\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m math\u001b[38;5;241m.\u001b[39mprod(np\u001b[38;5;241m.\u001b[39mshape(operand)) \u001b[38;5;241m==\u001b[39m math\u001b[38;5;241m.\u001b[39mprod(new_sizes)):\n\u001b[1;32m   3377\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreshape total size must be unchanged, got new_sizes \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 3378\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(new_sizes, np\u001b[38;5;241m.\u001b[39mshape(operand)))\n\u001b[1;32m   3379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dimensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(dimensions) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(np\u001b[38;5;241m.\u001b[39mndim(operand))):\n",
      "\u001b[0;31mTypeError\u001b[0m: reshape total size must be unchanged, got new_sizes (4, 1, 1) for shape (4, 71)."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "n_of_positives = jnp.array( [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 5, 2, 5, 3, 2, 7, 7, 3, 3, 2, 9, 10, 4, 4, 4, 4, 4, 4, 4, 10, 4, 4, 4, 5, 11, 12, 5, 5, 6, 5, 6, 6, 6, 6, 16, 15, 15, 9, 4, ], dtype=jnp.float32)\n",
    "group_size = jnp.array( [ 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18, 18, 17, 20, 20, 20, 20, 19, 19, 18, 18, 25, 24, 23, 20, 20, 20, 20, 20, 20, 10, 49, 19, 46, 27, 17, 49, 47, 20, 20, 13, 48, 50, 20, 20, 20, 20, 20, 20, 20, 48, 19, 19, 19, 22, 46, 49, 20, 20, 23, 19, 22, 20, 20, 20, 52, 46, 47, 24, 14, ], dtype=jnp.float32)\n",
    "n_rat_tumors = len(group_size)  # number of different kind of rat tumors\n",
    "\n",
    "\n",
    "tfd = tfp.distributions\n",
    "jacobian_fn = jax.jacfwd(jax.nn.sigmoid)\n",
    "jacobian_fn_softplus = jax.jacfwd(jax.nn.softplus)\n",
    "\n",
    "def joint_log_prob(params):\n",
    "    a, b, logits = params[\"a\"], params[\"b\"], params[\"logits\"]\n",
    "\n",
    "    # HMC requires unconstrained sample space, thus it samples logits and we apply change of variable to\n",
    "    # get theta from logits\n",
    "    thetas = jax.nn.sigmoid(logits)\n",
    "    a = jax.nn.softplus(a)\n",
    "    b = jax.nn.softplus(b)\n",
    "    print(logits.shape)\n",
    "    log_det_jacob = jnp.sum(\n",
    "        jax.vmap(lambda logit: jnp.log(jnp.abs(jnp.linalg.det(jacobian_fn(logit.reshape(1, 1))))))(logits)\n",
    "    )\n",
    "    print(log_det_jacob.shape)\n",
    "\n",
    "    log_det_jacob_priors = jnp.log(jnp.abs(jacobian_fn(a))) + jnp.log(jnp.abs(jacobian_fn(b)))\n",
    "\n",
    "    # improper prior for a,b\n",
    "    logprob_ab = jnp.log(jnp.power(a + b, -2.5))\n",
    "\n",
    "    # logprob prior of theta\n",
    "    logprob_thetas = tfd.Beta(a, b).log_prob(thetas).sum()\n",
    "\n",
    "    # loglikelihood of y\n",
    "    logprob_y = jnp.sum(\n",
    "        jax.vmap(lambda y, N, theta: tfd.Binomial(N, probs=theta).log_prob(y))(n_of_positives, group_size, thetas)\n",
    "    )\n",
    "    return logprob_ab + logprob_thetas + logprob_y + log_det_jacob + log_det_jacob_priors\n",
    "\n",
    "    # return logprob_ab + logprob_thetas + logprob_y + log_det_jacob\n",
    "\n",
    "rng_key = jax.random.PRNGKey(82)\n",
    "\n",
    "def init_param_fn(seed):\n",
    "    \"\"\"\n",
    "    initialize a, b & logits\n",
    "    \"\"\"\n",
    "    key1, key2 = jax.random.split(seed)\n",
    "    return {\n",
    "        \"a\": tfd.Uniform(0, 3).sample(seed=key1),\n",
    "        \"b\": tfd.Uniform(0, 3).sample(seed=key2),\n",
    "        \"logits\": tfd.Uniform(-2, 2).sample(71, seed),\n",
    "    }\n",
    "\n",
    "def init(val):\n",
    "    return val\n",
    "\n",
    "keys = jax.random.split(rng_key, 4)\n",
    "initial_states = jax.vmap(lambda seed: init(init_param_fn(seed)))(keys)\n",
    "print(joint_log_prob(initial_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2210])\n",
      "tensor([True])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "accpt = torch.min(torch.exp(torch.tensor([-1.5094884365301])), torch.ones(1))\n",
    "print(accpt)\n",
    "accept_condition = torch.distributions.Bernoulli(accpt).sample()\n",
    "print(accept_condition.bool())\n",
    "if accept_condition.bool():\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "lax.fori_loop(0, num_integrations_steps, one_step, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5000]])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3])\n",
      "tensor(0.5103)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(0.5*torch.eye(3))\n",
    "covariance = 0.5*torch.eye(3)\n",
    "velocity = torch.distributions.MultivariateNormal(torch.zeros(covariance.shape[-1]),covariance).sample()\n",
    "print(covariance.shape)\n",
    "# velocity = velocity.unsqueeze(0)\n",
    "print(velocity.shape)\n",
    "print(0.5*torch.matmul(velocity, torch.matmul(covariance, velocity.T)))\n",
    "# 0.5*torch.dot(velocity,torch.multiply(covariance, velocity))\n",
    "# print(torch.matmul(torch.tensor([1,2,3]), torch.tensor([[1],[1],[1]])))\n",
    "\n",
    "print(torch.where(accept_condition.bool(), HMCState(proposal_energy_parameters,None), torch.ten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "import os\n",
    "import blackjax\n",
    "from jax import jit, lax\n",
    "\n",
    "\n",
    "# from probml_utils.blackjax_utils import arviz_trace_from_states, inference_loop_multiple_chains\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "try:\n",
    "    from tensorflow_probability.substrates import jax as tfp\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qqq tensorflow_probability\n",
    "    from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "import arviz as az\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2) (50,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(50, 2), (50,)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/util.py:284\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/util.py:277\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 277\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/lax/lax.py:152\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 152\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(50, 2), (50,)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m rng_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     36\u001b[0m initial_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m: weights_prior\u001b[38;5;241m.\u001b[39msample(seed\u001b[38;5;241m=\u001b[39mrng_key, sample_shape\u001b[38;5;241m=\u001b[39m(n_chains, \u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m: bias_prior\u001b[38;5;241m.\u001b[39msample(seed\u001b[38;5;241m=\u001b[39mrng_key, sample_shape\u001b[38;5;241m=\u001b[39m(n_chains,)),\n\u001b[1;32m     39\u001b[0m }\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjoint_logprob_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_params\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# inverse_mass_matrix = jnp.array([0.5] * 3)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# step_size = 0.01\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# nuts = blackjax.nuts(joint_logprob_fn, step_size, inverse_mass_matrix)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#     return (states, infos)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 29\u001b[0m, in \u001b[0;36mjoint_logprob_fn\u001b[0;34m(params, inputs, outputs)\u001b[0m\n\u001b[1;32m     27\u001b[0m logits \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;241m@\u001b[39m weights \u001b[38;5;241m+\u001b[39m bias\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape,outputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 29\u001b[0m log_lik \u001b[38;5;241m=\u001b[39m \u001b[43mtfd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(log_lik)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_prior \u001b[38;5;241m+\u001b[39m log_lik\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1287\u001b[0m, in \u001b[0;36mDistribution.log_prob\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_prob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Log probability density/mass function.\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \n\u001b[1;32m   1278\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;124;03m      values of type `self.dtype`.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/distribution.py:1269\u001b[0m, in \u001b[0;36mDistribution._call_log_prob\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_and_control_scope(name, value, kwargs):\n\u001b[1;32m   1268\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_log_prob\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_prob\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prob(value, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/tensorflow_probability/substrates/jax/distributions/bernoulli.py:122\u001b[0m, in \u001b[0;36mBernoulli._log_prob\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    120\u001b[0m log_probs0, log_probs1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outcome_log_probs()\n\u001b[1;32m    121\u001b[0m event \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(event, log_probs0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply_no_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    123\u001b[0m         tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mmultiply_no_nan(log_probs1, event))\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/tensorflow_probability/python/internal/backend/jax/_utils.py:62\u001b[0m, in \u001b[0;36mcopy_docstring.<locals>.wrap\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@wrapt\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(wrapped, instance, args, kwargs):\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m instance, wrapped\n\u001b[0;32m---> 62\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/tensorflow_probability/python/internal/backend/jax/numpy_math.py:814\u001b[0m, in \u001b[0;36m_multiply_no_nan\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    811\u001b[0m dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(x, y)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# TODO(b/146385087): The gradient should be\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# `lambda dz: [multiply_no_nan(dz, y), multiply_no_nan(x, dz)]`.\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mequal(y, \u001b[38;5;241m0.\u001b[39m), np\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mdtype), \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/numpy/ufuncs.py:96\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x1, x2, \u001b[38;5;241m/\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m   x1, x2 \u001b[38;5;241m=\u001b[39m \u001b[43mpromote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[38;5;28;01mif\u001b[39;00m x1\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_ \u001b[38;5;28;01melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/numpy/util.py:363\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    361\u001b[0m check_arraylike(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    362\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpromote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpromote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/numpy/util.py:248\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnumpy_rank_promotion\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 248\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    166\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(50, 2), (50,)]"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "df_iris = pd.DataFrame(data=iris.data, columns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])\n",
    "df_iris[\"species\"] = pd.Series(iris.target_names[y], dtype=\"category\")\n",
    "\n",
    "\n",
    "df = df_iris.query(\"species == ('setosa', 'versicolor')\")\n",
    "\n",
    "df_balanced = df[25:75]\n",
    "df_balanced.head()\n",
    "X = df_balanced[[\"sepal_length\", \"sepal_width\"]].values  # features\n",
    "# X.shape\n",
    "y = pd.Categorical(df_balanced[\"species\"]).codes\n",
    "\n",
    "bias_prior = tfd.Normal(0, 20)\n",
    "weights_prior = tfd.Normal(0, 2)\n",
    "\n",
    "def joint_logprob_fn(params, inputs=X, outputs=y):\n",
    "    weights = params[\"weights\"]\n",
    "    bias = params[\"bias\"]\n",
    "    # prior logprob\n",
    "    log_prior = weights_prior.log_prob(weights).sum() + bias_prior.log_prob(bias)\n",
    "    # likelihood logprob\n",
    "    logits = inputs @ weights + bias\n",
    "    print(logits.shape,outputs.shape)\n",
    "    log_lik = tfd.Bernoulli(logits=logits).log_prob(outputs).sum()\n",
    "    print(log_lik)\n",
    "    return log_prior + log_lik\n",
    "\n",
    "# initialize the params for two chains\n",
    "n_chains = 2\n",
    "rng_key = jax.random.PRNGKey(10)\n",
    "initial_params = {\n",
    "    \"weights\": weights_prior.sample(seed=rng_key, sample_shape=(n_chains, 2)),\n",
    "    \"bias\": bias_prior.sample(seed=rng_key, sample_shape=(n_chains,)),\n",
    "}\n",
    "print(joint_logprob_fn(initial_params))\n",
    "\n",
    "\n",
    "# inverse_mass_matrix = jnp.array([0.5] * 3)\n",
    "# step_size = 0.01\n",
    "# nuts = blackjax.nuts(joint_logprob_fn, step_size, inverse_mass_matrix)\n",
    "\n",
    "# initial_states = jax.vmap(nuts.init)(initial_params)\n",
    "# kernel = jax.jit(nuts.step)\n",
    "\n",
    "# def inference_loop_multiple_chains(rng_key, kernel, initial_states, num_samples, num_chains):\n",
    "#     @jax.jit\n",
    "#     def one_step(states, rng_key):\n",
    "#         keys = jax.random.split(rng_key, num_chains)\n",
    "#         states, infos = jax.vmap(kernel)(keys, states)\n",
    "#         return states, (states, infos)\n",
    "    \n",
    "#     keys = jax.random.split(rng_key, num_samples)\n",
    "#     _, (states, infos) = lax.scan(one_step, initial_states, keys)\n",
    "\n",
    "#     return (states, infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "clip() got an unexpected keyword argument 'max'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 58\u001b[0m, in \u001b[0;36minference_loop_multiple_chains\u001b[0;34m(rng_key, kernel, initial_states, num_samples, num_chains)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m states, (states, infos)\n\u001b[1;32m     57\u001b[0m keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng_key, num_samples)\n\u001b[0;32m---> 58\u001b[0m _, (states, infos) \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (states, infos)\n",
      "    \u001b[0;31m[... skipping hidden 21 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 54\u001b[0m, in \u001b[0;36minference_loop_multiple_chains.<locals>.one_step\u001b[0;34m(states, rng_key)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step\u001b[39m(states, rng_key):\n\u001b[1;32m     53\u001b[0m     keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng_key, num_chains)\n\u001b[0;32m---> 54\u001b[0m     states, infos \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m states, (states, infos)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/blackjax/mcmc/nuts.py:220\u001b[0m, in \u001b[0;36mas_top_level_api.<locals>.step_fn\u001b[0;34m(rng_key, state)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_fn\u001b[39m(rng_key: PRNGKey, state):\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdensity_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43minverse_mass_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_num_doublings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/blackjax/mcmc/nuts.py:141\u001b[0m, in \u001b[0;36mbuild_kernel.<locals>.kernel\u001b[0;34m(rng_key, state, logdensity_fn, step_size, inverse_mass_matrix, max_num_doublings)\u001b[0m\n\u001b[1;32m    136\u001b[0m momentum \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39msample_momentum(key_momentum, position)\n\u001b[1;32m    138\u001b[0m integrator_state \u001b[38;5;241m=\u001b[39m integrators\u001b[38;5;241m.\u001b[39mIntegratorState(\n\u001b[1;32m    139\u001b[0m     position, momentum, logdensity, logdensity_grad\n\u001b[1;32m    140\u001b[0m )\n\u001b[0;32m--> 141\u001b[0m proposal, info \u001b[38;5;241m=\u001b[39m \u001b[43mproposal_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_integrator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegrator_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m proposal \u001b[38;5;241m=\u001b[39m hmc\u001b[38;5;241m.\u001b[39mHMCState(\n\u001b[1;32m    143\u001b[0m     proposal\u001b[38;5;241m.\u001b[39mposition, proposal\u001b[38;5;241m.\u001b[39mlogdensity, proposal\u001b[38;5;241m.\u001b[39mlogdensity_grad\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proposal, info\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/blackjax/mcmc/nuts.py:305\u001b[0m, in \u001b[0;36miterative_nuts_proposal.<locals>.propose\u001b[0;34m(rng_key, initial_state, step_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m initial_trajectory \u001b[38;5;241m=\u001b[39m trajectory\u001b[38;5;241m.\u001b[39mTrajectory(\n\u001b[1;32m    296\u001b[0m     initial_state,\n\u001b[1;32m    297\u001b[0m     initial_state,\n\u001b[1;32m    298\u001b[0m     initial_state\u001b[38;5;241m.\u001b[39mmomentum,\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    300\u001b[0m )\n\u001b[1;32m    301\u001b[0m initial_expansion_state \u001b[38;5;241m=\u001b[39m trajectory\u001b[38;5;241m.\u001b[39mDynamicExpansionState(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;241m0\u001b[39m, initial_proposal, initial_trajectory, initial_termination_state\n\u001b[1;32m    303\u001b[0m )\n\u001b[0;32m--> 305\u001b[0m expansion_state, info \u001b[38;5;241m=\u001b[39m \u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_expansion_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_energy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m is_diverging, is_turning \u001b[38;5;241m=\u001b[39m info\n\u001b[1;32m    309\u001b[0m num_doublings, sampled_proposal, new_trajectory, _ \u001b[38;5;241m=\u001b[39m expansion_state\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/blackjax/mcmc/trajectory.py:611\u001b[0m, in \u001b[0;36mdynamic_multiplicative_expansion.<locals>.expand\u001b[0;34m(rng_key, initial_expansion_state, initial_energy, step_size)\u001b[0m\n\u001b[1;32m    607\u001b[0m     info \u001b[38;5;241m=\u001b[39m (is_diverging, is_turning_subtree \u001b[38;5;241m|\u001b[39m is_turning)\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (new_state, info)\n\u001b[0;32m--> 611\u001b[0m expansion_state, (is_diverging, is_turning) \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_keep_expanding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand_once\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_expansion_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expansion_state, (is_diverging, is_turning)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/blackjax/mcmc/trajectory.py:581\u001b[0m, in \u001b[0;36mdynamic_multiplicative_expansion.<locals>.expand.<locals>.expand_once\u001b[0;34m(loop_state)\u001b[0m\n\u001b[1;32m    571\u001b[0m     _, proposal, new_proposal \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Proposal(\n\u001b[1;32m    573\u001b[0m         proposal\u001b[38;5;241m.\u001b[39mstate,\n\u001b[1;32m    574\u001b[0m         proposal\u001b[38;5;241m.\u001b[39menergy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m         ),\n\u001b[1;32m    579\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m updated_proposal \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_diverging\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mis_turning_subtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_sum_log_p_accept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposal_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproposal_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_proposal\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# Is the full trajectory making a U-Turn?\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# We first merge the subtrajectory that was just generated with the\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# trajectory and check the U-Turn criterior on the whole trajectory.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m left_trajectory, right_trajectory \u001b[38;5;241m=\u001b[39m reorder_trajectories(\n\u001b[1;32m    593\u001b[0m     direction, trajectory, new_trajectory\n\u001b[1;32m    594\u001b[0m )\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/blackjax/mcmc/trajectory.py:584\u001b[0m, in \u001b[0;36mdynamic_multiplicative_expansion.<locals>.expand.<locals>.expand_once.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    571\u001b[0m     _, proposal, new_proposal \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Proposal(\n\u001b[1;32m    573\u001b[0m         proposal\u001b[38;5;241m.\u001b[39mstate,\n\u001b[1;32m    574\u001b[0m         proposal\u001b[38;5;241m.\u001b[39menergy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m         ),\n\u001b[1;32m    579\u001b[0m     )\n\u001b[1;32m    581\u001b[0m updated_proposal \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    582\u001b[0m     is_diverging \u001b[38;5;241m|\u001b[39m is_turning_subtree,\n\u001b[1;32m    583\u001b[0m     update_sum_log_p_accept,\n\u001b[0;32m--> 584\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mproposal_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    585\u001b[0m     operand\u001b[38;5;241m=\u001b[39m(proposal_key, proposal, new_proposal),\n\u001b[1;32m    586\u001b[0m )\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# Is the full trajectory making a U-Turn?\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# We first merge the subtrajectory that was just generated with the\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# trajectory and check the U-Turn criterior on the whole trajectory.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m left_trajectory, right_trajectory \u001b[38;5;241m=\u001b[39m reorder_trajectories(\n\u001b[1;32m    593\u001b[0m     direction, trajectory, new_trajectory\n\u001b[1;32m    594\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/blackjax/mcmc/proposal.py:156\u001b[0m, in \u001b[0;36mprogressive_biased_sampling\u001b[0;34m(rng_key, proposal, new_proposal)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprogressive_biased_sampling\u001b[39m(\n\u001b[1;32m    148\u001b[0m     rng_key: PRNGKey, proposal: Proposal, new_proposal: Proposal\n\u001b[1;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Proposal:\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Baised proposal sampling :cite:p:`betancourt2017conceptual`.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    Unlike uniform sampling, biased sampling favors new proposals. It thus\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    biases the transition away from the trajectory's initial state.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     p_accept \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_proposal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproposal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     do_accept \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbernoulli(rng_key, p_accept)\n\u001b[1;32m    158\u001b[0m     new_weight \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mlogaddexp(proposal\u001b[38;5;241m.\u001b[39mweight, new_proposal\u001b[38;5;241m.\u001b[39mweight)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Projects/DeepLearning/.venv/lib/python3.11/site-packages/jax/_src/linear_util.py:191\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    194\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "\u001b[0;31mTypeError\u001b[0m: clip() got an unexpected keyword argument 'max'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rng_key = jax.random.PRNGKey(1)\n",
    "states, infos = inference_loop_multiple_chains(rng_key, kernel, initial_states, 2000, n_chains)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
