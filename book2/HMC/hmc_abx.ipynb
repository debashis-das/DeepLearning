{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, NamedTuple\n",
    "import torch\n",
    "import torch.jit as jit  \n",
    "import arviz as az\n",
    "\n",
    "# theta\n",
    "class Position(NamedTuple):\n",
    "    a: torch.Tensor\n",
    "    b: torch.Tensor\n",
    "    logits: torch.Tensor\n",
    "\n",
    "class EnergyParameters:\n",
    "    #potential\n",
    "    position: Position\n",
    "    potential_energy: torch.Tensor\n",
    "    potential_energy_grad: torch.Tensor\n",
    "    #kinetic\n",
    "    velocity: torch.Tensor\n",
    "    kinetic_energy: torch.Tensor\n",
    "\n",
    "    def __init__(self, potential_energy_fn:Callable):\n",
    "        self.potential_energy_fn = potential_energy_fn\n",
    "    \n",
    "    def set_parameters(self, position:Position):\n",
    "        self.position = position\n",
    "        self.potential_energy = self.potential_energy_fn(position)\n",
    "        self.potential_energy_grad = torch.func.grad(self.potential_energy_fn)(position)\n",
    "        self.velocity, self.precision, self.covariance = self.generate_default_velocity(position)\n",
    "        self.kinetic_energy = self.kinetic_energy_fn()\n",
    "    \n",
    "    def set_velocity(self, velocity:torch.Tensor):\n",
    "        self.velocity = velocity\n",
    "        self.kinetic_energy = self.kinetic_energy_fn()\n",
    "    \n",
    "    def generate_default_velocity(self, position:Position):\n",
    "        precision, covariance = self.generate_inverse_mass_matrix_and_sigma(self.covariance_shape_from_position(position))\n",
    "        return self.generate_random_gaussian(covariance), precision, covariance\n",
    "\n",
    "    def generate_random_gaussian(self, covariance: torch.Tensor):\n",
    "        return torch.distributions.MultivariateNormal(loc=torch.zeros(covariance.shape[-1]),covariance_matrix=covariance).sample()\n",
    "\n",
    "    def covariance_shape_from_position(self, position:Position):\n",
    "        covariance_shape = 0\n",
    "        for value in position:\n",
    "            covariance_shape += value.unsqueeze(0).shape[-1]\n",
    "        return covariance_shape\n",
    "\n",
    "    def generate_inverse_mass_matrix_and_sigma(self, covariance_shape: int):\n",
    "        covariance = torch.eye(covariance_shape)\n",
    "        inverse_mass_matrix = covariance.inverse()\n",
    "        return inverse_mass_matrix, covariance\n",
    "\n",
    "    def kinetic_energy_fn(self):\n",
    "        return 0.5*torch.matmul(self.velocity, torch.matmul(self.covariance, self.velocity.T))\n",
    "\n",
    "def ravel_fn(value: Position):\n",
    "        return torch.cat([current_tensor.ravel() for current_tensor in value])\n",
    "\n",
    "def unravel_fn(values: torch.Tensor, structured_tuple:Position):\n",
    "    shapes = []\n",
    "    output = {}\n",
    "    for struct_val in structured_tuple:\n",
    "        shapes.append(struct_val.unsqueeze(0).shape[-1])\n",
    "    split_tensors = torch.split(values,shapes)\n",
    "    for i,name in enumerate(structured_tuple._fields):\n",
    "        # if else removes inner dimensions of single tensor\n",
    "        output[name] = split_tensors[i] if split_tensors[i].shape[-1] != 1 else split_tensors[i][0]\n",
    "    return Position(**output)\n",
    "\n",
    "class HMCAlgorithm : \n",
    "\n",
    "    def __init__(self, log_density_fn: Callable, step_size=0.011, l=1, n_chains=4):\n",
    "        self.log_density_fn = log_density_fn\n",
    "        self.step_size = step_size\n",
    "        self.l = l\n",
    "        self.energy_parameters = EnergyParameters(log_density_fn)\n",
    "        self.proposal_accepted_count = [0]*n_chains\n",
    "\n",
    "    def updated_position(self, position:Position, step_size:float, inverse_mass_matrix:torch.Tensor, velocity:torch.Tensor):\n",
    "        # print(position)\n",
    "        markov_chain = torch.cat([value.ravel() for value in position])\n",
    "        # torch.matmul(inverse_mass_matrix,velocity) is the kinetic gradient\n",
    "        result = markov_chain + step_size*torch.matmul(inverse_mass_matrix,velocity)\n",
    "        return unravel_fn(result, position)\n",
    "\n",
    "    def updated_velocity(self, velocity:torch.Tensor, step_size:float, next_potential_energy_grad:torch.Tensor, is_half_step_momentum = False):\n",
    "        return torch.subtract(velocity,step_size*(0.5 if is_half_step_momentum else 1)*next_potential_energy_grad)\n",
    "    \n",
    "    def update(self, position: Position, chain_index: int):\n",
    "        delta_energy, proposal_position = self.step_integrator(position)\n",
    "        # nan fix\n",
    "        delta_energy = torch.where(torch.isnan(delta_energy), -torch.inf, delta_energy)\n",
    "        # MH Algo\n",
    "        alpha = torch.min(torch.exp(delta_energy), torch.ones(1))\n",
    "        # print(f'Delta energy : {delta_energy}, {total_prev_energy}, {total_proposal_energy}, {alpha}')\n",
    "        accept_condition = torch.distributions.Bernoulli(alpha).sample()\n",
    "        # print(f'Delta energy : {delta_energy}, {alpha}, {proposal_position.a}, {proposal_position.b}, {self.proposal_accepted_count[chain_index]}')\n",
    "        if accept_condition.bool():\n",
    "            self.proposal_accepted_count[chain_index] += 1\n",
    "            return proposal_position\n",
    "        return position\n",
    "  \n",
    "    def step_integrator(self, position:Position):\n",
    "        self.energy_parameters.set_parameters(position)\n",
    "        prev_potential_energy = self.energy_parameters.potential_energy\n",
    "        prev_kinetic_energy = self.energy_parameters.kinetic_energy\n",
    "        prev_velocity = self.energy_parameters.velocity\n",
    "        prev_potential_energy_grad = self.energy_parameters.potential_energy_grad\n",
    "\n",
    "        prev_half_velocity = torch.subtract(prev_velocity,\n",
    "                                             self.step_size*0.5*ravel_fn(prev_potential_energy_grad))\n",
    "        # print('prev_half_velocity',prev_half_velocity)\n",
    "        self.energy_parameters.set_velocity(prev_half_velocity)\n",
    "\n",
    "        # l_frog_steps\n",
    "        next_position, next_velocity = self.l_frog_steps(prev_half_velocity, position, self.energy_parameters.precision)\n",
    "        \n",
    "        full_step_position = self.updated_position(position=next_position, step_size=self.step_size,\n",
    "                                                   inverse_mass_matrix=self.energy_parameters.precision, velocity=next_velocity)\n",
    "        # print('full_step_position',full_step_position)\n",
    "        self.energy_parameters.set_parameters(full_step_position)\n",
    "        half_step_potential_energy_grad = self.energy_parameters.potential_energy_grad\n",
    "        # print('half_step_potential_energy_grad',half_step_potential_energy_grad)\n",
    "\n",
    "        half_step_velocity = self.updated_velocity(velocity=next_velocity,step_size=self.step_size,\n",
    "                                                   next_potential_energy_grad=ravel_fn(half_step_potential_energy_grad),is_half_step_momentum=True)\n",
    "        # print('half_step_velocity',half_step_velocity)\n",
    "        self.energy_parameters.set_velocity(half_step_velocity)\n",
    "\n",
    "        total_proposal_energy = -self.energy_parameters.potential_energy - self.energy_parameters.kinetic_energy\n",
    "        total_prev_energy = -prev_potential_energy - prev_kinetic_energy\n",
    "        return total_prev_energy - total_proposal_energy, full_step_position\n",
    "\n",
    "    def l_frog_steps(self, prev_half_velocity, prev_position, precision):\n",
    "        for _ in range(self.l):\n",
    "            # print(prev_position)\n",
    "            next_position = self.updated_position(position=prev_position, step_size=self.step_size,\n",
    "                                                  inverse_mass_matrix=precision, velocity=prev_half_velocity)\n",
    "            # print('next_position',next_position)\n",
    "\n",
    "            self.energy_parameters.set_parameters(next_position)\n",
    "            next_potential_energy_grad = self.energy_parameters.potential_energy_grad\n",
    "            # print('next_potential_energy_grad',next_potential_energy_grad)\n",
    "\n",
    "            next_velocity = self.updated_velocity(prev_half_velocity, step_size=self.step_size, next_potential_energy_grad=ravel_fn(next_potential_energy_grad))\n",
    "            # print('next_velocity',next_velocity)\n",
    "            self.energy_parameters.set_velocity(next_velocity)\n",
    "            prev_half_velocity = next_velocity\n",
    "            prev_position = next_position\n",
    "        return next_position,next_velocity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMCState(energy_paramaters=EnergyParameters(position=Position(a=tensor(-2.4434), b=tensor(3.8943), logits=tensor([ 0.6965,  0.4389,  0.2596, -1.6715,  1.5320,  0.9416,  2.9204,  1.4727,\n",
      "         3.3886,  3.5345,  2.2745, -1.8754,  3.2213,  3.7690,  3.0669,  1.2809,\n",
      "         1.5686,  2.1126, -0.0246, -0.8735, -0.3380,  1.7291, -0.0295,  0.5149,\n",
      "         0.9970, -0.7616,  2.4506, -1.0129, -0.0968,  2.9327,  0.5931, -2.0695,\n",
      "        -1.0704,  3.7104, -1.0380,  0.3449,  3.3789,  0.9748,  3.9644,  0.7453,\n",
      "         2.7617, -2.0301,  1.2965,  2.2034, -3.6257,  4.6222,  1.8803, -2.8767,\n",
      "         1.0681,  0.2130, -1.0211, -0.9462,  2.1205, -0.2591,  1.1841,  1.8190,\n",
      "         3.0280,  3.5425,  4.4441,  1.5752,  0.7568, -1.3941,  0.0270,  0.7065,\n",
      "         2.1893,  3.6363,  1.4081, -2.2193, -2.8088,  2.0223, -0.7398])), potential_energy=tensor(-2615.9692, dtype=torch.float64), potential_energy_grad=Position(a=tensor(74.0876), b=tensor(-110.8989), logits=tensor([-15.9333, -14.5071, -13.4641,  -3.7136, -19.6499, -17.1810, -22.6868,\n",
      "        -18.6247, -22.1635, -22.2625, -20.7698,  -2.8406, -21.0704, -20.4408,\n",
      "        -21.8467, -17.6973, -18.7770, -20.3257, -10.2740,  -5.6902,  -8.0742,\n",
      "        -17.5992, -12.2017, -15.4419, -17.6379,  -5.5550, -20.0091,  -4.3101,\n",
      "         -9.3352, -20.7011, -13.3732,  -0.4857,  -8.4485, -20.3651,  -7.9932,\n",
      "        -15.0619, -18.2223, -31.3968, -42.9646, -13.1908, -19.4882,   0.1103,\n",
      "        -31.7477, -38.5444,   3.4608, -19.6808, -16.7382,   2.8037, -13.7765,\n",
      "         -9.1888,  -2.2716,  -4.4574, -16.4509,  -5.9341, -13.5257, -17.2865,\n",
      "        -36.6058, -39.4237, -18.6358, -14.7995, -12.2927,   0.5123,  -7.0910,\n",
      "         -9.9863, -15.4975, -17.2983, -28.9086,  10.1819,  12.1841, -15.6420,\n",
      "         -1.7308])), velocity=tensor([-13.1469,  13.9882,   3.9858,   3.4980,   1.6316,  -1.6882,   2.0743,\n",
      "          3.0264,   5.1297,   3.5729,   7.1284,   6.1277,   4.0470,   0.1833,\n",
      "          7.3778,   7.0523,   6.1440,   2.6238,   5.4754,   6.5891,   3.6749,\n",
      "          1.6145,   0.9720,   4.0124,   1.3465,   2.4432,   2.8659,  -0.0554,\n",
      "          4.0957,   0.8586,   0.5988,   5.7832,   2.2425,  -2.6779,   0.6599,\n",
      "          5.6134,   0.7587,   4.0470,   7.2675,   5.8228,  11.5951,   1.7431,\n",
      "          5.2180,  -0.5949,   5.3415,   8.0173,  -3.6092,   7.2466,   2.9497,\n",
      "         -2.4643,   3.3846,   1.9452,   0.8152,   0.4616,   4.5371,   2.9154,\n",
      "          3.6539,   5.5560,   7.4563,   8.1484,   6.7841,   3.4647,   2.1889,\n",
      "         -2.7071,   2.7777,   1.3008,   3.1594,   7.4483,   5.0694,  -3.0101,\n",
      "         -4.3569,   2.2161,   0.0647]), kinetic_energy=tensor(445.8057)), acceptance_ratio=tensor([1.], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "n_of_positives = torch.tensor( [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 5, 2, 5, 3, 2, 7, 7, 3, 3, 2, 9, 10, 4, 4, 4, 4, 4, 4, 4, 10, 4, 4, 4, 5, 11, 12, 5, 5, 6, 5, 6, 6, 6, 6, 16, 15, 15, 9, 4, ], dtype=torch.float32)\n",
    "group_size = torch.tensor( [ 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18, 18, 17, 20, 20, 20, 20, 19, 19, 18, 18, 25, 24, 23, 20, 20, 20, 20, 20, 20, 10, 49, 19, 46, 27, 17, 49, 47, 20, 20, 13, 48, 50, 20, 20, 20, 20, 20, 20, 20, 48, 19, 19, 19, 22, 46, 49, 20, 20, 23, 19, 22, 20, 20, 20, 52, 46, 47, 24, 14, ], dtype=torch.float32)\n",
    "n_rat_tumors = len(group_size)\n",
    "\n",
    "# rng_key = jax.random.PRNGKey(82)\n",
    "# tfd = tfp.distributions\n",
    "n_rat_tumors = 71\n",
    "\n",
    "jacobian_fn = torch.func.jacfwd(torch.nn.functional.sigmoid)\n",
    "jacobian_fn_softplus = torch.func.jacfwd(torch.nn.functional.softplus)\n",
    "\n",
    "def joint_log_prob(params: Position) -> torch.Tensor:\n",
    "    a, b, logits = params.a, params.b, params.logits\n",
    "\n",
    "    thetas = torch.nn.functional.sigmoid(logits)\n",
    "    a = torch.nn.functional.softplus(a)\n",
    "    b = torch.nn.functional.softplus(b)\n",
    "    log_det_jacob = torch.sum(\n",
    "        torch.vmap(lambda logit: torch.log(torch.abs(torch.linalg.det(jacobian_fn(logit.reshape(1, 1))))))(logits)\n",
    "    )\n",
    "    # improper prior for a,b\n",
    "    logprob_ab = torch.log(torch.float_power(a + b, -2.5))\n",
    "\n",
    "    # logprob prior of theta\n",
    "    logprob_thetas = torch.distributions.beta.Beta(a, b).log_prob(thetas).sum()\n",
    "\n",
    "    apply_data =  torch.vmap(lambda y, N, theta: torch.distributions.binomial.Binomial(N, probs=theta, validate_args=False).log_prob(y))\n",
    "    # loglikelihood of y\n",
    "    logprob_y = torch.sum(\n",
    "       apply_data(n_of_positives, group_size, thetas)\n",
    "    )\n",
    "    return logprob_ab + logprob_thetas + logprob_y + log_det_jacob\n",
    "\n",
    "def init_param_fn(total_logits=len(group_size),n_chains=4) -> Position:\n",
    "    \"\"\"\n",
    "    initialize a, b & logits\n",
    "    \"\"\"\n",
    "    return Position (\n",
    "        a = torch.distributions.uniform.Uniform(0, 3, validate_args=False).sample(),\n",
    "        b = torch.distributions.uniform.Uniform(0, 3, validate_args=False).sample(),\n",
    "        logits =  torch.distributions.uniform.Uniform(-2, 2, validate_args=False).sample(sample_shape=(total_logits,)),\n",
    "    )\n",
    "\n",
    "def scan(func, init_values, length):\n",
    "    carry = init_values\n",
    "    result = []\n",
    "    for i in range(length):\n",
    "        carry = func(carry)\n",
    "        current = {}\n",
    "        current['a'] = torch.cat([torch.unsqueeze(param.a,0) for param in carry])\n",
    "        current['b'] = torch.cat([torch.unsqueeze(param.b,0) for param in carry])\n",
    "        current['logits'] = torch.cat([torch.unsqueeze(param.logits,0) for param in carry])\n",
    "        result.append(current)\n",
    "    return carry, result \n",
    "\n",
    "def same_state_multiple_chains(hmc: HMCAlgorithm, n_chains=4, n_samples = 10, burn_in_time = 5):\n",
    "    def delta_step(positions:list[Position]):\n",
    "        current_state = []\n",
    "        for i,position in enumerate(positions):\n",
    "            position_after_update = hmc.update(position,i)\n",
    "            current_state.append(position_after_update)\n",
    "        # print('-----------------------------')\n",
    "        return current_state\n",
    "    \n",
    "    init_states = [init_param_fn(total_logits=len(group_size)) for _ in range(n_chains)]\n",
    "    final_state, result = scan(delta_step, init_values=init_states, length=n_samples)\n",
    "    output = {}\n",
    "    output['a'] = torch.cat([torch.unsqueeze(param['a'],0) for param in result[burn_in_time:]]).swapaxes(0,1)\n",
    "    output['b'] = torch.cat([torch.unsqueeze(param['b'],0) for param in result[burn_in_time:]]).swapaxes(0,1)\n",
    "    output['thetas'] = torch.cat([torch.unsqueeze(param['logits'],0) for param in result[burn_in_time:]]).swapaxes(0,1)\n",
    "    return (final_state, output)\n",
    "\n",
    "n_chains = 4\n",
    "n_samples = 5000\n",
    "burn_in_time = 3000\n",
    "hmc = HMCAlgorithm(log_density_fn=joint_log_prob, n_chains=n_chains)\n",
    "final_state, position_per_sample = same_state_multiple_chains(hmc, n_chains, n_samples, burn_in_time)\n",
    "# print(position_per_sample['a'].shape)\n",
    "# print(position_per_sample['b'].shape)\n",
    "# print(position_per_sample['thetas'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = az.convert_to_inference_data(position_per_sample)\n",
    "summary = az.summary(trace)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "az.plot_trace(trace)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
