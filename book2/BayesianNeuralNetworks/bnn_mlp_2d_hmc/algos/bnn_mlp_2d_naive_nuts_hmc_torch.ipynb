{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Callable\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP1D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_shapes=[2,10,10,10], out_shapes = [10,10,10,1]):\n",
    "        super().__init__()\n",
    "        self.linears = [nn.Linear(in_shapes[i], out_shapes[i]) for i in range(len(in_shapes))]\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def model(self):\n",
    "        return nn.Sequential(self.linears[0], self.relu, \n",
    "                             self.linears[1], self.relu, \n",
    "                             self.linears[2], self.relu, self.linears[3])\n",
    "\n",
    "    def forward(self, params, X):\n",
    "        self.weights_and_biases(params)\n",
    "        return self.model()(X)\n",
    "    \n",
    "    def weights_and_biases(self, params):\n",
    "        for i,param in enumerate(params):\n",
    "            self.linears[i].weight = nn.Parameter(param['weight'])\n",
    "            self.linears[i].bias = nn.Parameter(param['bias'])\n",
    "    \n",
    "    def fetch_params(self):\n",
    "        params = []\n",
    "        for linear in self.linears:\n",
    "            params.append({\n",
    "                'weight': linear.weight.detach(),\n",
    "                'bias': linear.bias.detach()\n",
    "            })\n",
    "        return params\n",
    "\n",
    "def ravel_params(params):\n",
    "    def ravel_per_item():\n",
    "        all_params = []\n",
    "        def ravel_per_item_per_key(param):\n",
    "            for value in param.values(): \n",
    "                all_params.append(value.ravel())\n",
    "\n",
    "        if type(params) == list:\n",
    "            for param in params:\n",
    "                ravel_per_item_per_key(param)\n",
    "        else:\n",
    "            ravel_per_item_per_key(params)\n",
    "        return all_params\n",
    "    return torch.cat(ravel_per_item())\n",
    "\n",
    "def unravel_params(params_raveled: torch.Tensor, structured_tuple):\n",
    "    \n",
    "    def unravel_all_shapes(structure, is_list):\n",
    "        shapes = []\n",
    "        def extract_shapes(structure):\n",
    "            if is_list:\n",
    "                for per_structure in structure:\n",
    "                    for val in per_structure.values():\n",
    "                        shapes.append(val.numel())\n",
    "            else:\n",
    "                for val in structure.values():\n",
    "                    shapes.append(val.numel())\n",
    "            values_per_structure = params_raveled.split(shapes)\n",
    "            return values_per_structure\n",
    "        \n",
    "        values_per_structure = extract_shapes(structure)\n",
    "        result = []\n",
    "        index = [0]\n",
    "\n",
    "        def result_per_dict(values_per_structure, per_structure):\n",
    "            per_result = {}\n",
    "            for key, val in per_structure.items():\n",
    "                # print(key, val.shape, values_per_structure[index[0]].shape)\n",
    "                per_result[key] = values_per_structure[index[0]].reshape(val.shape)\n",
    "                index[0] += 1\n",
    "            return per_result\n",
    "        \n",
    "        if is_list:\n",
    "            for per_structure in structure:\n",
    "                per_result = result_per_dict(values_per_structure, per_structure)\n",
    "                result.append(per_result)\n",
    "            return result\n",
    "        else:\n",
    "            return result_per_dict(values_per_structure, structure)\n",
    "        \n",
    "    return unravel_all_shapes(structured_tuple, type(structured_tuple) == list)\n",
    "    \n",
    "def bnn_log_joint(params, X, y, model:MLP1D):\n",
    "    logits = model.forward(params, X).ravel()\n",
    "    flatten_params = ravel_params(params)\n",
    "    log_prior = torch.distributions.Normal(0.0, 1.0).log_prob(flatten_params).sum()\n",
    "    log_likelihood = torch.distributions.Bernoulli(logits=logits).log_prob(y).sum()\n",
    "    log_joint = log_prior + log_likelihood\n",
    "    return log_joint\n",
    "\n",
    "noise = 0.2\n",
    "num_samples = 50\n",
    "num_warmup = 1000\n",
    "num_steps = 500\n",
    "in_shapes = [2,10,10,10]\n",
    "out_shapes = [10,10,10,1]\n",
    "\n",
    "X, y = make_moons(n_samples=num_samples, noise=noise, random_state=314)\n",
    "model = MLP1D(in_shapes=in_shapes, out_shapes=out_shapes)\n",
    "params = model.fetch_params()\n",
    "potential = partial(bnn_log_joint, X=torch.Tensor(X), y=torch.Tensor(y), model=model)\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "# potential(params)\n",
    "# print(ravel_params(params[0]).shape)\n",
    "# print(unravel_params(ravel_params(params[1]),params[1]))\n",
    "# print(unravel_params(ravel_params(params),params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyParameters:\n",
    "\n",
    "    def __init__(self, potential_energy_fn:Callable, precision):\n",
    "        self.potential_energy_fn = potential_energy_fn \n",
    "        self.precision = precision\n",
    "    \n",
    "    def set_position(self, position):\n",
    "        self.position = position\n",
    "        self.potential_energy = self.potential_energy_fn(position)\n",
    "        self.potential_energy_grad = torch.func.grad(self.potential_energy_fn)(position)\n",
    "    \n",
    "    def set_velocity(self, velocity):\n",
    "        self.velocity = velocity\n",
    "        self.kinetic_energy = self.kinetic_energy_fn(self.velocity)\n",
    "        self.kinetic_energy_grad = torch.func.grad(self.kinetic_energy_fn)(self.velocity)\n",
    "\n",
    "    def init_per_step(self, position, velocity):\n",
    "        self.set_position(position)\n",
    "        self.set_velocity(velocity)\n",
    "        self.total_init_energy = self.total_current_energy()\n",
    "    \n",
    "    def kinetic_energy_fn(self, velocity:torch.Tensor):\n",
    "        return 0.5*torch.matmul(velocity, torch.matmul(self.precision, velocity.T))\n",
    "    \n",
    "    def update_position(self, step_size_with_direction):\n",
    "        position_raveled = ravel_params(self.position)\n",
    "        result = position_raveled + step_size_with_direction*self.kinetic_energy_grad\n",
    "        n_position = unravel_params(result, self.position)\n",
    "        self.set_position(n_position)\n",
    "        return n_position\n",
    "\n",
    "    def updated_velocity(self, step_size_with_direction, is_half_step_momentum = False):\n",
    "        n_velocity = torch.subtract(self.velocity, step_size_with_direction*(0.5 if is_half_step_momentum else 1)*ravel_params(self.potential_energy_grad))\n",
    "        self.set_velocity(n_velocity)\n",
    "        return n_velocity\n",
    "    \n",
    "    def delta_energy(self):\n",
    "        return self.total_current_energy() - self.total_init_energy \n",
    "    \n",
    "    def total_current_energy(self):\n",
    "        return - self.potential_energy - self.kinetic_energy\n",
    "\n",
    "# Test\n",
    "\n",
    "# e_params = EnergyParameters(potential_energy_fn=potential)\n",
    "# e_params.set_energy_parameters(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive No-U-Turn Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMCAlgorithm:\n",
    "    \n",
    "    def __init__(self, log_density_fn: Callable, precision, l=1, step_size=0.1):\n",
    "        self.log_density_fn = log_density_fn\n",
    "        self.l = l\n",
    "        self.step_size = step_size\n",
    "        self.ep = EnergyParameters(potential_energy_fn=log_density_fn, precision=precision)\n",
    "    \n",
    "\n",
    "    def generate_default_velocity(self, position_shape):\n",
    "\n",
    "        def generate_random_gaussian(covariance: torch.Tensor):\n",
    "            return torch.distributions.MultivariateNormal(loc=torch.zeros(covariance.shape[-1]),covariance_matrix=covariance).sample()\n",
    "\n",
    "        def generate_inverse_mass_matrix_and_sigma(covariance_shape: int):\n",
    "            covariance = torch.eye(covariance_shape)\n",
    "            inverse_mass_matrix = covariance.inverse()\n",
    "            return inverse_mass_matrix, covariance\n",
    "        \n",
    "        _, covariance = generate_inverse_mass_matrix_and_sigma(position_shape)\n",
    "        return generate_random_gaussian(covariance)\n",
    "\n",
    "    def current_direction(self):\n",
    "        direction = torch.distributions.Bernoulli(logits=torch.tensor([0.5])).sample()\n",
    "        return -1 if direction.bool() else 1\n",
    "\n",
    "    def stop_condition(self, p_minus, v_minus, p_plus, v_plus):\n",
    "        diff = ravel_params(p_plus)-ravel_params(p_minus)\n",
    "        condition = torch.all(diff*v_minus >= torch.zeros(diff.shape)) and torch.all(diff*v_plus >= torch.zeros(diff.shape))\n",
    "        return condition.int()\n",
    "\n",
    "    def build_tree(self, position, velocity, energy, direction, iter_no, step_size, m):\n",
    "        delta_max = torch.tensor(1000)\n",
    "        parameters = []\n",
    "        if iter_no == 0:\n",
    "            _, new_position, new_velocity, new_energy = self.step_integrator(position, velocity, direction*step_size)\n",
    "            print(new_energy, energy, m)\n",
    "            if energy <= torch.exp(new_energy):\n",
    "                parameters.append((new_position, new_velocity))\n",
    "            s = 1 if new_energy > torch.log(energy) - delta_max else 0\n",
    "            return new_position, new_velocity, new_position, new_velocity, parameters, s\n",
    "        else:\n",
    "            p_minus, v_minus, p_plus, v_plus, n_params, n_s = self.build_tree(position, velocity, energy, direction, iter_no-1, step_size, m)\n",
    "            if direction == -1:\n",
    "                p_minus, v_minus, _, _, n_params, n_s = self.build_tree(p_minus, v_minus, energy, direction, iter_no-1, step_size, m)\n",
    "            else:\n",
    "                _, _, p_plus, v_plus, n_params, n_s = self.build_tree(p_plus, v_plus, energy, direction, iter_no-1, step_size, m)\n",
    "            s = n_s * self.stop_condition(p_minus, v_minus, p_plus, v_plus)\n",
    "            parameters += n_params\n",
    "            return p_minus, v_minus, p_plus, v_plus, parameters, s\n",
    "\n",
    "    def update(self, position, m):\n",
    "        # self.step_size = self.find_reasonable_epsilon(position)\n",
    "        velocity = self.generate_default_velocity(ravel_params(position).shape[-1])\n",
    "        self.ep.init_per_step(position, velocity)\n",
    "        energy = torch.distributions.Uniform(torch.tensor([0.0]),self.ep.total_current_energy()).sample()\n",
    "        p_minus, p_plus, v_minus, v_plus = position, position, velocity, velocity\n",
    "        counter, s = 0, 1\n",
    "        parameters = []\n",
    "\n",
    "        parameters.append((position, velocity))\n",
    "        while s == 1:\n",
    "            direction = self.current_direction()\n",
    "            if direction == -1:\n",
    "                p_minus, v_minus, _,_, n_parameters, new_s = self.build_tree(p_minus, v_minus, energy, direction, counter, self.step_size, m) \n",
    "            else:\n",
    "                _, _, p_plus, v_plus, n_parameters, new_s = self.build_tree(p_plus, v_plus, energy, direction, counter, self.step_size, m) \n",
    "            if new_s == 1:\n",
    "                parameters += n_parameters\n",
    "            s = new_s * self.stop_condition(p_minus, v_minus, p_plus, v_plus)\n",
    "            counter += 1\n",
    "        selected_index = torch.distributions.Categorical(logits=torch.tensor([1.0])/len(parameters)).sample()\n",
    "        return parameters[selected_index][0]\n",
    "\n",
    "    def step_integrator(self, position, velocity, step_size_with_direction):\n",
    "        self.ep.init_per_step(position, velocity)\n",
    "        # init step\n",
    "        self.ep.updated_velocity(step_size_with_direction, is_half_step_momentum=True)\n",
    "        # l_frog_steps\n",
    "        for i in range(self.l):\n",
    "            self.ep.update_position(step_size_with_direction)\n",
    "            self.ep.updated_velocity(step_size_with_direction, is_half_step_momentum=False)\n",
    "        # final step\n",
    "        f_step_position = self.ep.update_position(step_size_with_direction)\n",
    "        half_step_velocity = self.ep.updated_velocity(step_size_with_direction, is_half_step_momentum=True)\n",
    "        return self.ep.delta_energy(), f_step_position, half_step_velocity, self.ep.total_current_energy()\n",
    "\n",
    "precision_shape = ravel_params(params).shape[-1]\n",
    "# I^-1 = I so inverse doesn't matter but writing it for visibility\n",
    "default_precision = torch.eye(precision_shape).inverse()\n",
    "hmc_kernel = HMCAlgorithm(log_density_fn=potential, precision=default_precision)\n",
    "\n",
    "# Test\n",
    "# hmc_kernel.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(165.8301, grad_fn=<SubBackward0>) tensor([135.4637]) 0\n",
      "tensor(152.3228, grad_fn=<SubBackward0>) tensor([146.5278]) 1\n",
      "tensor(145.9471, grad_fn=<SubBackward0>) tensor([33.2595]) 2\n",
      "tensor(143.4247, grad_fn=<SubBackward0>) tensor([86.9108]) 3\n",
      "tensor(162.0123, grad_fn=<SubBackward0>) tensor([69.2887]) 4\n",
      "tensor(167.0401, grad_fn=<SubBackward0>) tensor([60.5012]) 5\n",
      "tensor(167.4197, grad_fn=<SubBackward0>) tensor([134.6359]) 6\n",
      "tensor(139.0300, grad_fn=<SubBackward0>) tensor([60.0303]) 7\n",
      "tensor(140.0326, grad_fn=<SubBackward0>) tensor([97.1153]) 8\n",
      "tensor(140.8889, grad_fn=<SubBackward0>) tensor([130.6704]) 9\n",
      "tensor(146.6672, grad_fn=<SubBackward0>) tensor([106.7928]) 10\n",
      "tensor(167.2018, grad_fn=<SubBackward0>) tensor([97.0777]) 11\n",
      "tensor(151.4746, grad_fn=<SubBackward0>) tensor([16.1752]) 12\n",
      "tensor(131.6249, grad_fn=<SubBackward0>) tensor([45.6553]) 13\n",
      "tensor(130.2507, grad_fn=<SubBackward0>) tensor([98.6568]) 14\n",
      "tensor(155.9018, grad_fn=<SubBackward0>) tensor([46.0738]) 15\n",
      "tensor(164.9619, grad_fn=<SubBackward0>) tensor([158.7866]) 16\n",
      "tensor(156.3721, grad_fn=<SubBackward0>) tensor([18.8231]) 17\n",
      "tensor(141.7866, grad_fn=<SubBackward0>) tensor([59.5412]) 18\n",
      "tensor(145.6325, grad_fn=<SubBackward0>) tensor([111.4111]) 19\n",
      "tensor(143.6122, grad_fn=<SubBackward0>) tensor([119.8191]) 20\n",
      "tensor(150.0821, grad_fn=<SubBackward0>) tensor([68.7684]) 21\n",
      "tensor(158.1319, grad_fn=<SubBackward0>) tensor([72.7879]) 22\n",
      "tensor(138.6406, grad_fn=<SubBackward0>) tensor([38.1682]) 23\n",
      "tensor(156.3471, grad_fn=<SubBackward0>) tensor([78.7107]) 24\n",
      "tensor(151.0017, grad_fn=<SubBackward0>) tensor([85.9112]) 25\n",
      "tensor(149.2335, grad_fn=<SubBackward0>) tensor([111.4648]) 26\n",
      "tensor(150.9073, grad_fn=<SubBackward0>) tensor([11.0379]) 27\n",
      "tensor(148.8443, grad_fn=<SubBackward0>) tensor([106.8259]) 28\n",
      "tensor(139.3253, grad_fn=<SubBackward0>) tensor([28.1595]) 29\n",
      "tensor(129.7059, grad_fn=<SubBackward0>) tensor([17.5372]) 30\n",
      "tensor(156.2943, grad_fn=<SubBackward0>) tensor([68.8205]) 31\n",
      "tensor(161.9240, grad_fn=<SubBackward0>) tensor([129.6589]) 32\n",
      "tensor(129.9658, grad_fn=<SubBackward0>) tensor([112.7156]) 33\n",
      "tensor(154.6871, grad_fn=<SubBackward0>) tensor([96.1947]) 34\n",
      "tensor(146.7629, grad_fn=<SubBackward0>) tensor([69.0935]) 35\n",
      "tensor(140.3076, grad_fn=<SubBackward0>) tensor([74.2568]) 36\n",
      "tensor(149.4919, grad_fn=<SubBackward0>) tensor([49.6505]) 37\n",
      "tensor(139.8027, grad_fn=<SubBackward0>) tensor([130.5897]) 38\n",
      "tensor(155.8701, grad_fn=<SubBackward0>) tensor([96.6883]) 39\n",
      "tensor(135.1017, grad_fn=<SubBackward0>) tensor([54.9422]) 40\n",
      "tensor(136.0566, grad_fn=<SubBackward0>) tensor([128.8126]) 41\n",
      "tensor(154.7240, grad_fn=<SubBackward0>) tensor([136.0223]) 42\n",
      "tensor(144.0202, grad_fn=<SubBackward0>) tensor([69.2575]) 43\n",
      "tensor(156.7545, grad_fn=<SubBackward0>) tensor([118.0860]) 44\n",
      "tensor(168.3625, grad_fn=<SubBackward0>) tensor([102.8240]) 45\n",
      "tensor(152.0668, grad_fn=<SubBackward0>) tensor([23.4961]) 46\n",
      "tensor(163.5270, grad_fn=<SubBackward0>) tensor([115.5834]) 47\n",
      "tensor(152.6318, grad_fn=<SubBackward0>) tensor([57.2569]) 48\n",
      "tensor(160.7885, grad_fn=<SubBackward0>) tensor([103.4178]) 49\n",
      "tensor(156.1448, grad_fn=<SubBackward0>) tensor([74.4208]) 50\n",
      "tensor(145.8378, grad_fn=<SubBackward0>) tensor([60.5848]) 51\n",
      "tensor(171.2080, grad_fn=<SubBackward0>) tensor([46.3723]) 52\n",
      "tensor(138.3181, grad_fn=<SubBackward0>) tensor([97.5472]) 53\n",
      "tensor(151.0324, grad_fn=<SubBackward0>) tensor([74.3976]) 54\n",
      "tensor(137.0811, grad_fn=<SubBackward0>) tensor([127.4905]) 55\n",
      "tensor(150.5541, grad_fn=<SubBackward0>) tensor([88.4620]) 56\n",
      "tensor(130.4594, grad_fn=<SubBackward0>) tensor([87.6395]) 57\n",
      "tensor(171.1772, grad_fn=<SubBackward0>) tensor([97.8279]) 58\n",
      "tensor(157.8589, grad_fn=<SubBackward0>) tensor([123.6256]) 59\n",
      "tensor(158.3379, grad_fn=<SubBackward0>) tensor([6.4218]) 60\n",
      "tensor(146.3361, grad_fn=<SubBackward0>) tensor([53.5503]) 61\n",
      "tensor(165.0688, grad_fn=<SubBackward0>) tensor([150.1620]) 62\n",
      "tensor(168.7227, grad_fn=<SubBackward0>) tensor([124.6650]) 63\n",
      "tensor(141.3181, grad_fn=<SubBackward0>) tensor([20.5613]) 64\n",
      "tensor(149.8241, grad_fn=<SubBackward0>) tensor([115.8366]) 65\n",
      "tensor(151.7901, grad_fn=<SubBackward0>) tensor([26.8353]) 66\n",
      "tensor(158.3494, grad_fn=<SubBackward0>) tensor([144.2657]) 67\n",
      "tensor(158.2933, grad_fn=<SubBackward0>) tensor([65.2988]) 68\n",
      "tensor(176.9167, grad_fn=<SubBackward0>) tensor([39.6905]) 69\n",
      "tensor(147.4576, grad_fn=<SubBackward0>) tensor([69.8099]) 70\n",
      "tensor(164.7865, grad_fn=<SubBackward0>) tensor([111.5093]) 71\n",
      "tensor(144.2239, grad_fn=<SubBackward0>) tensor([107.4819]) 72\n",
      "tensor(154.1998, grad_fn=<SubBackward0>) tensor([142.3812]) 73\n",
      "tensor(155.7681, grad_fn=<SubBackward0>) tensor([124.5692]) 74\n",
      "tensor(149.0265, grad_fn=<SubBackward0>) tensor([46.3997]) 75\n",
      "tensor(138.7707, grad_fn=<SubBackward0>) tensor([139.3958]) 76\n",
      "tensor(141.6460, grad_fn=<SubBackward0>) tensor([139.3958]) 76\n",
      "tensor(180.4026, grad_fn=<SubBackward0>) tensor([139.3958]) 76\n",
      "tensor(144.6665, grad_fn=<SubBackward0>) tensor([2.3037]) 77\n",
      "tensor(146.6672, grad_fn=<SubBackward0>) tensor([2.3037]) 77\n",
      "tensor(153.6258, grad_fn=<SubBackward0>) tensor([2.3037]) 77\n",
      "tensor(164.5663, grad_fn=<SubBackward0>) tensor([72.5503]) 78\n",
      "tensor(153.8186, grad_fn=<SubBackward0>) tensor([120.9645]) 79\n",
      "tensor(145.4827, grad_fn=<SubBackward0>) tensor([48.6329]) 80\n",
      "tensor(149.6142, grad_fn=<SubBackward0>) tensor([29.4664]) 81\n",
      "tensor(150.7755, grad_fn=<SubBackward0>) tensor([26.0270]) 82\n",
      "tensor(143.1705, grad_fn=<SubBackward0>) tensor([10.0617]) 83\n",
      "tensor(139.1478, grad_fn=<SubBackward0>) tensor([98.0258]) 84\n",
      "tensor(151.6217, grad_fn=<SubBackward0>) tensor([9.7305]) 85\n",
      "tensor(161.4100, grad_fn=<SubBackward0>) tensor([135.3284]) 86\n",
      "tensor(146.8722, grad_fn=<SubBackward0>) tensor([85.9914]) 87\n",
      "tensor(146.4403, grad_fn=<SubBackward0>) tensor([27.5168]) 88\n",
      "tensor(152.1165, grad_fn=<SubBackward0>) tensor([31.5378]) 89\n",
      "tensor(166.3915, grad_fn=<SubBackward0>) tensor([16.0714]) 90\n",
      "tensor(152.7762, grad_fn=<SubBackward0>) tensor([145.6421]) 91\n",
      "tensor(152.8216, grad_fn=<SubBackward0>) tensor([52.2006]) 92\n",
      "tensor(161.8754, grad_fn=<SubBackward0>) tensor([157.4322]) 93\n",
      "tensor(168.4671, grad_fn=<SubBackward0>) tensor([129.3203]) 94\n",
      "tensor(148.9382, grad_fn=<SubBackward0>) tensor([113.7193]) 95\n",
      "tensor(144.2999, grad_fn=<SubBackward0>) tensor([130.7684]) 96\n",
      "tensor(150.0128, grad_fn=<SubBackward0>) tensor([7.7008]) 97\n",
      "tensor(156.3715, grad_fn=<SubBackward0>) tensor([13.8461]) 98\n",
      "tensor(150.7953, grad_fn=<SubBackward0>) tensor([59.9372]) 99\n",
      "tensor(161.6320, grad_fn=<SubBackward0>) tensor([78.0666]) 100\n",
      "tensor(150.6781, grad_fn=<SubBackward0>) tensor([104.6020]) 101\n",
      "tensor(148.2422, grad_fn=<SubBackward0>) tensor([55.5344]) 102\n",
      "tensor(150.7509, grad_fn=<SubBackward0>) tensor([143.3941]) 103\n",
      "tensor(158.9111, grad_fn=<SubBackward0>) tensor([145.1804]) 104\n",
      "tensor(146.8430, grad_fn=<SubBackward0>) tensor([41.9833]) 105\n",
      "tensor(160.0146, grad_fn=<SubBackward0>) tensor([116.2505]) 106\n",
      "tensor(137.1960, grad_fn=<SubBackward0>) tensor([69.8190]) 107\n",
      "tensor(143.6357, grad_fn=<SubBackward0>) tensor([119.6743]) 108\n",
      "tensor(135.8913, grad_fn=<SubBackward0>) tensor([74.7402]) 109\n",
      "tensor(154.5495, grad_fn=<SubBackward0>) tensor([2.3602]) 110\n",
      "tensor(134.8275, grad_fn=<SubBackward0>) tensor([20.7123]) 111\n",
      "tensor(156.8366, grad_fn=<SubBackward0>) tensor([28.2152]) 112\n",
      "tensor(162.1134, grad_fn=<SubBackward0>) tensor([67.0478]) 113\n",
      "tensor(118.7397, grad_fn=<SubBackward0>) tensor([24.6921]) 114\n",
      "tensor(141.6217, grad_fn=<SubBackward0>) tensor([126.5843]) 115\n",
      "tensor(156.1813, grad_fn=<SubBackward0>) tensor([18.0459]) 116\n",
      "tensor(167.9872, grad_fn=<SubBackward0>) tensor([48.5449]) 117\n",
      "tensor(174.2832, grad_fn=<SubBackward0>) tensor([128.7743]) 118\n",
      "tensor(142.5982, grad_fn=<SubBackward0>) tensor([38.2033]) 119\n",
      "tensor(150.9661, grad_fn=<SubBackward0>) tensor([65.1607]) 120\n",
      "tensor(154.7618, grad_fn=<SubBackward0>) tensor([145.7920]) 121\n",
      "tensor(145.4388, grad_fn=<SubBackward0>) tensor([20.1962]) 122\n",
      "tensor(150.9761, grad_fn=<SubBackward0>) tensor([123.1378]) 123\n",
      "tensor(159.2918, grad_fn=<SubBackward0>) tensor([141.8773]) 124\n",
      "tensor(158.8334, grad_fn=<SubBackward0>) tensor([98.1492]) 125\n",
      "tensor(170.2755, grad_fn=<SubBackward0>) tensor([86.6073]) 126\n",
      "tensor(172.1872, grad_fn=<SubBackward0>) tensor([86.6073]) 126\n",
      "tensor(213.3240, grad_fn=<SubBackward0>) tensor([86.6073]) 126\n",
      "tensor(154.4515, grad_fn=<SubBackward0>) tensor([49.3478]) 127\n",
      "tensor(157.7752, grad_fn=<SubBackward0>) tensor([90.1636]) 128\n",
      "tensor(122.8677, grad_fn=<SubBackward0>) tensor([55.9466]) 129\n",
      "tensor(161.5209, grad_fn=<SubBackward0>) tensor([42.8991]) 130\n",
      "tensor(156.4168, grad_fn=<SubBackward0>) tensor([117.3613]) 131\n",
      "tensor(131.1195, grad_fn=<SubBackward0>) tensor([116.4748]) 132\n",
      "tensor(149.4543, grad_fn=<SubBackward0>) tensor([38.4455]) 133\n",
      "tensor(144.5379, grad_fn=<SubBackward0>) tensor([31.9593]) 134\n",
      "tensor(149.8380, grad_fn=<SubBackward0>) tensor([141.5153]) 135\n",
      "tensor(165.3243, grad_fn=<SubBackward0>) tensor([100.0340]) 136\n",
      "tensor(158.2378, grad_fn=<SubBackward0>) tensor([13.6406]) 137\n",
      "tensor(151.7635, grad_fn=<SubBackward0>) tensor([33.8441]) 138\n",
      "tensor(144.6678, grad_fn=<SubBackward0>) tensor([52.7563]) 139\n",
      "tensor(152.4172, grad_fn=<SubBackward0>) tensor([43.9730]) 140\n",
      "tensor(152.6041, grad_fn=<SubBackward0>) tensor([130.6522]) 141\n",
      "tensor(139.9756, grad_fn=<SubBackward0>) tensor([28.3702]) 142\n",
      "tensor(158.7316, grad_fn=<SubBackward0>) tensor([140.3416]) 143\n",
      "tensor(153.7260, grad_fn=<SubBackward0>) tensor([90.8311]) 144\n",
      "tensor(161.0431, grad_fn=<SubBackward0>) tensor([74.4463]) 145\n",
      "tensor(168.3933, grad_fn=<SubBackward0>) tensor([102.4465]) 146\n",
      "tensor(144.3848, grad_fn=<SubBackward0>) tensor([142.7682]) 147\n",
      "tensor(149.2446, grad_fn=<SubBackward0>) tensor([8.2806]) 148\n",
      "tensor(136.2706, grad_fn=<SubBackward0>) tensor([130.2456]) 149\n",
      "tensor(129.0240, grad_fn=<SubBackward0>) tensor([15.9476]) 150\n",
      "tensor(148.7213, grad_fn=<SubBackward0>) tensor([148.7467]) 151\n",
      "tensor(160.9650, grad_fn=<SubBackward0>) tensor([15.7967]) 152\n",
      "tensor(163.5356, grad_fn=<SubBackward0>) tensor([26.5942]) 153\n",
      "tensor(140.0667, grad_fn=<SubBackward0>) tensor([103.7654]) 154\n",
      "tensor(130.9686, grad_fn=<SubBackward0>) tensor([105.4959]) 155\n",
      "tensor(152.3836, grad_fn=<SubBackward0>) tensor([143.2262]) 156\n",
      "tensor(150.2474, grad_fn=<SubBackward0>) tensor([88.4466]) 157\n",
      "tensor(149.2773, grad_fn=<SubBackward0>) tensor([88.4466]) 157\n",
      "tensor(178.4855, grad_fn=<SubBackward0>) tensor([88.4466]) 157\n",
      "tensor(169.2535, grad_fn=<SubBackward0>) tensor([116.2806]) 158\n",
      "tensor(151.7113, grad_fn=<SubBackward0>) tensor([65.4310]) 159\n",
      "tensor(147.7990, grad_fn=<SubBackward0>) tensor([89.3689]) 160\n",
      "tensor(153.5434, grad_fn=<SubBackward0>) tensor([127.0492]) 161\n",
      "tensor(170.8342, grad_fn=<SubBackward0>) tensor([15.7251]) 162\n",
      "tensor(155.7866, grad_fn=<SubBackward0>) tensor([108.1458]) 163\n",
      "tensor(153.5411, grad_fn=<SubBackward0>) tensor([95.7672]) 164\n",
      "tensor(147.9420, grad_fn=<SubBackward0>) tensor([108.7465]) 165\n",
      "tensor(155.1223, grad_fn=<SubBackward0>) tensor([126.4611]) 166\n",
      "tensor(134.9127, grad_fn=<SubBackward0>) tensor([70.0488]) 167\n",
      "tensor(156.2150, grad_fn=<SubBackward0>) tensor([102.5196]) 168\n",
      "tensor(128.8443, grad_fn=<SubBackward0>) tensor([90.1283]) 169\n",
      "tensor(153.8287, grad_fn=<SubBackward0>) tensor([76.8167]) 170\n",
      "tensor(156.4739, grad_fn=<SubBackward0>) tensor([147.3316]) 171\n",
      "tensor(152.2706, grad_fn=<SubBackward0>) tensor([86.4566]) 172\n",
      "tensor(162.6554, grad_fn=<SubBackward0>) tensor([120.4776]) 173\n",
      "tensor(161.0366, grad_fn=<SubBackward0>) tensor([95.7714]) 174\n",
      "tensor(142.3438, grad_fn=<SubBackward0>) tensor([69.4884]) 175\n",
      "tensor(140.4285, grad_fn=<SubBackward0>) tensor([81.4608]) 176\n",
      "tensor(149.0186, grad_fn=<SubBackward0>) tensor([121.4154]) 177\n",
      "tensor(154.8047, grad_fn=<SubBackward0>) tensor([57.7629]) 178\n",
      "tensor(126.1105, grad_fn=<SubBackward0>) tensor([21.6594]) 179\n",
      "tensor(151.5089, grad_fn=<SubBackward0>) tensor([124.9135]) 180\n",
      "tensor(151.3719, grad_fn=<SubBackward0>) tensor([13.3072]) 181\n",
      "tensor(165.5074, grad_fn=<SubBackward0>) tensor([137.6331]) 182\n",
      "tensor(135.6768, grad_fn=<SubBackward0>) tensor([87.4408]) 183\n",
      "tensor(144.0139, grad_fn=<SubBackward0>) tensor([38.7027]) 184\n",
      "tensor(155.0641, grad_fn=<SubBackward0>) tensor([23.9696]) 185\n",
      "tensor(160.0475, grad_fn=<SubBackward0>) tensor([50.1749]) 186\n",
      "tensor(139.0461, grad_fn=<SubBackward0>) tensor([120.7700]) 187\n",
      "tensor(149.7504, grad_fn=<SubBackward0>) tensor([52.3414]) 188\n",
      "tensor(154.7909, grad_fn=<SubBackward0>) tensor([89.5214]) 189\n",
      "tensor(137.7567, grad_fn=<SubBackward0>) tensor([20.6967]) 190\n",
      "tensor(148.8417, grad_fn=<SubBackward0>) tensor([138.5453]) 191\n",
      "tensor(154.3131, grad_fn=<SubBackward0>) tensor([141.7144]) 192\n",
      "tensor(147.7795, grad_fn=<SubBackward0>) tensor([8.1510]) 193\n",
      "tensor(150.8842, grad_fn=<SubBackward0>) tensor([81.7769]) 194\n",
      "tensor(167.1259, grad_fn=<SubBackward0>) tensor([88.7311]) 195\n",
      "tensor(167.8411, grad_fn=<SubBackward0>) tensor([120.2805]) 196\n",
      "tensor(141.0936, grad_fn=<SubBackward0>) tensor([68.9766]) 197\n",
      "tensor(165.2243, grad_fn=<SubBackward0>) tensor([14.7150]) 198\n",
      "tensor(155.0770, grad_fn=<SubBackward0>) tensor([73.1870]) 199\n",
      "tensor(168.3449, grad_fn=<SubBackward0>) tensor([38.9001]) 200\n",
      "tensor(135.8799, grad_fn=<SubBackward0>) tensor([88.9387]) 201\n",
      "tensor(143.2294, grad_fn=<SubBackward0>) tensor([48.0810]) 202\n",
      "tensor(151.6983, grad_fn=<SubBackward0>) tensor([25.1873]) 203\n",
      "tensor(160.0742, grad_fn=<SubBackward0>) tensor([148.2058]) 204\n",
      "tensor(157.0426, grad_fn=<SubBackward0>) tensor([54.6715]) 205\n",
      "tensor(131.9167, grad_fn=<SubBackward0>) tensor([128.7050]) 206\n",
      "tensor(129.6327, grad_fn=<SubBackward0>) tensor([128.7050]) 206\n",
      "tensor(131.2027, grad_fn=<SubBackward0>) tensor([128.7050]) 206\n",
      "tensor(169.7365, grad_fn=<SubBackward0>) tensor([157.9491]) 207\n",
      "tensor(146.5985, grad_fn=<SubBackward0>) tensor([80.5399]) 208\n",
      "tensor(149.0271, grad_fn=<SubBackward0>) tensor([64.0353]) 209\n",
      "tensor(138.2830, grad_fn=<SubBackward0>) tensor([32.5618]) 210\n",
      "tensor(142.7084, grad_fn=<SubBackward0>) tensor([97.6162]) 211\n",
      "tensor(156.8921, grad_fn=<SubBackward0>) tensor([104.6801]) 212\n",
      "tensor(149.2922, grad_fn=<SubBackward0>) tensor([134.6684]) 213\n",
      "tensor(163.4384, grad_fn=<SubBackward0>) tensor([74.9024]) 214\n",
      "tensor(137.2860, grad_fn=<SubBackward0>) tensor([94.6619]) 215\n",
      "tensor(146.5739, grad_fn=<SubBackward0>) tensor([35.1543]) 216\n",
      "tensor(172.9668, grad_fn=<SubBackward0>) tensor([87.1376]) 217\n",
      "tensor(161.2700, grad_fn=<SubBackward0>) tensor([39.8458]) 218\n",
      "tensor(155.9676, grad_fn=<SubBackward0>) tensor([95.2785]) 219\n",
      "tensor(130.8763, grad_fn=<SubBackward0>) tensor([48.8853]) 220\n",
      "tensor(160.5986, grad_fn=<SubBackward0>) tensor([157.8468]) 221\n",
      "tensor(145.8764, grad_fn=<SubBackward0>) tensor([19.9413]) 222\n",
      "tensor(153.4098, grad_fn=<SubBackward0>) tensor([2.9532]) 223\n",
      "tensor(171.5623, grad_fn=<SubBackward0>) tensor([84.7621]) 224\n",
      "tensor(154.4655, grad_fn=<SubBackward0>) tensor([128.0036]) 225\n",
      "tensor(165.5157, grad_fn=<SubBackward0>) tensor([63.4968]) 226\n",
      "tensor(144.1741, grad_fn=<SubBackward0>) tensor([89.5564]) 227\n",
      "tensor(157.1666, grad_fn=<SubBackward0>) tensor([16.4530]) 228\n",
      "tensor(153.3660, grad_fn=<SubBackward0>) tensor([150.9118]) 229\n",
      "tensor(151.7712, grad_fn=<SubBackward0>) tensor([123.5556]) 230\n",
      "tensor(161.9304, grad_fn=<SubBackward0>) tensor([154.3745]) 231\n",
      "tensor(129.0056, grad_fn=<SubBackward0>) tensor([55.4799]) 232\n",
      "tensor(152.0277, grad_fn=<SubBackward0>) tensor([33.6063]) 233\n",
      "tensor(153.0849, grad_fn=<SubBackward0>) tensor([96.2390]) 234\n",
      "tensor(152.0435, grad_fn=<SubBackward0>) tensor([73.0396]) 235\n",
      "tensor(150.1193, grad_fn=<SubBackward0>) tensor([83.6723]) 236\n",
      "tensor(154.5587, grad_fn=<SubBackward0>) tensor([43.3987]) 237\n",
      "tensor(155.4141, grad_fn=<SubBackward0>) tensor([132.4839]) 238\n",
      "tensor(156.8867, grad_fn=<SubBackward0>) tensor([3.0320]) 239\n",
      "tensor(155.6013, grad_fn=<SubBackward0>) tensor([116.2067]) 240\n",
      "tensor(146.4620, grad_fn=<SubBackward0>) tensor([115.0801]) 241\n",
      "tensor(137.3212, grad_fn=<SubBackward0>) tensor([77.4676]) 242\n",
      "tensor(161.0049, grad_fn=<SubBackward0>) tensor([23.5144]) 243\n",
      "tensor(148.4345, grad_fn=<SubBackward0>) tensor([85.1282]) 244\n",
      "tensor(154.5697, grad_fn=<SubBackward0>) tensor([77.6946]) 245\n",
      "tensor(162.5194, grad_fn=<SubBackward0>) tensor([33.0596]) 246\n",
      "tensor(157.5607, grad_fn=<SubBackward0>) tensor([69.0468]) 247\n",
      "tensor(151.8639, grad_fn=<SubBackward0>) tensor([87.2409]) 248\n",
      "tensor(147.8119, grad_fn=<SubBackward0>) tensor([107.2633]) 249\n",
      "tensor(147.9051, grad_fn=<SubBackward0>) tensor([118.5939]) 250\n",
      "tensor(152.8184, grad_fn=<SubBackward0>) tensor([102.0144]) 251\n",
      "tensor(156.4725, grad_fn=<SubBackward0>) tensor([77.5975]) 252\n",
      "tensor(137.7886, grad_fn=<SubBackward0>) tensor([26.2515]) 253\n",
      "tensor(136.2609, grad_fn=<SubBackward0>) tensor([120.7441]) 254\n",
      "tensor(152.0673, grad_fn=<SubBackward0>) tensor([55.9629]) 255\n",
      "tensor(161.7260, grad_fn=<SubBackward0>) tensor([119.2677]) 256\n",
      "tensor(144.4375, grad_fn=<SubBackward0>) tensor([23.9358]) 257\n",
      "tensor(137.0495, grad_fn=<SubBackward0>) tensor([30.4932]) 258\n",
      "tensor(165.1606, grad_fn=<SubBackward0>) tensor([126.3865]) 259\n",
      "tensor(146.4842, grad_fn=<SubBackward0>) tensor([77.2020]) 260\n",
      "tensor(172.4959, grad_fn=<SubBackward0>) tensor([95.7871]) 261\n",
      "tensor(155.1032, grad_fn=<SubBackward0>) tensor([67.1169]) 262\n",
      "tensor(160.3960, grad_fn=<SubBackward0>) tensor([31.1228]) 263\n",
      "tensor(159.2794, grad_fn=<SubBackward0>) tensor([131.2173]) 264\n",
      "tensor(138.2299, grad_fn=<SubBackward0>) tensor([0.0080]) 265\n",
      "tensor(163.3717, grad_fn=<SubBackward0>) tensor([55.0873]) 266\n",
      "tensor(143.0253, grad_fn=<SubBackward0>) tensor([110.1138]) 267\n",
      "tensor(138.8365, grad_fn=<SubBackward0>) tensor([69.1168]) 268\n",
      "tensor(161.2738, grad_fn=<SubBackward0>) tensor([67.9612]) 269\n",
      "tensor(145.2152, grad_fn=<SubBackward0>) tensor([22.8481]) 270\n",
      "tensor(144.7239, grad_fn=<SubBackward0>) tensor([22.8481]) 270\n",
      "tensor(180.4754, grad_fn=<SubBackward0>) tensor([22.8481]) 270\n",
      "tensor(161.2775, grad_fn=<SubBackward0>) tensor([36.7267]) 271\n",
      "tensor(149.0662, grad_fn=<SubBackward0>) tensor([108.4747]) 272\n",
      "tensor(147.0208, grad_fn=<SubBackward0>) tensor([124.2499]) 273\n",
      "tensor(151.7151, grad_fn=<SubBackward0>) tensor([4.2926]) 274\n",
      "tensor(159.8836, grad_fn=<SubBackward0>) tensor([76.6518]) 275\n",
      "tensor(145.0201, grad_fn=<SubBackward0>) tensor([3.3030]) 276\n",
      "tensor(139.1259, grad_fn=<SubBackward0>) tensor([14.6670]) 277\n",
      "tensor(151.5513, grad_fn=<SubBackward0>) tensor([137.9234]) 278\n",
      "tensor(150.4877, grad_fn=<SubBackward0>) tensor([56.2292]) 279\n",
      "tensor(150.1344, grad_fn=<SubBackward0>) tensor([96.6559]) 280\n",
      "tensor(159.7575, grad_fn=<SubBackward0>) tensor([16.7535]) 281\n",
      "tensor(166.9723, grad_fn=<SubBackward0>) tensor([18.7756]) 282\n",
      "tensor(139.9853, grad_fn=<SubBackward0>) tensor([62.0233]) 283\n",
      "tensor(171.4952, grad_fn=<SubBackward0>) tensor([61.4315]) 284\n",
      "tensor(164.3857, grad_fn=<SubBackward0>) tensor([91.4267]) 285\n",
      "tensor(139.2488, grad_fn=<SubBackward0>) tensor([89.4612]) 286\n",
      "tensor(144.2577, grad_fn=<SubBackward0>) tensor([114.0992]) 287\n",
      "tensor(152.1505, grad_fn=<SubBackward0>) tensor([50.3198]) 288\n",
      "tensor(154.0421, grad_fn=<SubBackward0>) tensor([132.6370]) 289\n",
      "tensor(131.7595, grad_fn=<SubBackward0>) tensor([42.1176]) 290\n",
      "tensor(136.9459, grad_fn=<SubBackward0>) tensor([61.8677]) 291\n",
      "tensor(132.9339, grad_fn=<SubBackward0>) tensor([89.7535]) 292\n",
      "tensor(140.9382, grad_fn=<SubBackward0>) tensor([49.8691]) 293\n",
      "tensor(152.6755, grad_fn=<SubBackward0>) tensor([70.1529]) 294\n",
      "tensor(144.4964, grad_fn=<SubBackward0>) tensor([84.6097]) 295\n",
      "tensor(154.1612, grad_fn=<SubBackward0>) tensor([1.4841]) 296\n",
      "tensor(165.0494, grad_fn=<SubBackward0>) tensor([95.1869]) 297\n",
      "tensor(129.6899, grad_fn=<SubBackward0>) tensor([120.1592]) 298\n",
      "tensor(152.6676, grad_fn=<SubBackward0>) tensor([111.5668]) 299\n",
      "tensor(161.3902, grad_fn=<SubBackward0>) tensor([61.2771]) 300\n",
      "tensor(144.1638, grad_fn=<SubBackward0>) tensor([107.4716]) 301\n",
      "tensor(152.8940, grad_fn=<SubBackward0>) tensor([118.7592]) 302\n",
      "tensor(157.2305, grad_fn=<SubBackward0>) tensor([108.4060]) 303\n",
      "tensor(145.8795, grad_fn=<SubBackward0>) tensor([134.0483]) 304\n",
      "tensor(164.0967, grad_fn=<SubBackward0>) tensor([46.3869]) 305\n",
      "tensor(130.9619, grad_fn=<SubBackward0>) tensor([113.8285]) 306\n",
      "tensor(132.3644, grad_fn=<SubBackward0>) tensor([9.7998]) 307\n",
      "tensor(162.9840, grad_fn=<SubBackward0>) tensor([67.2917]) 308\n",
      "tensor(139.4177, grad_fn=<SubBackward0>) tensor([86.4771]) 309\n",
      "tensor(153.6713, grad_fn=<SubBackward0>) tensor([118.2742]) 310\n",
      "tensor(150.4707, grad_fn=<SubBackward0>) tensor([85.6917]) 311\n",
      "tensor(134.9865, grad_fn=<SubBackward0>) tensor([133.7142]) 312\n",
      "tensor(126.4688, grad_fn=<SubBackward0>) tensor([13.4597]) 313\n",
      "tensor(159.3502, grad_fn=<SubBackward0>) tensor([88.1060]) 314\n",
      "tensor(146.9664, grad_fn=<SubBackward0>) tensor([58.1127]) 315\n",
      "tensor(149.4855, grad_fn=<SubBackward0>) tensor([36.6468]) 316\n",
      "tensor(146.6407, grad_fn=<SubBackward0>) tensor([47.3108]) 317\n",
      "tensor(151.8224, grad_fn=<SubBackward0>) tensor([114.0363]) 318\n",
      "tensor(155.3500, grad_fn=<SubBackward0>) tensor([26.4132]) 319\n",
      "tensor(157.9223, grad_fn=<SubBackward0>) tensor([109.3690]) 320\n",
      "tensor(137.0308, grad_fn=<SubBackward0>) tensor([104.6108]) 321\n",
      "tensor(155.0562, grad_fn=<SubBackward0>) tensor([1.6333]) 322\n",
      "tensor(159.5026, grad_fn=<SubBackward0>) tensor([156.7539]) 323\n",
      "tensor(155.6111, grad_fn=<SubBackward0>) tensor([139.2057]) 324\n",
      "tensor(151.4507, grad_fn=<SubBackward0>) tensor([98.2269]) 325\n",
      "tensor(180.8503, grad_fn=<SubBackward0>) tensor([83.3877]) 326\n",
      "tensor(149.2900, grad_fn=<SubBackward0>) tensor([124.1533]) 327\n",
      "tensor(157.9245, grad_fn=<SubBackward0>) tensor([122.1150]) 328\n",
      "tensor(159.0147, grad_fn=<SubBackward0>) tensor([53.6726]) 329\n",
      "tensor(136.2151, grad_fn=<SubBackward0>) tensor([82.3731]) 330\n",
      "tensor(145.3595, grad_fn=<SubBackward0>) tensor([34.3552]) 331\n",
      "tensor(138.7374, grad_fn=<SubBackward0>) tensor([126.4981]) 332\n",
      "tensor(151.8203, grad_fn=<SubBackward0>) tensor([7.1014]) 333\n",
      "tensor(167.8628, grad_fn=<SubBackward0>) tensor([139.6295]) 334\n",
      "tensor(162.2364, grad_fn=<SubBackward0>) tensor([40.2770]) 335\n",
      "tensor(143.3567, grad_fn=<SubBackward0>) tensor([57.5027]) 336\n",
      "tensor(158.5354, grad_fn=<SubBackward0>) tensor([155.4794]) 337\n",
      "tensor(157.0735, grad_fn=<SubBackward0>) tensor([14.3550]) 338\n",
      "tensor(135.8313, grad_fn=<SubBackward0>) tensor([118.2025]) 339\n",
      "tensor(162.9708, grad_fn=<SubBackward0>) tensor([130.9358]) 340\n",
      "tensor(166.9912, grad_fn=<SubBackward0>) tensor([154.9574]) 341\n",
      "tensor(143.4326, grad_fn=<SubBackward0>) tensor([29.7391]) 342\n",
      "tensor(133.8152, grad_fn=<SubBackward0>) tensor([88.3881]) 343\n",
      "tensor(159.7953, grad_fn=<SubBackward0>) tensor([51.9249]) 344\n",
      "tensor(150.8087, grad_fn=<SubBackward0>) tensor([97.5903]) 345\n",
      "tensor(151.8446, grad_fn=<SubBackward0>) tensor([16.6639]) 346\n",
      "tensor(165.9281, grad_fn=<SubBackward0>) tensor([91.0410]) 347\n",
      "tensor(146.3546, grad_fn=<SubBackward0>) tensor([145.0378]) 348\n",
      "tensor(152.1501, grad_fn=<SubBackward0>) tensor([145.0378]) 348\n",
      "tensor(168.1954, grad_fn=<SubBackward0>) tensor([145.0378]) 348\n",
      "tensor(145.8179, grad_fn=<SubBackward0>) tensor([34.8061]) 349\n",
      "tensor(150.0389, grad_fn=<SubBackward0>) tensor([150.1198]) 350\n",
      "tensor(151.9401, grad_fn=<SubBackward0>) tensor([17.9494]) 351\n",
      "tensor(150.0446, grad_fn=<SubBackward0>) tensor([64.5277]) 352\n",
      "tensor(153.8282, grad_fn=<SubBackward0>) tensor([47.1239]) 353\n",
      "tensor(142.8343, grad_fn=<SubBackward0>) tensor([134.8194]) 354\n",
      "tensor(140.1387, grad_fn=<SubBackward0>) tensor([121.6100]) 355\n",
      "tensor(147.6839, grad_fn=<SubBackward0>) tensor([18.5011]) 356\n",
      "tensor(161.8020, grad_fn=<SubBackward0>) tensor([28.1625]) 357\n",
      "tensor(153.7834, grad_fn=<SubBackward0>) tensor([15.0869]) 358\n",
      "tensor(167.9838, grad_fn=<SubBackward0>) tensor([42.1264]) 359\n",
      "tensor(142.0126, grad_fn=<SubBackward0>) tensor([52.4966]) 360\n",
      "tensor(154.4400, grad_fn=<SubBackward0>) tensor([25.7752]) 361\n",
      "tensor(142.6213, grad_fn=<SubBackward0>) tensor([30.0970]) 362\n",
      "tensor(149.5523, grad_fn=<SubBackward0>) tensor([58.3554]) 363\n",
      "tensor(161.7051, grad_fn=<SubBackward0>) tensor([131.8098]) 364\n",
      "tensor(147.5722, grad_fn=<SubBackward0>) tensor([72.5609]) 365\n",
      "tensor(147.4767, grad_fn=<SubBackward0>) tensor([29.0447]) 366\n",
      "tensor(147.8481, grad_fn=<SubBackward0>) tensor([78.3708]) 367\n",
      "tensor(143.4921, grad_fn=<SubBackward0>) tensor([67.1979]) 368\n",
      "tensor(175.2490, grad_fn=<SubBackward0>) tensor([37.0468]) 369\n",
      "tensor(152.8801, grad_fn=<SubBackward0>) tensor([51.9483]) 370\n",
      "tensor(137.2698, grad_fn=<SubBackward0>) tensor([32.6923]) 371\n",
      "tensor(145.1168, grad_fn=<SubBackward0>) tensor([111.8003]) 372\n",
      "tensor(148.0796, grad_fn=<SubBackward0>) tensor([80.7376]) 373\n",
      "tensor(138.0568, grad_fn=<SubBackward0>) tensor([65.5947]) 374\n",
      "tensor(158.8952, grad_fn=<SubBackward0>) tensor([76.7918]) 375\n",
      "tensor(162.6143, grad_fn=<SubBackward0>) tensor([136.1518]) 376\n",
      "tensor(155.6712, grad_fn=<SubBackward0>) tensor([95.4946]) 377\n",
      "tensor(159.0777, grad_fn=<SubBackward0>) tensor([47.8588]) 378\n",
      "tensor(157.4858, grad_fn=<SubBackward0>) tensor([37.9472]) 379\n",
      "tensor(160.2491, grad_fn=<SubBackward0>) tensor([122.2926]) 380\n",
      "tensor(170.8984, grad_fn=<SubBackward0>) tensor([98.4168]) 381\n",
      "tensor(181.6564, grad_fn=<SubBackward0>) tensor([78.7440]) 382\n",
      "tensor(152.3625, grad_fn=<SubBackward0>) tensor([27.8457]) 383\n",
      "tensor(156.2747, grad_fn=<SubBackward0>) tensor([53.0664]) 384\n",
      "tensor(132.2516, grad_fn=<SubBackward0>) tensor([77.9528]) 385\n",
      "tensor(163.3178, grad_fn=<SubBackward0>) tensor([25.0195]) 386\n",
      "tensor(162.2162, grad_fn=<SubBackward0>) tensor([25.0195]) 386\n",
      "tensor(171.0271, grad_fn=<SubBackward0>) tensor([25.0195]) 386\n",
      "tensor(149.0669, grad_fn=<SubBackward0>) tensor([97.1950]) 387\n",
      "tensor(136.5939, grad_fn=<SubBackward0>) tensor([85.7266]) 388\n",
      "tensor(137.2237, grad_fn=<SubBackward0>) tensor([25.7882]) 389\n",
      "tensor(154.1050, grad_fn=<SubBackward0>) tensor([84.9883]) 390\n",
      "tensor(174.2292, grad_fn=<SubBackward0>) tensor([111.1062]) 391\n",
      "tensor(149.5272, grad_fn=<SubBackward0>) tensor([113.4090]) 392\n",
      "tensor(155.6298, grad_fn=<SubBackward0>) tensor([148.2979]) 393\n",
      "tensor(164.6480, grad_fn=<SubBackward0>) tensor([99.6188]) 394\n",
      "tensor(147.6859, grad_fn=<SubBackward0>) tensor([3.1935]) 395\n",
      "tensor(137.1087, grad_fn=<SubBackward0>) tensor([73.1776]) 396\n",
      "tensor(150.0316, grad_fn=<SubBackward0>) tensor([73.1776]) 396\n",
      "tensor(256.4287, grad_fn=<SubBackward0>) tensor([73.1776]) 396\n",
      "tensor(175.6062, grad_fn=<SubBackward0>) tensor([113.6783]) 397\n",
      "tensor(148.5204, grad_fn=<SubBackward0>) tensor([52.8303]) 398\n",
      "tensor(148.7541, grad_fn=<SubBackward0>) tensor([48.8447]) 399\n",
      "tensor(146.1748, grad_fn=<SubBackward0>) tensor([58.0409]) 400\n",
      "tensor(165.8657, grad_fn=<SubBackward0>) tensor([15.5967]) 401\n",
      "tensor(139.7737, grad_fn=<SubBackward0>) tensor([136.1319]) 402\n",
      "tensor(177.8814, grad_fn=<SubBackward0>) tensor([88.8508]) 403\n",
      "tensor(160.7087, grad_fn=<SubBackward0>) tensor([65.5227]) 404\n",
      "tensor(161.8529, grad_fn=<SubBackward0>) tensor([55.2419]) 405\n",
      "tensor(139.1776, grad_fn=<SubBackward0>) tensor([127.3437]) 406\n",
      "tensor(151.2798, grad_fn=<SubBackward0>) tensor([52.9071]) 407\n",
      "tensor(158.4211, grad_fn=<SubBackward0>) tensor([22.7173]) 408\n",
      "tensor(160.5117, grad_fn=<SubBackward0>) tensor([94.0632]) 409\n",
      "tensor(157.5109, grad_fn=<SubBackward0>) tensor([18.9024]) 410\n",
      "tensor(141.3569, grad_fn=<SubBackward0>) tensor([20.1482]) 411\n",
      "tensor(137.7053, grad_fn=<SubBackward0>) tensor([0.0763]) 412\n",
      "tensor(137.6093, grad_fn=<SubBackward0>) tensor([0.0763]) 412\n",
      "tensor(175.3890, grad_fn=<SubBackward0>) tensor([0.0763]) 412\n",
      "tensor(152.5707, grad_fn=<SubBackward0>) tensor([76.1195]) 413\n",
      "tensor(145.7100, grad_fn=<SubBackward0>) tensor([8.2783]) 414\n",
      "tensor(150.0243, grad_fn=<SubBackward0>) tensor([81.7424]) 415\n",
      "tensor(160.9704, grad_fn=<SubBackward0>) tensor([77.9993]) 416\n",
      "tensor(156.0943, grad_fn=<SubBackward0>) tensor([67.5855]) 417\n",
      "tensor(142.0431, grad_fn=<SubBackward0>) tensor([94.9663]) 418\n",
      "tensor(147.3795, grad_fn=<SubBackward0>) tensor([123.9252]) 419\n",
      "tensor(147.6748, grad_fn=<SubBackward0>) tensor([22.6319]) 420\n",
      "tensor(138.6380, grad_fn=<SubBackward0>) tensor([26.9882]) 421\n",
      "tensor(134.0318, grad_fn=<SubBackward0>) tensor([86.7420]) 422\n",
      "tensor(152.1373, grad_fn=<SubBackward0>) tensor([101.2266]) 423\n",
      "tensor(161.3219, grad_fn=<SubBackward0>) tensor([141.0996]) 424\n",
      "tensor(163.6056, grad_fn=<SubBackward0>) tensor([6.5374]) 425\n",
      "tensor(138.6818, grad_fn=<SubBackward0>) tensor([16.7518]) 426\n",
      "tensor(163.0543, grad_fn=<SubBackward0>) tensor([12.8263]) 427\n",
      "tensor(163.8285, grad_fn=<SubBackward0>) tensor([160.8243]) 428\n",
      "tensor(155.3920, grad_fn=<SubBackward0>) tensor([114.3154]) 429\n",
      "tensor(150.9976, grad_fn=<SubBackward0>) tensor([31.8350]) 430\n",
      "tensor(134.8744, grad_fn=<SubBackward0>) tensor([83.6757]) 431\n",
      "tensor(156.7945, grad_fn=<SubBackward0>) tensor([76.2305]) 432\n",
      "tensor(130.7982, grad_fn=<SubBackward0>) tensor([21.4035]) 433\n",
      "tensor(149.7990, grad_fn=<SubBackward0>) tensor([62.3534]) 434\n",
      "tensor(154.0175, grad_fn=<SubBackward0>) tensor([149.4392]) 435\n",
      "tensor(159.1052, grad_fn=<SubBackward0>) tensor([100.7956]) 436\n",
      "tensor(175.4771, grad_fn=<SubBackward0>) tensor([155.0461]) 437\n",
      "tensor(140.2061, grad_fn=<SubBackward0>) tensor([47.5074]) 438\n",
      "tensor(159.9304, grad_fn=<SubBackward0>) tensor([89.8750]) 439\n",
      "tensor(134.6109, grad_fn=<SubBackward0>) tensor([110.2852]) 440\n",
      "tensor(146.5909, grad_fn=<SubBackward0>) tensor([59.0038]) 441\n",
      "tensor(137.8723, grad_fn=<SubBackward0>) tensor([55.8410]) 442\n",
      "tensor(134.3628, grad_fn=<SubBackward0>) tensor([4.9747]) 443\n",
      "tensor(154.9092, grad_fn=<SubBackward0>) tensor([116.5073]) 444\n",
      "tensor(136.2652, grad_fn=<SubBackward0>) tensor([61.5898]) 445\n",
      "tensor(132.7757, grad_fn=<SubBackward0>) tensor([39.2055]) 446\n",
      "tensor(165.5101, grad_fn=<SubBackward0>) tensor([153.0673]) 447\n",
      "tensor(165.2424, grad_fn=<SubBackward0>) tensor([106.1653]) 448\n",
      "tensor(159.0464, grad_fn=<SubBackward0>) tensor([115.6558]) 449\n",
      "tensor(157.0201, grad_fn=<SubBackward0>) tensor([122.3402]) 450\n",
      "tensor(150.7773, grad_fn=<SubBackward0>) tensor([72.9846]) 451\n",
      "tensor(158.5807, grad_fn=<SubBackward0>) tensor([120.1731]) 452\n",
      "tensor(155.1722, grad_fn=<SubBackward0>) tensor([128.5815]) 453\n",
      "tensor(158.8172, grad_fn=<SubBackward0>) tensor([147.9885]) 454\n",
      "tensor(157.2226, grad_fn=<SubBackward0>) tensor([86.6549]) 455\n",
      "tensor(154.6575, grad_fn=<SubBackward0>) tensor([59.8983]) 456\n",
      "tensor(166.6842, grad_fn=<SubBackward0>) tensor([93.1722]) 457\n",
      "tensor(153.4105, grad_fn=<SubBackward0>) tensor([145.9106]) 458\n",
      "tensor(165.4406, grad_fn=<SubBackward0>) tensor([58.6869]) 459\n",
      "tensor(147.7774, grad_fn=<SubBackward0>) tensor([61.5719]) 460\n",
      "tensor(151.3979, grad_fn=<SubBackward0>) tensor([50.2852]) 461\n",
      "tensor(153.3655, grad_fn=<SubBackward0>) tensor([144.9919]) 462\n",
      "tensor(129.4093, grad_fn=<SubBackward0>) tensor([129.3932]) 463\n",
      "tensor(163.9144, grad_fn=<SubBackward0>) tensor([82.4520]) 464\n",
      "tensor(163.2539, grad_fn=<SubBackward0>) tensor([1.3526]) 465\n",
      "tensor(153.5928, grad_fn=<SubBackward0>) tensor([64.6062]) 466\n",
      "tensor(133.3074, grad_fn=<SubBackward0>) tensor([6.3636]) 467\n",
      "tensor(149.5794, grad_fn=<SubBackward0>) tensor([11.2021]) 468\n",
      "tensor(149.9824, grad_fn=<SubBackward0>) tensor([72.9144]) 469\n",
      "tensor(141.1765, grad_fn=<SubBackward0>) tensor([93.4262]) 470\n",
      "tensor(162.7580, grad_fn=<SubBackward0>) tensor([4.1642]) 471\n",
      "tensor(145.7629, grad_fn=<SubBackward0>) tensor([119.3560]) 472\n",
      "tensor(162.5868, grad_fn=<SubBackward0>) tensor([23.5224]) 473\n",
      "tensor(123.8161, grad_fn=<SubBackward0>) tensor([34.0232]) 474\n",
      "tensor(151.0247, grad_fn=<SubBackward0>) tensor([146.2646]) 475\n",
      "tensor(152.5751, grad_fn=<SubBackward0>) tensor([150.0316]) 476\n",
      "tensor(166.4920, grad_fn=<SubBackward0>) tensor([131.5116]) 477\n",
      "tensor(169.9378, grad_fn=<SubBackward0>) tensor([42.3414]) 478\n",
      "tensor(156.7696, grad_fn=<SubBackward0>) tensor([148.6046]) 479\n",
      "tensor(162.2990, grad_fn=<SubBackward0>) tensor([16.6044]) 480\n",
      "tensor(170.4340, grad_fn=<SubBackward0>) tensor([128.8130]) 481\n",
      "tensor(123.2824, grad_fn=<SubBackward0>) tensor([98.9921]) 482\n",
      "tensor(158.1199, grad_fn=<SubBackward0>) tensor([101.9873]) 483\n",
      "tensor(163.2122, grad_fn=<SubBackward0>) tensor([15.3356]) 484\n",
      "tensor(154.0621, grad_fn=<SubBackward0>) tensor([99.9921]) 485\n",
      "tensor(141.1204, grad_fn=<SubBackward0>) tensor([75.0941]) 486\n",
      "tensor(152.2915, grad_fn=<SubBackward0>) tensor([76.2633]) 487\n",
      "tensor(149.8637, grad_fn=<SubBackward0>) tensor([74.1567]) 488\n",
      "tensor(154.0398, grad_fn=<SubBackward0>) tensor([125.5111]) 489\n",
      "tensor(154.2755, grad_fn=<SubBackward0>) tensor([70.3572]) 490\n",
      "tensor(155.8203, grad_fn=<SubBackward0>) tensor([83.9273]) 491\n",
      "tensor(168.5211, grad_fn=<SubBackward0>) tensor([50.8770]) 492\n",
      "tensor(144.3752, grad_fn=<SubBackward0>) tensor([107.4685]) 493\n",
      "tensor(160.7917, grad_fn=<SubBackward0>) tensor([2.7157]) 494\n",
      "tensor(164.7226, grad_fn=<SubBackward0>) tensor([123.8046]) 495\n",
      "tensor(171.2425, grad_fn=<SubBackward0>) tensor([47.6371]) 496\n",
      "tensor(169.9391, grad_fn=<SubBackward0>) tensor([35.2220]) 497\n",
      "tensor(147.0433, grad_fn=<SubBackward0>) tensor([46.0607]) 498\n",
      "tensor(161.6595, grad_fn=<SubBackward0>) tensor([111.2305]) 499\n"
     ]
    }
   ],
   "source": [
    "def scan(func, init_values, length):\n",
    "    carry = init_values\n",
    "    result = []\n",
    "    for i in range(length):\n",
    "        carry = func(carry,i)\n",
    "        result.append(carry)\n",
    "    return carry, result \n",
    "\n",
    "final_state, result = scan(hmc_kernel.update, init_values=params, length=num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.2\n",
    "vmin, vmax = X.min() - step, X.max() + step\n",
    "X_grid = torch.mgrid[vmin:vmax:100j, vmin:vmax:100j]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
