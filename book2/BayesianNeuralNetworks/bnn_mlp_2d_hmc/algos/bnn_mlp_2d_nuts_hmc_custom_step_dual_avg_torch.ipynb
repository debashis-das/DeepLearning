{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Callable\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2]) torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "class MLP1D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_shapes=[2,10,10,10], out_shapes = [10,10,10,1]):\n",
    "        super().__init__()\n",
    "        self.linears = [nn.Linear(in_shapes[i], out_shapes[i]) for i in range(len(in_shapes))]\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def model(self):\n",
    "        return nn.Sequential(self.linears[0], self.relu, \n",
    "                             self.linears[1], self.relu, \n",
    "                             self.linears[2], self.relu, self.linears[3])\n",
    "\n",
    "    def forward(self, params, X):\n",
    "        self.weights_and_biases(params)\n",
    "        return self.model()(X)\n",
    "    \n",
    "    def weights_and_biases(self, params):\n",
    "        for i,param in enumerate(params):\n",
    "            self.linears[i].weight = nn.Parameter(param['weight'])\n",
    "            self.linears[i].bias = nn.Parameter(param['bias'])\n",
    "    \n",
    "    def fetch_params(self):\n",
    "        params = []\n",
    "        for linear in self.linears:\n",
    "            params.append({\n",
    "                'weight': linear.weight.detach(),\n",
    "                'bias': linear.bias.detach()\n",
    "            })\n",
    "        return params\n",
    "\n",
    "def ravel_params(params):\n",
    "    def ravel_per_item():\n",
    "        all_params = []\n",
    "        def ravel_per_item_per_key(param):\n",
    "            for value in param.values(): \n",
    "                all_params.append(value.ravel())\n",
    "\n",
    "        if type(params) == list:\n",
    "            for param in params:\n",
    "                ravel_per_item_per_key(param)\n",
    "        else:\n",
    "            ravel_per_item_per_key(params)\n",
    "        return all_params\n",
    "    return torch.cat(ravel_per_item())\n",
    "\n",
    "def unravel_params(params_raveled: torch.Tensor, structured_tuple):\n",
    "    \n",
    "    def unravel_all_shapes(structure, is_list):\n",
    "        shapes = []\n",
    "        def extract_shapes(structure):\n",
    "            if is_list:\n",
    "                for per_structure in structure:\n",
    "                    for val in per_structure.values():\n",
    "                        shapes.append(val.numel())\n",
    "            else:\n",
    "                for val in structure.values():\n",
    "                    shapes.append(val.numel())\n",
    "            values_per_structure = params_raveled.split(shapes)\n",
    "            return values_per_structure\n",
    "        \n",
    "        values_per_structure = extract_shapes(structure)\n",
    "        result = []\n",
    "        index = [0]\n",
    "\n",
    "        def result_per_dict(values_per_structure, per_structure):\n",
    "            per_result = {}\n",
    "            for key, val in per_structure.items():\n",
    "                # print(key, val.shape, values_per_structure[index[0]].shape)\n",
    "                per_result[key] = values_per_structure[index[0]].reshape(val.shape)\n",
    "                index[0] += 1\n",
    "            return per_result\n",
    "        \n",
    "        if is_list:\n",
    "            for per_structure in structure:\n",
    "                per_result = result_per_dict(values_per_structure, per_structure)\n",
    "                result.append(per_result)\n",
    "            return result\n",
    "        else:\n",
    "            return result_per_dict(values_per_structure, structure)\n",
    "        \n",
    "    return unravel_all_shapes(structured_tuple, type(structured_tuple) == list)\n",
    "    \n",
    "def bnn_log_joint(params, X, y, model:MLP1D):\n",
    "    logits = model.forward(params, X).ravel()\n",
    "    flatten_params = ravel_params(params)\n",
    "    log_prior = torch.distributions.Normal(0.0, 1.0).log_prob(flatten_params).sum()\n",
    "    log_likelihood = torch.distributions.Bernoulli(logits=logits).log_prob(y).sum()\n",
    "    log_joint = log_prior + log_likelihood\n",
    "    return log_joint\n",
    "\n",
    "noise = 0.2\n",
    "num_samples = 50\n",
    "num_warmup = 1000\n",
    "num_steps = 500\n",
    "in_shapes = [2,10,10,10]\n",
    "out_shapes = [10,10,10,1]\n",
    "\n",
    "X, y = make_moons(n_samples=num_samples, noise=noise, random_state=314)\n",
    "model = MLP1D(in_shapes=in_shapes, out_shapes=out_shapes)\n",
    "params = model.fetch_params()\n",
    "potential = partial(bnn_log_joint, X=torch.Tensor(X), y=torch.Tensor(y), model=model)\n",
    "\n",
    "\n",
    "# Test\n",
    "print(torch.Tensor(X).shape, torch.Tensor(y).shape)\n",
    "# potential(params)\n",
    "# print(ravel_params(params[0]).shape)\n",
    "# print(unravel_params(ravel_params(params[1]),params[1]))\n",
    "# print(unravel_params(ravel_params(params),params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WelfordComputation:\n",
    "\n",
    "    def __init__(self, params_shape):\n",
    "        self.parameters = (torch.zeros(params_shape), torch.zeros(params_shape), 0)\n",
    "\n",
    "    def update_step(self, value):\n",
    "        mean, m2, no_of_samples = self.parameters\n",
    "        no_of_samples += 1\n",
    "        delta_change = value - mean\n",
    "        mean = mean + delta_change/no_of_samples\n",
    "        new_delta_change = value - mean\n",
    "        new_m2 = m2 + delta_change*new_delta_change\n",
    "        self.parameters = (mean, new_m2, no_of_samples)\n",
    "    \n",
    "    def final_step(self):\n",
    "        mean, m2, no_of_samples = self.parameters\n",
    "        baised_variance = m2/no_of_samples\n",
    "        unbaised_variance = m2/(no_of_samples-1)\n",
    "        return (mean, baised_variance, unbaised_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyParameters:\n",
    "\n",
    "    def __init__(self, potential_energy_fn:Callable, precision):\n",
    "        self.potential_energy_fn = potential_energy_fn \n",
    "        self.precision = precision\n",
    "    \n",
    "    def set_position(self, position):\n",
    "        self.position = position\n",
    "        self.potential_energy = self.potential_energy_fn(position)\n",
    "        self.potential_energy_grad = torch.func.grad(self.potential_energy_fn)(position)\n",
    "    \n",
    "    def set_velocity(self, velocity):\n",
    "        self.velocity = velocity\n",
    "        self.kinetic_energy = self.kinetic_energy_fn(self.velocity)\n",
    "        self.kinetic_energy_grad = torch.func.grad(self.kinetic_energy_fn)(self.velocity)\n",
    "\n",
    "    def init_per_step(self, position, velocity):\n",
    "        self.set_position(position)\n",
    "        self.set_velocity(velocity)\n",
    "        self.total_init_energy = self.total_current_energy()\n",
    "    \n",
    "    def kinetic_energy_fn(self, velocity:torch.Tensor):\n",
    "        return 0.5*torch.matmul(velocity, torch.matmul(self.precision, velocity.T))\n",
    "    \n",
    "    def update_position(self, step_size_with_direction):\n",
    "        position_raveled = ravel_params(self.position)\n",
    "        result = position_raveled + step_size_with_direction*self.kinetic_energy_grad\n",
    "        n_position = unravel_params(result, self.position)\n",
    "        self.set_position(n_position)\n",
    "        return n_position\n",
    "\n",
    "    def updated_velocity(self, step_size_with_direction, is_half_step_momentum = False):\n",
    "        n_velocity = torch.subtract(self.velocity, step_size_with_direction*(0.5 if is_half_step_momentum else 1)*ravel_params(self.potential_energy_grad))\n",
    "        self.set_velocity(n_velocity)\n",
    "        return n_velocity\n",
    "    \n",
    "    def delta_energy(self):\n",
    "        return - self.total_current_energy() + self.total_init_energy \n",
    "    \n",
    "    def total_current_energy(self):\n",
    "        return - self.potential_energy - self.kinetic_energy\n",
    "\n",
    "# Test\n",
    "\n",
    "# e_params = EnergyParameters(potential_energy_fn=potential)\n",
    "# e_params.set_energy_parameters(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient No-U-Turn Sampler with Dual Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size (find_reasonable_epsilon) : 0.1599999964237213\n",
      "1 tensor([0.4908], grad_fn=<ExpBackward0>) tensor(1.4091e-08, grad_fn=<DivBackward0>)\n",
      "2 tensor([0.1602], grad_fn=<ExpBackward0>) tensor(0., grad_fn=<DivBackward0>)\n",
      "3 tensor([0.1448], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "4 tensor([0.0673], grad_fn=<ExpBackward0>) tensor(0., grad_fn=<DivBackward0>)\n",
      "5 tensor([0.0569], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "6 tensor([0.0298], grad_fn=<ExpBackward0>) tensor(0., grad_fn=<DivBackward0>)\n",
      "7 tensor([0.0245], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "8 tensor([0.0138], grad_fn=<ExpBackward0>) tensor(0., grad_fn=<DivBackward0>)\n",
      "9 tensor([0.0061], grad_fn=<ExpBackward0>) tensor(7.5760e-06, grad_fn=<DivBackward0>)\n",
      "10 tensor([0.0023], grad_fn=<ExpBackward0>) tensor(5.0953e-08, grad_fn=<DivBackward0>)\n",
      "11 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(0.0922, grad_fn=<DivBackward0>)\n",
      "12 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "13 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "14 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "15 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "16 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(0.4882, grad_fn=<DivBackward0>)\n",
      "17 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "18 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "19 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(0.0198, grad_fn=<DivBackward0>)\n",
      "20 tensor([0.0005], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "21 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(0.0003, grad_fn=<DivBackward0>)\n",
      "22 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "23 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(7.5463e-09, grad_fn=<DivBackward0>)\n",
      "24 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "25 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "26 tensor([0.0005], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "27 tensor([0.0005], grad_fn=<ExpBackward0>) tensor(0.4598, grad_fn=<DivBackward0>)\n",
      "28 tensor([0.0005], grad_fn=<ExpBackward0>) tensor(0.0229, grad_fn=<DivBackward0>)\n",
      "29 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "30 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "31 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "32 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "33 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(5.6229e-27, grad_fn=<DivBackward0>)\n",
      "34 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "35 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(3.1399e-12, grad_fn=<DivBackward0>)\n",
      "36 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "37 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "38 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(0.9618, grad_fn=<DivBackward0>)\n",
      "39 tensor([0.0005], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "40 tensor([0.0005], grad_fn=<ExpBackward0>) tensor(0.5156, grad_fn=<DivBackward0>)\n",
      "41 tensor([0.0006], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "42 tensor([0.0007], grad_fn=<ExpBackward0>) tensor(0.0024, grad_fn=<DivBackward0>)\n",
      "43 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "44 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "45 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(0.2247, grad_fn=<DivBackward0>)\n",
      "46 tensor([0.0011], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "47 tensor([0.0011], grad_fn=<ExpBackward0>) tensor(1.4133e-06, grad_fn=<DivBackward0>)\n",
      "48 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "49 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(0.3187, grad_fn=<DivBackward0>)\n",
      "50 tensor([0.0014], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "51 tensor([0.0016], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "52 tensor([0.0018], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "53 tensor([0.0020], grad_fn=<ExpBackward0>) tensor(0.0003, grad_fn=<DivBackward0>)\n",
      "54 tensor([0.0019], grad_fn=<ExpBackward0>) tensor(0.0388, grad_fn=<DivBackward0>)\n",
      "55 tensor([0.0020], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "56 tensor([0.0022], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "57 tensor([0.0022], grad_fn=<ExpBackward0>) tensor(0.0031, grad_fn=<DivBackward0>)\n",
      "58 tensor([0.0023], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "59 tensor([0.0025], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "60 tensor([0.0025], grad_fn=<ExpBackward0>) tensor(7.7036e-05, grad_fn=<DivBackward0>)\n",
      "61 tensor([0.0023], grad_fn=<ExpBackward0>) tensor(1.5584e-15, grad_fn=<DivBackward0>)\n",
      "62 tensor([0.0021], grad_fn=<ExpBackward0>) tensor(2.5536e-07, grad_fn=<DivBackward0>)\n",
      "63 tensor([0.0018], grad_fn=<ExpBackward0>) tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "64 tensor([0.0016], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "65 tensor([0.0014], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "66 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "67 tensor([0.0011], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "68 tensor([0.0011], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "69 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(0.0359, grad_fn=<DivBackward0>)\n",
      "70 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "71 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "72 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "73 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "74 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(9.3254e-10, grad_fn=<DivBackward0>)\n",
      "75 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "76 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "77 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "78 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(0.0164, grad_fn=<DivBackward0>)\n",
      "79 tensor([0.0011], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "80 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(2.2138e-10, grad_fn=<DivBackward0>)\n",
      "81 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(6.7598e-06, grad_fn=<DivBackward0>)\n",
      "82 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "83 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "84 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(0.1724, grad_fn=<DivBackward0>)\n",
      "85 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(0.0573, grad_fn=<DivBackward0>)\n",
      "86 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "87 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "88 tensor([0.0007], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "89 tensor([0.0007], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "90 tensor([0.0007], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "91 tensor([0.0007], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "92 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "93 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(0.0028, grad_fn=<DivBackward0>)\n",
      "94 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "95 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "96 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "97 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "98 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "99 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(0.1541, grad_fn=<DivBackward0>)\n",
      "100 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(0.6841, grad_fn=<DivBackward0>)\n",
      "101 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "102 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "103 tensor([0.0011], grad_fn=<ExpBackward0>) tensor(0.4435, grad_fn=<DivBackward0>)\n",
      "104 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "105 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(2.8887e-05, grad_fn=<DivBackward0>)\n",
      "106 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "107 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "108 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "109 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "110 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "111 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(0.0099, grad_fn=<DivBackward0>)\n",
      "112 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "113 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "114 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(2.7003e-10, grad_fn=<DivBackward0>)\n",
      "115 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(0.0003, grad_fn=<DivBackward0>)\n",
      "116 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "117 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "118 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "119 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "120 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "121 tensor([0.0014], grad_fn=<ExpBackward0>) tensor(0.0202, grad_fn=<DivBackward0>)\n",
      "122 tensor([0.0014], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "123 tensor([0.0014], grad_fn=<ExpBackward0>) tensor(0.1910, grad_fn=<DivBackward0>)\n",
      "124 tensor([0.0014], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "125 tensor([0.0014], grad_fn=<ExpBackward0>) tensor(8.0527e-06, grad_fn=<DivBackward0>)\n",
      "126 tensor([0.0015], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "127 tensor([0.0015], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "128 tensor([0.0015], grad_fn=<ExpBackward0>) tensor(0.0252, grad_fn=<DivBackward0>)\n",
      "129 tensor([0.0015], grad_fn=<ExpBackward0>) tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "130 tensor([0.0014], grad_fn=<ExpBackward0>) tensor(0.3218, grad_fn=<DivBackward0>)\n",
      "131 tensor([0.0013], grad_fn=<ExpBackward0>) tensor(3.6985e-08, grad_fn=<DivBackward0>)\n",
      "132 tensor([0.0012], grad_fn=<ExpBackward0>) tensor(0.1632, grad_fn=<DivBackward0>)\n",
      "133 tensor([0.0011], grad_fn=<ExpBackward0>) tensor(0.0049, grad_fn=<DivBackward0>)\n",
      "134 tensor([0.0010], grad_fn=<ExpBackward0>) tensor(0.1400, grad_fn=<DivBackward0>)\n",
      "135 tensor([0.0009], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "136 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "137 tensor([0.0008], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "138 tensor([0.0007], grad_fn=<ExpBackward0>) tensor(0.1517, grad_fn=<DivBackward0>)\n",
      "139 tensor([0.0006], grad_fn=<ExpBackward0>) tensor(0.0479, grad_fn=<DivBackward0>)\n",
      "140 tensor([0.0006], grad_fn=<ExpBackward0>) tensor(0.1973, grad_fn=<DivBackward0>)\n",
      "141 tensor([0.0005], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "142 tensor([0.0005], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "143 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "144 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(0.2204, grad_fn=<DivBackward0>)\n",
      "145 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "146 tensor([0.0004], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "147 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(0.2151, grad_fn=<DivBackward0>)\n",
      "148 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "149 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(0.6353, grad_fn=<DivBackward0>)\n",
      "150 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(0.0554, grad_fn=<DivBackward0>)\n",
      "151 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "152 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "153 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(0.4955, grad_fn=<DivBackward0>)\n",
      "154 tensor([0.0003], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "155 tensor([0.0002], grad_fn=<ExpBackward0>) tensor(5.0357e-08, grad_fn=<DivBackward0>)\n",
      "156 tensor([0.0002], grad_fn=<ExpBackward0>) tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "157 tensor([0.0002], grad_fn=<ExpBackward0>) tensor(0.0018, grad_fn=<DivBackward0>)\n",
      "158 tensor([0.0002], grad_fn=<ExpBackward0>) tensor(0.0014, grad_fn=<DivBackward0>)\n",
      "159 tensor([0.0002], grad_fn=<ExpBackward0>) tensor(1.0037e-06, grad_fn=<DivBackward0>)\n",
      "160 tensor([0.0001], grad_fn=<ExpBackward0>) tensor(0.0158, grad_fn=<DivBackward0>)\n",
      "161 tensor([0.0001], grad_fn=<ExpBackward0>) tensor(1.2414e-12, grad_fn=<DivBackward0>)\n",
      "162 tensor([0.0001], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "163 tensor([9.1244e-05], grad_fn=<ExpBackward0>) tensor(4.2276e-15, grad_fn=<DivBackward0>)\n",
      "164 tensor([7.9419e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "165 tensor([6.9151e-05], grad_fn=<ExpBackward0>) tensor(0.5170, grad_fn=<DivBackward0>)\n",
      "166 tensor([6.1159e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "167 tensor([5.3233e-05], grad_fn=<ExpBackward0>) tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "168 tensor([4.7056e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "169 tensor([4.2222e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "170 tensor([3.8434e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "171 tensor([3.4416e-05], grad_fn=<ExpBackward0>) tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "172 tensor([3.0547e-05], grad_fn=<ExpBackward0>) tensor(0.2368, grad_fn=<DivBackward0>)\n",
      "173 tensor([2.7507e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "174 tensor([2.5118e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "175 tensor([2.3248e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "176 tensor([2.1799e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "177 tensor([2.0242e-05], grad_fn=<ExpBackward0>) tensor(0.2359, grad_fn=<DivBackward0>)\n",
      "178 tensor([1.9038e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "179 tensor([1.7695e-05], grad_fn=<ExpBackward0>) tensor(0.1648, grad_fn=<DivBackward0>)\n",
      "180 tensor([1.6656e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "181 tensor([1.5423e-05], grad_fn=<ExpBackward0>) tensor(3.0049e-09, grad_fn=<DivBackward0>)\n",
      "182 tensor([1.4462e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "183 tensor([1.3346e-05], grad_fn=<ExpBackward0>) tensor(3.1276e-06, grad_fn=<DivBackward0>)\n",
      "184 tensor([1.2138e-05], grad_fn=<ExpBackward0>) tensor(0.0367, grad_fn=<DivBackward0>)\n",
      "185 tensor([1.1182e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "186 tensor([1.0430e-05], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "187 tensor([9.8461e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "188 tensor([9.1502e-06], grad_fn=<ExpBackward0>) tensor(6.0738e-05, grad_fn=<DivBackward0>)\n",
      "189 tensor([8.6058e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "190 tensor([7.9917e-06], grad_fn=<ExpBackward0>) tensor(0.0992, grad_fn=<DivBackward0>)\n",
      "191 tensor([7.3457e-06], grad_fn=<ExpBackward0>) tensor(0.1750, grad_fn=<DivBackward0>)\n",
      "192 tensor([6.8336e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "193 tensor([6.4317e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "194 tensor([6.1222e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "195 tensor([5.8917e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "196 tensor([5.5835e-06], grad_fn=<ExpBackward0>) tensor(3.6982e-29, grad_fn=<DivBackward0>)\n",
      "197 tensor([5.2146e-06], grad_fn=<ExpBackward0>) tensor(0.0096, grad_fn=<DivBackward0>)\n",
      "198 tensor([4.9251e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "199 tensor([4.5842e-06], grad_fn=<ExpBackward0>) tensor(4.7420e-05, grad_fn=<DivBackward0>)\n",
      "200 tensor([4.2070e-06], grad_fn=<ExpBackward0>) tensor(2.5006e-10, grad_fn=<DivBackward0>)\n",
      "201 tensor([3.9054e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "202 tensor([3.5754e-06], grad_fn=<ExpBackward0>) tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "203 tensor([3.3109e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "204 tensor([3.1000e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "205 tensor([2.9340e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "206 tensor([2.7381e-06], grad_fn=<ExpBackward0>) tensor(1.4564e-08, grad_fn=<DivBackward0>)\n",
      "207 tensor([2.5208e-06], grad_fn=<ExpBackward0>) tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "208 tensor([2.2905e-06], grad_fn=<ExpBackward0>) tensor(0.0018, grad_fn=<DivBackward0>)\n",
      "209 tensor([2.1047e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "210 tensor([1.9552e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "211 tensor([1.8357e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n",
      "212 tensor([1.7350e-06], grad_fn=<ExpBackward0>) tensor(0.8474, grad_fn=<DivBackward0>)\n",
      "213 tensor([1.6564e-06], grad_fn=<ExpBackward0>) tensor(1., grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class HMCAlgorithm:\n",
    "    \n",
    "    def __init__(self, log_density_fn: Callable, precision, position_shape, expected_prob_density=0.65, Lambda = 1, m_warmup=1000, m_after_warmup=50):\n",
    "        self.log_density_fn = log_density_fn\n",
    "        # Lambda\n",
    "        self.Lambda = Lambda\n",
    "        # delta\n",
    "        self.expected_prob_density = expected_prob_density\n",
    "        # M_adapt\n",
    "        self.m_warmup = m_warmup\n",
    "        # M\n",
    "        self.M = m_warmup +  m_after_warmup\n",
    "        self.ep = EnergyParameters(potential_energy_fn=log_density_fn, precision=precision)\n",
    "        self.welford_position = WelfordComputation(position_shape)\n",
    "    \n",
    "\n",
    "    def generate_default_velocity(self, position_shape):\n",
    "\n",
    "        def generate_random_gaussian(covariance: torch.Tensor):\n",
    "            return torch.distributions.MultivariateNormal(loc=torch.zeros(covariance.shape[-1]),covariance_matrix=covariance).sample()\n",
    "\n",
    "        def generate_inverse_mass_matrix_and_sigma(covariance_shape: int):\n",
    "            covariance = torch.eye(covariance_shape)\n",
    "            inverse_mass_matrix = covariance.inverse()\n",
    "            return inverse_mass_matrix, covariance\n",
    "        \n",
    "        _, covariance = generate_inverse_mass_matrix_and_sigma(position_shape)\n",
    "        return generate_random_gaussian(covariance)\n",
    "\n",
    "    # welford algorithm\n",
    "    def square_distance_from_mean(self, position):\n",
    "        mean, m2, no_of_samples = self.parameters\n",
    "        no_of_samples += 1\n",
    "        delta_change = position - mean\n",
    "        mean = mean + delta_change/no_of_samples\n",
    "        new_delta_change = position - mean\n",
    "        new_m2 = m2 + delta_change*new_delta_change\n",
    "        self.parameters = (mean, new_m2, no_of_samples)\n",
    "\n",
    "    def find_reasonable_epsilon(self, position):\n",
    "        step_size, velocity = 0.01, self.generate_default_velocity(ravel_params(position).shape[-1])\n",
    "        delta_energy, proposal_position, proposal_velocity, _ = self.step_integrator(position, velocity, step_size)\n",
    "        alpha = 2*(1 if torch.exp(delta_energy) > 0.5 else 0) - 1\n",
    "        while torch.pow(torch.exp(delta_energy), alpha) > torch.pow(torch.tensor(0.5), alpha):\n",
    "            step_size = torch.pow(torch.tensor(2), alpha) * step_size\n",
    "            delta_energy, proposal_position, proposal_velocity, _ = self.step_integrator(proposal_position, proposal_velocity, step_size)\n",
    "        print(f'Step size (find_reasonable_epsilon) : {step_size}')\n",
    "        return step_size\n",
    "\n",
    "    def current_direction(self):\n",
    "        direction = torch.distributions.Bernoulli(logits=torch.tensor([0.5])).sample()\n",
    "        return -1 if direction.bool() else 1\n",
    "\n",
    "    def stop_condition(self, p_minus, v_minus, p_plus, v_plus):\n",
    "        diff = ravel_params(p_plus)-ravel_params(p_minus)\n",
    "        condition = torch.all(diff*v_minus >= torch.zeros(diff.shape)) and torch.all(diff*v_plus >= torch.zeros(diff.shape))\n",
    "        return 1 if condition else 0\n",
    "\n",
    "    def build_tree(self, position, velocity, energy, direction, iter_no, step_size, init_energy, m):\n",
    "        delta_max = torch.tensor(1000)\n",
    "        if iter_no == 0:\n",
    "            delta_energy, new_position, new_velocity, new_energy = self.step_integrator(position, velocity, direction*step_size)\n",
    "            n_leaves = 1 if energy <= new_energy else 0\n",
    "            s = 1 if energy < torch.log(delta_max) + new_energy  else 0\n",
    "            # print(f'build_tree {m} : delta, steps :', delta_energy, direction*step_size)\n",
    "            return new_position, new_velocity, new_position, new_velocity, new_position, n_leaves, s, torch.min(torch.tensor(1), torch.exp(-new_energy+init_energy)), 1\n",
    "        else:\n",
    "            p_minus, v_minus, p_plus, v_plus, p, n_leaves, s, alpha, n_alpha = self.build_tree(position, velocity, energy, direction, iter_no-1, step_size, init_energy, m)\n",
    "            if s == 1:\n",
    "                if direction == -1:\n",
    "                    p_minus, v_minus, _, _, new_p, n_n_leaves, new_s, new_alpha, n_new_alpha = self.build_tree(p_minus, v_minus, energy, direction, iter_no-1, step_size, init_energy, m)\n",
    "                else:\n",
    "                    _, _, p_plus, v_plus, new_p, n_n_leaves, new_s, new_alpha, n_new_alpha = self.build_tree(p_plus, v_plus, energy, direction, iter_no-1, step_size, init_energy, m)\n",
    "                p = new_p if torch.distributions.Bernoulli(logits=new_s/(s+new_s)).sample().bool() else p\n",
    "                alpha += new_alpha\n",
    "                n_alpha += n_new_alpha\n",
    "                s = new_s * self.stop_condition(p_minus, v_minus, p_plus, v_plus)\n",
    "                n_leaves += n_n_leaves\n",
    "            return p_minus, v_minus, p_plus, v_plus, p, n_leaves, s, alpha, n_alpha\n",
    "\n",
    "    def update(self, init_position):\n",
    "        self.init_step(init_position)\n",
    "        log_step_size_average = torch.log(self.step_size_average)\n",
    "        # starting from 1\n",
    "        # annotations : n --> number, new -> new value \n",
    "        proposal_position = init_position\n",
    "        for m in range(1,self.M+1):\n",
    "            velocity = self.generate_default_velocity(ravel_params(proposal_position).shape[-1])\n",
    "            self.ep.init_per_step(proposal_position, velocity)\n",
    "            init_energy = self.ep.total_current_energy()\n",
    "            # print(init_energy)\n",
    "            energy = torch.distributions.Uniform(torch.tensor([0.0]),init_energy).sample()\n",
    "            p_minus, p_plus, v_minus, v_plus = proposal_position, proposal_position, velocity, velocity\n",
    "            counter, s, n_leaves = 0, 1, 1\n",
    "            while s == 1:\n",
    "                direction = self.current_direction()\n",
    "                if direction == -1:\n",
    "                    p_minus, v_minus, _,_, new_p, n_n_leaves, new_s, alpha, n_alpha = self.build_tree(p_minus, v_minus, energy, direction, counter, self.step_size, init_energy, m) \n",
    "                else:\n",
    "                    _, _, p_plus, v_plus, new_p, n_n_leaves, new_s, alpha, n_alpha = self.build_tree(p_plus, v_plus, energy, direction, counter, self.step_size, init_energy, m) \n",
    "                if new_s == 1:\n",
    "                    probs = torch.tensor(min(1, n_n_leaves/n_leaves))\n",
    "                    proposal_position = new_p if torch.distributions.Bernoulli(logits=probs).sample().bool() else proposal_position\n",
    "                n_leaves += n_n_leaves\n",
    "                s = new_s * self.stop_condition(p_minus, v_minus, p_plus, v_plus)\n",
    "                counter += 1\n",
    "            if m <= self.m_warmup:\n",
    "                part_1 = (1-1/(m+self.t_0))*self.H_m\n",
    "                part_2 = (1/(m+self.t_0))*(self.expected_prob_density - alpha/n_alpha)\n",
    "                self.H_m =  part_1 + part_2 \n",
    "                log_step_size = self.mu - (torch.sqrt(torch.tensor(m))/self.gamma)*self.H_m\n",
    "                step = self.step_fn(m)\n",
    "                log_step_size_average = step * log_step_size + (1-step)*log_step_size_average\n",
    "                self.step_size_average = torch.exp(log_step_size_average)\n",
    "                self.step_size = torch.exp(log_step_size_average)\n",
    "                # print(\"s, s', m, step, alpha, n_alpha, H_m: \", s, new_s, m, self.step_size, alpha, n_alpha, self.H_m)\n",
    "            else:\n",
    "                # print(\"After warmup : s, s', m, step_size, alpha, n_alpha : \", s, new_s, m, self.step_size, alpha, n_alpha)\n",
    "                self.step_size = torch.exp(log_step_size_average)\n",
    "            print(m, self.step_size, alpha/n_alpha)\n",
    "            self.welford_position.update_step(ravel_params(proposal_position))\n",
    "        return proposal_position, self.welford_position.final_step()\n",
    "    \n",
    "    def step_fn(self, current_step):\n",
    "        return torch.pow(torch.tensor(1/current_step), self.k)\n",
    "\n",
    "    def init_step(self, position):\n",
    "        self.step_size = self.find_reasonable_epsilon(position)\n",
    "        self.mu = torch.log(10*self.step_size)  \n",
    "        self.step_size_average = torch.tensor(1)\n",
    "        self.H_m = torch.zeros(1)\n",
    "        self.gamma = 0.05\n",
    "        self.t_0 = 10\n",
    "        self.k = 0.75\n",
    "\n",
    "    def step_integrator(self, position, velocity, step_size_with_direction, l_frog_step=1):\n",
    "        self.ep.init_per_step(position, velocity)\n",
    "        # init step\n",
    "        self.ep.updated_velocity(step_size_with_direction, is_half_step_momentum=True)\n",
    "        # l_frog_steps\n",
    "        for i in range(l_frog_step):\n",
    "            self.ep.update_position(step_size_with_direction)\n",
    "            self.ep.updated_velocity(step_size_with_direction, is_half_step_momentum=False)\n",
    "        # final step\n",
    "        f_step_position = self.ep.update_position(step_size_with_direction)\n",
    "        half_step_velocity = self.ep.updated_velocity(step_size_with_direction, is_half_step_momentum=True)\n",
    "        return self.ep.delta_energy(), f_step_position, half_step_velocity, self.ep.total_current_energy()\n",
    "\n",
    "params_shape = ravel_params(params).shape[-1]\n",
    "# I^-1 = I so inverse doesn't matter but writing it for visibility\n",
    "default_precision = torch.eye(params_shape).inverse()\n",
    "hmc_kernel = HMCAlgorithm(log_density_fn=potential, precision=default_precision, position_shape=params_shape)\n",
    "# Test\n",
    "final_state, (_,_, unbiased_variance_positon) = hmc_kernel.update(params)\n",
    "final_params = unravel_params(unbiased_variance_positon, final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0129, 0.0127, 0.0129, 0.0127, 0.0127, 0.0127, 0.0128, 0.0127, 0.0129,\n",
      "        0.0129], grad_fn=<SliceBackward0>) [1 0 0 0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predict_X, predict_y = make_moons(n_samples=100, random_state=400)\n",
    "y_predicted = model.forward(final_params, torch.Tensor(predict_X)).ravel()\n",
    "print(y_predicted[:10], y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(vmin, vmax, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
