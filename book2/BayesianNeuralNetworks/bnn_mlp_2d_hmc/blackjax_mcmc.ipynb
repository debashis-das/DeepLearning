{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (0.4.33)\n",
      "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from jax) (0.4.33)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from jax) (0.3.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from jax) (1.26.1)\n",
      "Requirement already satisfied: opt-einsum in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from jax) (1.11.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: flax in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (0.7.5)\n",
      "Collecting flax\n",
      "  Downloading flax-0.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: jax>=0.4.27 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from flax) (0.4.33)\n",
      "Requirement already satisfied: msgpack in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from flax) (1.0.7)\n",
      "Requirement already satisfied: optax in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from flax) (0.1.7)\n",
      "Requirement already satisfied: orbax-checkpoint in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from flax) (0.4.8)\n",
      "Requirement already satisfied: tensorstore in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from flax) (0.1.51)\n",
      "Requirement already satisfied: rich>=11.1 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from flax) (13.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from flax) (4.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from flax) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from flax) (1.26.1)\n",
      "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from jax>=0.4.27->flax) (0.4.33)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from jax>=0.4.27->flax) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from jax>=0.4.27->flax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from jax>=0.4.27->flax) (1.11.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from rich>=11.1->flax) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from optax->flax) (1.4.0)\n",
      "Requirement already satisfied: chex>=0.1.5 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from optax->flax) (0.1.85)\n",
      "Requirement already satisfied: etils[epath,epy] in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Requirement already satisfied: nest_asyncio in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from orbax-checkpoint->flax) (1.5.8)\n",
      "Requirement already satisfied: protobuf in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from orbax-checkpoint->flax) (3.20.3)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2023.10.0)\n",
      "Requirement already satisfied: importlib_resources in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.1.1)\n",
      "Requirement already satisfied: zipp in /Users/debashisdas/Projects/DeepLearning/.venv/lib/python3.11/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\n",
      "Downloading flax-0.9.0-py3-none-any.whl (780 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.7/780.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: flax\n",
      "  Attempting uninstall: flax\n",
      "    Found existing installation: flax 0.7.5\n",
      "    Uninstalling flax-0.7.5:\n",
      "      Successfully uninstalled flax-0.7.5\n",
      "Successfully installed flax-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jax --upgrade\n",
    "%pip install flax --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from sklearn.datasets import make_moons\n",
    "import flax.linen as nn\n",
    "import blackjax\n",
    "import distrax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from functools import partial\n",
    "\n",
    "key = jax.random.PRNGKey(314)\n",
    "key_samples, key_init, key_warmup, key = jax.random.split(key, 4)\n",
    "\n",
    "noise = 0.2\n",
    "num_samples = 50\n",
    "num_warmup = 1000\n",
    "num_steps = 500\n",
    "X, y = make_moons(n_samples=num_samples, noise=noise, random_state=314)\n",
    "\n",
    "step = 0.2\n",
    "vmin, vmax = X.min() - step, X.max() + step\n",
    "X_grid = jnp.mgrid[vmin:vmax:100j, vmin:vmax:100j]\n",
    "\n",
    "class MLP1D(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.relu(nn.Dense(10)(x))\n",
    "        x = nn.relu(nn.Dense(10)(x))\n",
    "        x = nn.relu(nn.Dense(10)(x))\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n",
    "\n",
    "batch = jnp.ones((num_samples, 2))    \n",
    "model = MLP1D()\n",
    "params = model.init(key_init, batch)\n",
    "\n",
    "def bnn_log_joint(params, X, y, model):\n",
    "    logits = model.apply(params, X).ravel()\n",
    "\n",
    "    flatten_params, _ = ravel_pytree(params)\n",
    "    log_prior = distrax.Normal(0.0, 1.0).log_prob(flatten_params).sum()\n",
    "    log_likelihood = distrax.Bernoulli(logits=logits).log_prob(y).sum()\n",
    "\n",
    "    log_joint = log_prior + log_likelihood\n",
    "    return log_joint\n",
    "\n",
    "potential = partial(bnn_log_joint, X=X, y=y, model=model)\n",
    "\n",
    "adapt = blackjax.window_adaptation(blackjax.nuts, potential, num_warmup)\n",
    "final_state, kernel = adapt.run(key_warmup, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AdaptationInfo' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     _, states \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mscan(one_step, initial_state, keys)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m states\n\u001b[0;32m---> 11\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43minference_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m sampled_params \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39mposition\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36minference_loop\u001b[0;34m(rng_key, kernel, initial_state, num_samples)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, state\n\u001b[1;32m      6\u001b[0m keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng_key, num_samples)\n\u001b[0;32m----> 7\u001b[0m _, states \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m states\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36minference_loop.<locals>.one_step\u001b[0;34m(state, rng_key)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step\u001b[39m(state, rng_key):\n\u001b[0;32m----> 3\u001b[0m     state, _ \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, state\n",
      "\u001b[0;31mTypeError\u001b[0m: 'AdaptationInfo' object is not callable"
     ]
    }
   ],
   "source": [
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    def one_step(state, rng_key):\n",
    "        state, _ = kernel(rng_key, state)\n",
    "        return state, state\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, states = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states\n",
    "\n",
    "states = inference_loop(key_samples, kernel, final_state, num_samples)\n",
    "\n",
    "sampled_params = states.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(params)\n",
    "vapply = jax.vmap(model.apply, in_axes=(0, None), out_axes=0)\n",
    "vapply = jax.vmap(vapply, in_axes=(None, 1), out_axes=1)\n",
    "vapply = jax.vmap(vapply, in_axes=(None, 2), out_axes=2)\n",
    "\n",
    "logits_grid = vapply(params, X_grid)[..., -1]\n",
    "p_grid = jax.nn.sigmoid(logits_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Iterable, Mapping, Optional, Union, NamedTuple\n",
    "from typing_extensions import Protocol\n",
    "from threading import Lock\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "\n",
    "from jax.typing import ArrayLike\n",
    "from jax.tree_util import tree_leaves\n",
    "from jax import lax\n",
    "from jax.experimental import io_callback\n",
    "\n",
    "Array = jax.Array\n",
    "PRNGKey = jax.Array\n",
    "\n",
    "ArrayLikeTree = Union[\n",
    "    ArrayLike, Iterable[\"ArrayLikeTree\"], Mapping[Any, \"ArrayLikeTree\"]\n",
    "]\n",
    "\n",
    "ArrayTree = Union[jax.Array, Iterable[\"ArrayTree\"], Mapping[Any, \"ArrayTree\"]]\n",
    "\n",
    "Scalar = Union[float, int]\n",
    "Numeric = Union[jax.Array, Scalar]\n",
    "\n",
    "def pytree_size(pytree: ArrayLikeTree) -> int:\n",
    "    \"\"\"Return the dimension of the flatten PyTree.\"\"\"\n",
    "    return sum(jnp.size(value) for value in tree_leaves(pytree))\n",
    "\n",
    "def progress_bar_scan(num_samples, print_rate=None):\n",
    "    \"Progress bar for a JAX scan\"\n",
    "    progress_bars = {}\n",
    "    idx_counter = 0\n",
    "    lock = Lock()\n",
    "\n",
    "    if print_rate is None:\n",
    "        if num_samples > 20:\n",
    "            print_rate = int(num_samples / 20)\n",
    "        else:\n",
    "            print_rate = 1  # if you run the sampler for less than 20 iterations\n",
    "\n",
    "    def _calc_chain_idx(iter_num):\n",
    "        nonlocal idx_counter\n",
    "        with lock:\n",
    "            idx = idx_counter\n",
    "            idx_counter += 1\n",
    "        return idx\n",
    "\n",
    "    def _update_bar(arg, chain_id):\n",
    "        chain_id = int(chain_id)\n",
    "        if arg == 0:\n",
    "            chain_id = _calc_chain_idx(arg)\n",
    "            progress_bars[chain_id] = progress_bar(range(num_samples))\n",
    "            progress_bars[chain_id].update(0)\n",
    "\n",
    "        progress_bars[chain_id].update_bar(arg + 1)\n",
    "        return chain_id\n",
    "\n",
    "    def _close_bar(arg, chain_id):\n",
    "        progress_bars[int(chain_id)].on_iter_end()\n",
    "\n",
    "    def _update_progress_bar(iter_num, chain_id):\n",
    "        \"Updates progress bar of a JAX scan or loop\"\n",
    "\n",
    "        chain_id = lax.cond(\n",
    "            # update every multiple of `print_rate` except at the end\n",
    "            (iter_num % print_rate == 0) | (iter_num == (num_samples - 1)),\n",
    "            lambda _: io_callback(_update_bar, jnp.array(0), iter_num, chain_id),\n",
    "            lambda _: chain_id,\n",
    "            operand=None,\n",
    "        )\n",
    "\n",
    "        _ = lax.cond(\n",
    "            iter_num == num_samples - 1,\n",
    "            lambda _: io_callback(_close_bar, None, iter_num + 1, chain_id),\n",
    "            lambda _: None,\n",
    "            operand=None,\n",
    "        )\n",
    "        return chain_id\n",
    "\n",
    "    def _progress_bar_scan(func):\n",
    "        \"\"\"Decorator that adds a progress bar to `body_fun` used in `lax.scan`.\n",
    "        Note that `body_fun` must either be looping over `np.arange(num_samples)`,\n",
    "        or be looping over a tuple who's first element is `np.arange(num_samples)`\n",
    "        This means that `iter_num` is the current iteration number\n",
    "        \"\"\"\n",
    "\n",
    "        def wrapper_progress_bar(carry, x):\n",
    "            if type(x) is tuple:\n",
    "                iter_num, *_ = x\n",
    "            else:\n",
    "                iter_num = x\n",
    "            subcarry, chain_id = carry\n",
    "            chain_id = _update_progress_bar(iter_num, chain_id)\n",
    "            subcarry, y = func(subcarry, x)\n",
    "\n",
    "            return (subcarry, chain_id), y\n",
    "\n",
    "        return wrapper_progress_bar\n",
    "\n",
    "    return _progress_bar_scan\n",
    "\n",
    "def gen_scan_fn(num_samples, progress_bar, print_rate=None):\n",
    "    if progress_bar:\n",
    "\n",
    "        def scan_wrap(f, init, *args, **kwargs):\n",
    "            func = progress_bar_scan(num_samples, print_rate)(f)\n",
    "            carry = (init, -1)\n",
    "            (last_state, _), output = lax.scan(func, carry, *args, **kwargs)\n",
    "            return last_state, output\n",
    "\n",
    "        return scan_wrap\n",
    "    else:\n",
    "        return lax.scan\n",
    "\n",
    "class RunFn(Protocol):\n",
    "    \"\"\"A `Callable` used to run the adaptation procedure.\"\"\"\n",
    "\n",
    "    def __call__(self, rng_key: PRNGKey, position: ArrayLikeTree):\n",
    "        \"\"\"Run the compiled algorithm.\"\"\"\n",
    "\n",
    "class AdaptationAlgorithm(NamedTuple):\n",
    "    \"\"\"A function that implements an adaptation algorithm.\"\"\"\n",
    "\n",
    "    run: RunFn\n",
    "\n",
    "class AdaptationResults(NamedTuple):\n",
    "    state: ArrayTree\n",
    "    parameters: dict\n",
    "\n",
    "class AdaptationInfo(NamedTuple):\n",
    "    state: NamedTuple\n",
    "    info: NamedTuple\n",
    "    adaptation_state: NamedTuple\n",
    "\n",
    "\n",
    "class WelfordAlgorithmState(NamedTuple):\n",
    "    \"\"\"State carried through the Welford algorithm.\n",
    "\n",
    "    mean\n",
    "        The running sample mean.\n",
    "    m2\n",
    "        The running value of the sum of difference of squares. See documentation\n",
    "        of the `welford_algorithm` function for an explanation.\n",
    "    sample_size\n",
    "        The number of successive states the previous values have been computed on;\n",
    "        also the current number of iterations of the algorithm.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mean: Array\n",
    "    m2: Array\n",
    "    sample_size: int\n",
    "\n",
    "class MassMatrixAdaptationState(NamedTuple):\n",
    "    \"\"\"State carried through the mass matrix adaptation.\n",
    "\n",
    "    inverse_mass_matrix\n",
    "        The curent value of the inverse mass matrix.\n",
    "    wc_state\n",
    "        The current state of the Welford Algorithm.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    inverse_mass_matrix: Array\n",
    "    wc_state: WelfordAlgorithmState\n",
    "\n",
    "\n",
    "class DualAveragingAdaptationState(NamedTuple):\n",
    "    \"\"\"State carried through the dual averaging procedure.\n",
    "\n",
    "    log_step_size\n",
    "        The logarithm of the current value of the step size.\n",
    "    log_step_size_avg\n",
    "        The time-weighted average of the values that the logarithm of the step\n",
    "        size has taken so far.\n",
    "    step\n",
    "        The current iteration step.\n",
    "    avg_err\n",
    "        The time average of the value of the quantity :math:`H_t`, the\n",
    "        difference between the target acceptance rate and the current\n",
    "        acceptance rate.\n",
    "    mu\n",
    "        Arbitrary point the values of log_step_size are shrunk towards. Chose\n",
    "        to be :math:`\\\\log(10 \\\\epsilon_0)` where :math:`\\\\epsilon_0` is chosen\n",
    "        in this context to be the step size given by the\n",
    "        `find_reasonable_step_size` procedure.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    log_step_size: float\n",
    "    log_step_size_avg: float\n",
    "    step: int\n",
    "    avg_error: float\n",
    "    mu: float\n",
    "\n",
    "class WindowAdaptationState(NamedTuple):\n",
    "    ss_state: DualAveragingAdaptationState  # step size\n",
    "    imm_state: MassMatrixAdaptationState  # inverse mass matrix\n",
    "    step_size: float\n",
    "    inverse_mass_matrix: Array\n",
    "\n",
    "class KineticEnergy(Protocol):\n",
    "    def __call__(\n",
    "        self, momentum: ArrayLikeTree, position: Optional[ArrayLikeTree] = None\n",
    "    ) -> Numeric:\n",
    "        ...\n",
    "\n",
    "class IntegratorState(NamedTuple):\n",
    "    \"\"\"State of the trajectory integration.\n",
    "\n",
    "    We keep the gradient of the logdensity function (negative potential energy)\n",
    "    to speedup computations.\n",
    "    \"\"\"\n",
    "\n",
    "    position: ArrayTree\n",
    "    momentum: ArrayTree\n",
    "    logdensity: float\n",
    "    logdensity_grad: ArrayTree\n",
    "\n",
    "Integrator = Callable[[IntegratorState, float], IntegratorState]\n",
    "\n",
    "def return_all_adapt_info(state, info, adaptation_state):\n",
    "    \"\"\"Return fully populated AdaptationInfo.  Used for adaptation_info_fn\n",
    "    parameters of the adaptation algorithms.\n",
    "    \"\"\"\n",
    "    return AdaptationInfo(state, info, adaptation_state)\n",
    "\n",
    "def generalized_two_stage_integrator(\n",
    "    operator1: Callable,\n",
    "    operator2: Callable,\n",
    "    coefficients: list[float],\n",
    "    format_output_fn: Callable = lambda x: x,\n",
    "):\n",
    "    \"\"\"Generalized numerical integrator for solving ODEs.\n",
    "\n",
    "    The generalized integrator performs numerical integration of a ODE system by\n",
    "    alernating between stage 1 and stage 2 updates.\n",
    "    The update scheme is decided by the coefficients, The scheme should be palindromic,\n",
    "    i.e. the coefficients of the update scheme should be symmetric with respect to the\n",
    "    middle of the scheme.\n",
    "\n",
    "    For instance, for *any* differential equation of the form:\n",
    "\n",
    "    .. math:: \\\\frac{d}{dt}f = (O_1+O_2)f\n",
    "\n",
    "    The velocity_verlet operator can be seen as approximating :math:`e^{\\\\epsilon(O_1 + O_2)}`\n",
    "    by :math:`e^{\\\\epsilon O_1/2}e^{\\\\epsilon O_2}e^{\\\\epsilon O_1/2}`.\n",
    "\n",
    "    In a standard Hamiltonian, the forms of :math:`e^{\\\\epsilon O_2}` and\n",
    "    :math:`e^{\\\\epsilon O_1}` are simple, but for other differential equations,\n",
    "    they may be more complex.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    operator1\n",
    "        Stage 1 operator, a function that updates the momentum.\n",
    "    operator2\n",
    "        Stage 2 operator, a function that updates the position.\n",
    "    coefficients\n",
    "        Coefficients of the integrator.\n",
    "    format_output_fn\n",
    "        Function that formats the output of the integrator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    integrator\n",
    "        Integrator function.\n",
    "    \"\"\"\n",
    "\n",
    "    def one_step(state: IntegratorState, step_size: float):\n",
    "        position, momentum, _, logdensity_grad = state\n",
    "        # auxiliary infomation generated during integration for diagnostics. It is\n",
    "        # updated by the operator1 and operator2 at each call.\n",
    "        momentum_update_info = None\n",
    "        position_update_info = None\n",
    "        for i, coef in enumerate(coefficients[:-1]):\n",
    "            if i % 2 == 0:\n",
    "                momentum, kinetic_grad, momentum_update_info = operator1(\n",
    "                    momentum,\n",
    "                    logdensity_grad,\n",
    "                    step_size,\n",
    "                    coef,\n",
    "                    momentum_update_info,\n",
    "                    is_last_call=False,\n",
    "                )\n",
    "            else:\n",
    "                (\n",
    "                    position,\n",
    "                    logdensity,\n",
    "                    logdensity_grad,\n",
    "                    position_update_info,\n",
    "                ) = operator2(\n",
    "                    position,\n",
    "                    kinetic_grad,\n",
    "                    step_size,\n",
    "                    coef,\n",
    "                    position_update_info,\n",
    "                )\n",
    "        # Separate the last steps to short circuit the computation of the kinetic_grad.\n",
    "        momentum, kinetic_grad, momentum_update_info = operator1(\n",
    "            momentum,\n",
    "            logdensity_grad,\n",
    "            step_size,\n",
    "            coefficients[-1],\n",
    "            momentum_update_info,\n",
    "            is_last_call=True,\n",
    "        )\n",
    "        return format_output_fn(\n",
    "            position,\n",
    "            momentum,\n",
    "            logdensity,\n",
    "            logdensity_grad,\n",
    "            kinetic_grad,\n",
    "            position_update_info,\n",
    "            momentum_update_info,\n",
    "        )\n",
    "\n",
    "    return one_step\n",
    "\n",
    "def euclidean_position_update_fn(logdensity_fn: Callable):\n",
    "    logdensity_and_grad_fn = jax.value_and_grad(logdensity_fn)\n",
    "\n",
    "    def update(\n",
    "        position: ArrayTree,\n",
    "        kinetic_grad: ArrayTree,\n",
    "        step_size: float,\n",
    "        coef: float,\n",
    "        auxiliary_info=None,\n",
    "    ):\n",
    "        del auxiliary_info\n",
    "        new_position = jax.tree_util.tree_map(\n",
    "            lambda x, grad: x + step_size * coef * grad,\n",
    "            position,\n",
    "            kinetic_grad,\n",
    "        )\n",
    "        logdensity, logdensity_grad = logdensity_and_grad_fn(new_position)\n",
    "        return new_position, logdensity, logdensity_grad, None\n",
    "\n",
    "    return update\n",
    "\n",
    "def euclidean_momentum_update_fn(kinetic_energy_fn: KineticEnergy):\n",
    "    kinetic_energy_grad_fn = jax.grad(kinetic_energy_fn)\n",
    "\n",
    "    def update(\n",
    "        momentum: ArrayTree,\n",
    "        logdensity_grad: ArrayTree,\n",
    "        step_size: float,\n",
    "        coef: float,\n",
    "        auxiliary_info=None,\n",
    "        is_last_call=False,\n",
    "    ):\n",
    "        del auxiliary_info\n",
    "        new_momentum = jax.tree_util.tree_map(\n",
    "            lambda x, grad: x + step_size * coef * grad,\n",
    "            momentum,\n",
    "            logdensity_grad,\n",
    "        )\n",
    "        if is_last_call:\n",
    "            return new_momentum, None, None\n",
    "        kinetic_grad = kinetic_energy_grad_fn(new_momentum)\n",
    "        return new_momentum, kinetic_grad, None\n",
    "\n",
    "    return update\n",
    "\n",
    "def format_euclidean_state_output(\n",
    "    position,\n",
    "    momentum,\n",
    "    logdensity,\n",
    "    logdensity_grad,\n",
    "    kinetic_grad,\n",
    "    position_update_info,\n",
    "    momentum_update_info,\n",
    "):\n",
    "    del kinetic_grad, position_update_info, momentum_update_info\n",
    "    return IntegratorState(position, momentum, logdensity, logdensity_grad)\n",
    "\n",
    "\n",
    "def generate_euclidean_integrator(coefficients):\n",
    "    \"\"\"Generate symplectic integrator for solving a Hamiltonian system.\n",
    "\n",
    "    The resulting integrator is volume-preserve and preserves the symplectic structure\n",
    "    of phase space.\n",
    "    \"\"\"\n",
    "\n",
    "    def euclidean_integrator(\n",
    "        logdensity_fn: Callable, kinetic_energy_fn: KineticEnergy\n",
    "    ) -> Integrator:\n",
    "        position_update_fn = euclidean_position_update_fn(logdensity_fn)\n",
    "        momentum_update_fn = euclidean_momentum_update_fn(kinetic_energy_fn)\n",
    "        one_step = generalized_two_stage_integrator(\n",
    "            momentum_update_fn,\n",
    "            position_update_fn,\n",
    "            coefficients,\n",
    "            format_output_fn=format_euclidean_state_output,\n",
    "        )\n",
    "        return one_step\n",
    "\n",
    "    return euclidean_integrator\n",
    "\n",
    "velocity_verlet_coefficients = [0.5, 1.0, 0.5]\n",
    "velocity_verlet = generate_euclidean_integrator(velocity_verlet_coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welford Algorithm with regularizer\n",
    "$$M_{2,n} = \\sum_{i=1}^n \\left(x_i-\\overline{x_n}\\right)^2$$\n",
    "Reference Welford Algorithm: \n",
    "https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n",
    "\n",
    "### Dual Averaging\n",
    "Tune the step size in order to achieve a desired target acceptance rate.\n",
    "\n",
    "Let us note $\\epsilon$ the current step size, $\\alpha_t$ the metropolis acceptance rate at time $t$ and $\\delta$ the desired aceptance rate. We define:\n",
    "$$H_t = \\delta - \\alpha_t$$\n",
    "the error at time t. We would like to find a procedure that adapts the value of $\\epsilon$ such that $$h(x) =\\mathbb{E}\\left[H_t|\\epsilon\\right] = 0$$\n",
    "\n",
    "Following :cite:p:`nesterov2009primal`, the authors of :cite:p:`hoffman2014no` proposed the following update scheme. If\n",
    "we note \n",
    "$$x = \\log \\epsilon$$ \n",
    "\n",
    "we follow:\n",
    "\n",
    "$$\n",
    "    x_{t+1} \\Longleftarrow \\mu - \\frac{\\sqrt{t}}{\\gamma} \\frac{1}{t+t_0} \\sum_{i=1}^t H_i\n",
    "    \\overline{x}_{t+1} \\Longleftarrow x_{t+1}\\, t^{-\\kappa}  + \\left(1-t^\\kappa\\right)\\overline{x}_t\n",
    "$$\n",
    "$\\overline{x}_{t}$ is guaranteed to converge to a value such that\n",
    "$h(\\overline{x}_t)$ converges to 0, i.e. the Metropolis acceptance rate converges to the desired rate.\n",
    "\n",
    "See reference :cite:p:`hoffman2014no` (section 3.2.1) for a detailed discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WelfordAlgorithmState(NamedTuple):\n",
    "    \"\"\"State carried through the Welford algorithm.\n",
    "\n",
    "    mean\n",
    "        The running sample mean.\n",
    "    m2\n",
    "        The running value of the sum of difference of squares. See documentation\n",
    "        of the `welford_algorithm` function for an explanation.\n",
    "    sample_size\n",
    "        The number of successive states the previous values have been computed on;\n",
    "        also the current number of iterations of the algorithm.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mean: Array\n",
    "    m2: Array\n",
    "    sample_size: int\n",
    "\n",
    "\n",
    "class MassMatrixAdaptationState(NamedTuple):\n",
    "    \"\"\"State carried through the mass matrix adaptation.\n",
    "\n",
    "    inverse_mass_matrix\n",
    "        The curent value of the inverse mass matrix.\n",
    "    wc_state\n",
    "        The current state of the Welford Algorithm.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    inverse_mass_matrix: Array\n",
    "    wc_state: WelfordAlgorithmState\n",
    "    \n",
    "def welford_algorithm(is_diagonal_matrix: bool) -> tuple[Callable, Callable, Callable]:\n",
    "    r\"\"\"Welford's online estimator of covariance.\n",
    "\n",
    "    It is possible to compute the variance of a population of values in an\n",
    "    on-line fashion to avoid storing intermediate results. The naive recurrence\n",
    "    relations between the sample mean and variance at a step and the next are\n",
    "    however not numerically stable.\n",
    "\n",
    "    Welford's algorithm uses the sum of square of differences\n",
    "    :math:`M_{2,n} = \\sum_{i=1}^n \\left(x_i-\\overline{x_n}\\right)^2`\n",
    "    for updating where :math:`x_n` is the current mean and the following\n",
    "    recurrence relationships\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    is_diagonal_matrix\n",
    "        When True the algorithm adapts and returns a diagonal mass matrix\n",
    "        (default), otherwise adaps and returns a dense mass matrix.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    It might seem pedantic to separate the Welford algorithm from mass adaptation,\n",
    "    but this covariance estimator is used in other parts of the library.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def init(n_dims: int) -> WelfordAlgorithmState:\n",
    "        \"\"\"Initialize the covariance estimation.\n",
    "\n",
    "        When the matrix is diagonal it is sufficient to work with an array that contains\n",
    "        the diagonal value. Otherwise we need to work with the matrix in full.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_dims: int\n",
    "            The number of dimensions of the problem, which corresponds to the size\n",
    "            of the corresponding square mass matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        sample_size = 0\n",
    "        mean = jnp.zeros((n_dims,))\n",
    "        if is_diagonal_matrix:\n",
    "            m2 = jnp.zeros((n_dims,))\n",
    "        else:\n",
    "            m2 = jnp.zeros((n_dims, n_dims))\n",
    "        return WelfordAlgorithmState(mean, m2, sample_size)\n",
    "\n",
    "    def update(\n",
    "        wa_state: WelfordAlgorithmState, value: ArrayLike\n",
    "    ) -> WelfordAlgorithmState:\n",
    "        \"\"\"Update the M2 matrix using the new value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        wa_state:\n",
    "            The current state of the Welford Algorithm\n",
    "        value: Array, shape (1,)\n",
    "            The new sample (typically position of the chain) used to update m2\n",
    "\n",
    "        \"\"\"\n",
    "        mean, m2, sample_size = wa_state\n",
    "        sample_size = sample_size + 1\n",
    "\n",
    "        delta = value - mean\n",
    "        mean = mean + delta / sample_size\n",
    "        updated_delta = value - mean\n",
    "        if is_diagonal_matrix:\n",
    "            new_m2 = m2 + delta * updated_delta\n",
    "        else:\n",
    "            new_m2 = m2 + jnp.outer(updated_delta, delta)\n",
    "\n",
    "        return WelfordAlgorithmState(mean, new_m2, sample_size)\n",
    "\n",
    "    def final(\n",
    "        wa_state: WelfordAlgorithmState,\n",
    "    ) -> tuple[Array, int, Array]:\n",
    "        mean, m2, sample_size = wa_state\n",
    "        covariance = m2 / (sample_size - 1)\n",
    "        return covariance, sample_size, mean\n",
    "\n",
    "    return init, update, final\n",
    "\n",
    "def mass_matrix_adaptation(\n",
    "    is_diagonal_matrix: bool = True,\n",
    ") -> tuple[Callable, Callable, Callable]:\n",
    "    \"\"\"Adapts the values in the mass matrix by computing the covariance\n",
    "    between parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    is_diagonal_matrix\n",
    "        When True the algorithm adapts and returns a diagonal mass matrix\n",
    "        (default), otherwise adaps and returns a dense mass matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    init\n",
    "        A function that initializes the step of the mass matrix adaptation.\n",
    "    update\n",
    "        A function that updates the state of the mass matrix.\n",
    "    final\n",
    "        A function that computes the inverse mass matrix based on the current\n",
    "        state.\n",
    "\n",
    "    \"\"\"\n",
    "    wc_init, wc_update, wc_final = welford_algorithm(is_diagonal_matrix)\n",
    "\n",
    "    def init(n_dims: int) -> MassMatrixAdaptationState:\n",
    "        \"\"\"Initialize the matrix adaptation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ndims\n",
    "            The number of dimensions of the mass matrix, which corresponds to\n",
    "            the number of dimensions of the chain position.\n",
    "\n",
    "        \"\"\"\n",
    "        if is_diagonal_matrix:\n",
    "            inverse_mass_matrix = jnp.ones(n_dims)\n",
    "        else:\n",
    "            inverse_mass_matrix = jnp.identity(n_dims)\n",
    "\n",
    "        wc_state = wc_init(n_dims)\n",
    "\n",
    "        return MassMatrixAdaptationState(inverse_mass_matrix, wc_state)\n",
    "\n",
    "    def update(\n",
    "        mm_state: MassMatrixAdaptationState, position: ArrayLike\n",
    "    ) -> MassMatrixAdaptationState:\n",
    "        \"\"\"Update the algorithm's state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state:\n",
    "            The current state of the mass matrix adapation.\n",
    "        position:\n",
    "            The current position of the chain.\n",
    "\n",
    "        \"\"\"\n",
    "        inverse_mass_matrix, wc_state = mm_state\n",
    "        position, _ = jax.flatten_util.ravel_pytree(position)\n",
    "        wc_state = wc_update(wc_state, position)\n",
    "        return MassMatrixAdaptationState(inverse_mass_matrix, wc_state)\n",
    "\n",
    "    def final(mm_state: MassMatrixAdaptationState) -> MassMatrixAdaptationState:\n",
    "        \"\"\"Final iteration of the mass matrix adaptation.\n",
    "\n",
    "        In this step we compute the mass matrix from the covariance matrix computed\n",
    "        by the Welford algorithm, and re-initialize the later.\n",
    "\n",
    "        \"\"\"\n",
    "        _, wc_state = mm_state\n",
    "        covariance, count, mean = wc_final(wc_state)\n",
    "\n",
    "        # Regularize the covariance matrix, see Stan\n",
    "        scaled_covariance = (count / (count + 5)) * covariance\n",
    "        shrinkage = 1e-3 * (5 / (count + 5))\n",
    "        if is_diagonal_matrix:\n",
    "            inverse_mass_matrix = scaled_covariance + shrinkage\n",
    "        else:\n",
    "            inverse_mass_matrix = scaled_covariance + shrinkage * jnp.identity(\n",
    "                mean.shape[0]\n",
    "            )\n",
    "\n",
    "        ndims = jnp.shape(inverse_mass_matrix)[-1]\n",
    "        new_mm_state = MassMatrixAdaptationState(inverse_mass_matrix, wc_init(ndims))\n",
    "\n",
    "        return new_mm_state\n",
    "\n",
    "    return init, update, final\n",
    "\n",
    "class DualAveragingState(NamedTuple):\n",
    "    \"\"\"State carried through the dual averaging procedure.\n",
    "\n",
    "    log_x\n",
    "        The logarithm of the current state\n",
    "    log_x_avg\n",
    "        The time-weighted average of the values that the logarithm of the state\n",
    "        has taken so far.\n",
    "    step\n",
    "        The current iteration step.\n",
    "    avg_err\n",
    "        The time average of the value of the quantity :math:`H_t`, the\n",
    "        difference between the target acceptance rate and the current\n",
    "        acceptance rate.\n",
    "    mu\n",
    "        Arbitrary point the values of log_step_size are shrunk towards. Chose\n",
    "        to be :math:`\\\\log(10 \\\\epsilon_0)` where :math:`\\\\epsilon_0` is chosen\n",
    "        in this context to be the step size given by the\n",
    "        `find_reasonable_step_size` procedure.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    log_x: float\n",
    "    log_x_avg: float\n",
    "    step: int\n",
    "    avg_error: float\n",
    "    mu: float\n",
    "\n",
    "\n",
    "def dual_averaging(\n",
    "    t0: int = 10, gamma: float = 0.05, kappa: float = 0.75\n",
    ") -> tuple[Callable, Callable, Callable]:\n",
    "    \"\"\"Find the state that minimizes an objective function using a primal-dual\n",
    "    subgradient method.\n",
    "\n",
    "    See :cite:p:`nesterov2009primal` for a detailed explanation of the algorithm and its mathematical\n",
    "    properties.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t0: float >= 0\n",
    "        Free parameter that stabilizes the initial iterations of the algorithm.\n",
    "        Large values may slow down convergence. Introduced in :cite:p:`hoffman2014no` with a default\n",
    "        value of 10.\n",
    "    gamma\n",
    "        Controls the speed of convergence of the scheme. The authors of :cite:p:`hoffman2014no` recommend\n",
    "        a value of 0.05.\n",
    "    kappa: float in ]0.5, 1]\n",
    "        Controls the weights of past steps in the current update. The scheme will\n",
    "        quickly forget earlier step for a small value of `kappa`. Introduced\n",
    "        in :cite:p:`hoffman2014no`, with a recommended value of .75\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    init\n",
    "        A function that initializes the state of the dual averaging scheme.\n",
    "    update\n",
    "        a function that updates the state of the dual averaging scheme.\n",
    "    final\n",
    "        a function that returns the state that minimizes the objective function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def init(x_init: float) -> DualAveragingState:\n",
    "        \"\"\"Initialize the state of the dual averaging scheme.\n",
    "\n",
    "        The parameter :math:`\\\\mu` is set to :math:`\\\\log(10 \\\\x_init)`\n",
    "        where :math:`\\\\x_init` is the initial value of the state.\n",
    "\n",
    "        \"\"\"\n",
    "        mu: float = jnp.log(10 * x_init)\n",
    "        step = 1\n",
    "        avg_error: float = 0.0\n",
    "        log_x: float = jnp.log(x_init)\n",
    "        log_x_avg: float = 0.0\n",
    "        return DualAveragingState(log_x, log_x_avg, step, avg_error, mu)\n",
    "\n",
    "    def update(da_state: DualAveragingState, gradient) -> DualAveragingState:\n",
    "        \"\"\"Update the state of the Dual Averaging adaptive algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gradient:\n",
    "            The gradient of the function to optimize with respect to the state\n",
    "            `x`, computed at the current value of `x`.\n",
    "        da_state:\n",
    "            The current state of the dual averaging algorithm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The updated state of the dual averaging algorithm.\n",
    "\n",
    "        \"\"\"\n",
    "        log_step, avg_log_step, step, avg_error, mu = da_state\n",
    "        reg_step = step + t0\n",
    "        eta_t = step ** (-kappa)\n",
    "        avg_error = (1 - (1 / (reg_step))) * avg_error + gradient / reg_step\n",
    "        log_x = mu - (jnp.sqrt(step) / gamma) * avg_error\n",
    "        log_x_avg = eta_t * log_step + (1 - eta_t) * avg_log_step\n",
    "        return DualAveragingState(log_x, log_x_avg, step + 1, avg_error, mu)\n",
    "\n",
    "    def final(da_state: DualAveragingState) -> float:\n",
    "        \"\"\"Returns the state that minimizes the objective function.\"\"\"\n",
    "        return jnp.exp(da_state.log_x_avg)\n",
    "\n",
    "    return init, update, final\n",
    "\n",
    "def dual_averaging_adaptation(\n",
    "    target: float, t0: int = 10, gamma: float = 0.05, kappa: float = 0.75\n",
    ") -> tuple[Callable, Callable, Callable]:\n",
    "    \"\"\"Tune the step size in order to achieve a desired target acceptance rate.\n",
    "\n",
    "    Let us note :math:`\\\\epsilon` the current step size, :math:`\\\\alpha_t` the\n",
    "    metropolis acceptance rate at time :math:`t` and :math:`\\\\delta` the desired\n",
    "    aceptance rate. We define:\n",
    "\n",
    "    .. math:\n",
    "        H_t = \\\\delta - \\\\alpha_t\n",
    "\n",
    "    the error at time t. We would like to find a procedure that adapts the\n",
    "    value of :math:`\\\\epsilon` such that :math:`h(x) =\\\\mathbb{E}\\\\left[H_t|\\\\epsilon\\\\right] = 0`\n",
    "\n",
    "    Following :cite:p:`nesterov2009primal`, the authors of :cite:p:`hoffman2014no` proposed the following update scheme. If\n",
    "    we note :math:`x = \\\\log \\\\epsilon` we follow:\n",
    "\n",
    "    .. math:\n",
    "        x_{t+1} \\\\LongLeftArrow \\\\mu - \\\\frac{\\\\sqrt{t}}{\\\\gamma} \\\\frac{1}{t+t_0} \\\\sum_{i=1}^t H_i\n",
    "        \\\\overline{x}_{t+1} \\\\LongLeftArrow x_{t+1}\\\\, t^{-\\\\kappa}  + \\\\left(1-t^\\\\kappa\\\\right)\\\\overline{x}_t\n",
    "\n",
    "    :math:`\\\\overline{x}_{t}` is guaranteed to converge to a value such that\n",
    "    :math:`h(\\\\overline{x}_t)` converges to 0, i.e. the Metropolis acceptance\n",
    "    rate converges to the desired rate.\n",
    "\n",
    "    See reference :cite:p:`hoffman2014no` (section 3.2.1) for a detailed discussion.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t0: float >= 0\n",
    "        Free parameter that stabilizes the initial iterations of the algorithm.\n",
    "        Large values may slow down convergence. Introduced in :cite:p:`hoffman2014no` with a default\n",
    "        value of 10.\n",
    "    gamma:\n",
    "        Controls the speed of convergence of the scheme. The authors of :cite:p:`hoffman2014no` recommend\n",
    "        a value of 0.05.\n",
    "    kappa: float in [0.5, 1]\n",
    "        Controls the weights of past steps in the current update. The scheme will\n",
    "        quickly forget earlier step for a small value of `kappa`. Introduced\n",
    "        in :cite:p:`hoffman2014no`, with a recommended value of .75\n",
    "    target:\n",
    "        Target acceptance rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    init\n",
    "        A function that initializes the state of the dual averaging scheme.\n",
    "    update\n",
    "        A function that updates the state of the dual averaging scheme.\n",
    "\n",
    "    \"\"\"\n",
    "    da_init, da_update, da_final = dual_averaging(t0, gamma, kappa)\n",
    "\n",
    "    def init(inital_step_size: float) -> DualAveragingAdaptationState:\n",
    "        \"\"\"Initialize the state of the dual averaging scheme.\n",
    "\n",
    "        The parameter :math:`\\\\mu` is set to :math:`\\\\log(10 \\\\epsilon_1)`\n",
    "        where :math:`\\\\epsilon_1` is the initial value of the step size.\n",
    "        \"\"\"\n",
    "        return DualAveragingAdaptationState(*da_init(inital_step_size))\n",
    "\n",
    "    def update(\n",
    "        da_state: DualAveragingAdaptationState, acceptance_rate: float\n",
    "    ) -> DualAveragingAdaptationState:\n",
    "        \"\"\"Update the state of the Dual Averaging adaptive algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        da_state:\n",
    "            The current state of the dual averaging algorithm.\n",
    "        acceptance_rate: float in [0, 1]\n",
    "            The current metropolis acceptance rate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The updated state of the dual averaging algorithm.\n",
    "\n",
    "        \"\"\"\n",
    "        gradient = target - acceptance_rate\n",
    "        return DualAveragingAdaptationState(*da_update(da_state, gradient))\n",
    "\n",
    "    def final(da_state: DualAveragingAdaptationState) -> float:\n",
    "        return jnp.exp(da_state.log_step_size_avg)\n",
    "\n",
    "    return init, update, final\n",
    "\n",
    "def base(\n",
    "    is_mass_matrix_diagonal: bool,\n",
    "    target_acceptance_rate: float = 0.80,\n",
    ") -> tuple[Callable, Callable, Callable]:\n",
    "    \"\"\"Warmup scheme for sampling procedures based on euclidean manifold HMC.\n",
    "    The schedule and algorithms used match Stan's :cite:p:`stan_hmc_param` as closely as possible.\n",
    "\n",
    "    Unlike several other libraries, we separate the warmup and sampling phases\n",
    "    explicitly. This ensure a better modularity; a change in the warmup does\n",
    "    not affect the sampling. It also allows users to run their own warmup\n",
    "    should they want to.\n",
    "    We also decouple generating a new sample with the mcmc algorithm and\n",
    "    updating the values of the parameters.\n",
    "\n",
    "    Stan's warmup consists in the three following phases:\n",
    "\n",
    "    1. A fast adaptation window where only the step size is adapted using\n",
    "    Nesterov's dual averaging scheme to match a target acceptance rate.\n",
    "    2. A succession of slow adapation windows (where the size of a window is\n",
    "    double that of the previous window) where both the mass matrix and the step\n",
    "    size are adapted. The mass matrix is recomputed at the end of each window;\n",
    "    the step size is re-initialized to a \"reasonable\" value.\n",
    "    3. A last fast adaptation window where only the step size is adapted.\n",
    "\n",
    "    Schematically:\n",
    "\n",
    "    +---------+---+------+------------+------------------------+------+\n",
    "    |  fast   | s | slow |   slow     |        slow            | fast |\n",
    "    +---------+---+------+------------+------------------------+------+\n",
    "    |1        |2  |3     |3           |3                       |3     |\n",
    "    +---------+---+------+------------+------------------------+------+\n",
    "\n",
    "    Step (1) consists in find a \"reasonable\" first step size that is used to\n",
    "    initialize the dual averaging scheme. In (2) we initialize the mass matrix\n",
    "    to the matrix. In (3) we compute the mass matrix to use in the kernel and\n",
    "    re-initialize the mass matrix adaptation. The step size is still adapated\n",
    "    in slow adaptation windows, and is not re-initialized between windows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    is_mass_matrix_diagonal\n",
    "        Create and adapt a diagonal mass matrix if True, a dense matrix\n",
    "        otherwise.\n",
    "    target_acceptance_rate:\n",
    "        The target acceptance rate for the step size adaptation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    init\n",
    "        Function that initializes the warmup.\n",
    "    update\n",
    "        Function that moves the warmup one step.\n",
    "    final\n",
    "        Function that returns the step size and mass matrix given a warmup\n",
    "        state.\n",
    "\n",
    "    \"\"\"\n",
    "    mm_init, mm_update, mm_final = mass_matrix_adaptation(is_mass_matrix_diagonal)\n",
    "    da_init, da_update, da_final = dual_averaging_adaptation(target_acceptance_rate)\n",
    "\n",
    "    def init(\n",
    "        position: ArrayLikeTree, initial_step_size: float\n",
    "    ) -> WindowAdaptationState:\n",
    "        \"\"\"Initialze the adaptation state and parameter values.\n",
    "\n",
    "        Unlike the original Stan window adaptation we do not use the\n",
    "        `find_reasonable_step_size` algorithm which we found to be unnecessary.\n",
    "        We may reconsider this choice in the future.\n",
    "\n",
    "        \"\"\"\n",
    "        num_dimensions = pytree_size(position)\n",
    "        imm_state = mm_init(num_dimensions)\n",
    "\n",
    "        ss_state = da_init(initial_step_size)\n",
    "\n",
    "        return WindowAdaptationState(\n",
    "            ss_state,\n",
    "            imm_state,\n",
    "            initial_step_size,\n",
    "            imm_state.inverse_mass_matrix,\n",
    "        )\n",
    "\n",
    "    def fast_update(\n",
    "        position: ArrayLikeTree,\n",
    "        acceptance_rate: float,\n",
    "        warmup_state: WindowAdaptationState,\n",
    "    ) -> WindowAdaptationState:\n",
    "        \"\"\"Update the adaptation state when in a \"fast\" window.\n",
    "\n",
    "        Only the step size is adapted in fast windows. \"Fast\" refers to the fact\n",
    "        that the optimization algorithms are relatively fast to converge\n",
    "        compared to the covariance estimation with Welford's algorithm\n",
    "\n",
    "        \"\"\"\n",
    "        del position\n",
    "\n",
    "        new_ss_state = da_update(warmup_state.ss_state, acceptance_rate)\n",
    "        new_step_size = jnp.exp(new_ss_state.log_step_size)\n",
    "\n",
    "        return WindowAdaptationState(\n",
    "            new_ss_state,\n",
    "            warmup_state.imm_state,\n",
    "            new_step_size,\n",
    "            warmup_state.inverse_mass_matrix,\n",
    "        )\n",
    "\n",
    "    def slow_update(\n",
    "        position: ArrayLikeTree,\n",
    "        acceptance_rate: float,\n",
    "        warmup_state: WindowAdaptationState,\n",
    "    ) -> WindowAdaptationState:\n",
    "        \"\"\"Update the adaptation state when in a \"slow\" window.\n",
    "\n",
    "        Both the mass matrix adaptation *state* and the step size state are\n",
    "        adapted in slow windows. The value of the step size is updated as well,\n",
    "        but the new value of the inverse mass matrix is only computed at the end\n",
    "        of the slow window. \"Slow\" refers to the fact that we need many samples\n",
    "        to get a reliable estimation of the covariance matrix used to update the\n",
    "        value of the mass matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        new_imm_state = mm_update(warmup_state.imm_state, position)\n",
    "        new_ss_state = da_update(warmup_state.ss_state, acceptance_rate)\n",
    "        new_step_size = jnp.exp(new_ss_state.log_step_size)\n",
    "\n",
    "        return WindowAdaptationState(\n",
    "            new_ss_state, new_imm_state, new_step_size, warmup_state.inverse_mass_matrix\n",
    "        )\n",
    "\n",
    "    def slow_final(warmup_state: WindowAdaptationState) -> WindowAdaptationState:\n",
    "        \"\"\"Update the parameters at the end of a slow adaptation window.\n",
    "\n",
    "        We compute the value of the mass matrix and reset the mass matrix\n",
    "        adapation's internal state since middle windows are \"memoryless\".\n",
    "\n",
    "        \"\"\"\n",
    "        new_imm_state = mm_final(warmup_state.imm_state)\n",
    "        new_ss_state = da_init(da_final(warmup_state.ss_state))\n",
    "        new_step_size = jnp.exp(new_ss_state.log_step_size)\n",
    "\n",
    "        return WindowAdaptationState(\n",
    "            new_ss_state,\n",
    "            new_imm_state,\n",
    "            new_step_size,\n",
    "            new_imm_state.inverse_mass_matrix,\n",
    "        )\n",
    "\n",
    "    def update(\n",
    "        adaptation_state: WindowAdaptationState,\n",
    "        adaptation_stage: tuple,\n",
    "        position: ArrayLikeTree,\n",
    "        acceptance_rate: float,\n",
    "    ) -> WindowAdaptationState:\n",
    "        \"\"\"Update the adaptation state and parameter values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adaptation_state\n",
    "            Current adptation state.\n",
    "        adaptation_stage\n",
    "            The current stage of the warmup: whether this is a slow window,\n",
    "            a fast window and if we are at the last step of a slow window.\n",
    "        position\n",
    "            Current value of the model parameters.\n",
    "        acceptance_rate\n",
    "            Value of the acceptance rate for the last mcmc step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The updated adaptation state.\n",
    "\n",
    "        \"\"\"\n",
    "        stage, is_middle_window_end = adaptation_stage\n",
    "\n",
    "        warmup_state = jax.lax.switch(\n",
    "            stage,\n",
    "            (fast_update, slow_update),\n",
    "            position,\n",
    "            acceptance_rate,\n",
    "            adaptation_state,\n",
    "        )\n",
    "\n",
    "        warmup_state = jax.lax.cond(\n",
    "            is_middle_window_end,\n",
    "            slow_final,\n",
    "            lambda x: x,\n",
    "            warmup_state,\n",
    "        )\n",
    "\n",
    "        return warmup_state\n",
    "\n",
    "    def final(warmup_state: WindowAdaptationState) -> tuple[float, Array]:\n",
    "        \"\"\"Return the final values for the step size and mass matrix.\"\"\"\n",
    "        step_size = jnp.exp(warmup_state.ss_state.log_step_size_avg)\n",
    "        inverse_mass_matrix = warmup_state.imm_state.inverse_mass_matrix\n",
    "        return step_size, inverse_mass_matrix\n",
    "\n",
    "    return init, update, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_schedule(\n",
    "    num_steps: int,\n",
    "    initial_buffer_size: int = 75,\n",
    "    final_buffer_size: int = 50,\n",
    "    first_window_size: int = 25,\n",
    ") -> list[tuple[int, bool]]:\n",
    "    \"\"\"Return the schedule for Stan's warmup.\n",
    "\n",
    "    The schedule below is intended to be as close as possible to Stan's :cite:p:`stan_hmc_param`.\n",
    "    The warmup period is split into three stages:\n",
    "\n",
    "    1. An initial fast interval to reach the typical set. Only the step size is\n",
    "    adapted in this window.\n",
    "    2. \"Slow\" parameters that require global information (typically covariance)\n",
    "    are estimated in a series of expanding intervals with no memory; the step\n",
    "    size is re-initialized at the end of each window. Each window is twice the\n",
    "    size of the preceding window.\n",
    "    3. A final fast interval during which the step size is adapted using the\n",
    "    computed mass matrix.\n",
    "\n",
    "    Schematically:\n",
    "\n",
    "    ```\n",
    "    +---------+---+------+------------+------------------------+------+\n",
    "    |  fast   | s | slow |   slow     |        slow            | fast |\n",
    "    +---------+---+------+------------+------------------------+------+\n",
    "    ```\n",
    "\n",
    "    The distinction slow/fast comes from the speed at which the algorithms\n",
    "    converge to a stable value; in the common case, estimation of covariance\n",
    "    requires more steps than dual averaging to give an accurate value. See :cite:p:`stan_hmc_param`\n",
    "    for a more detailed explanation.\n",
    "\n",
    "    Fast intervals are given the label 0 and slow intervals the label 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_steps: int\n",
    "        The number of warmup steps to perform.\n",
    "    initial_buffer: int\n",
    "        The width of the initial fast adaptation interval.\n",
    "    first_window_size: int\n",
    "        The width of the first slow adaptation interval.\n",
    "    final_buffer_size: int\n",
    "        The width of the final fast adaptation interval.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of tuples (window_label, is_middle_window_end).\n",
    "\n",
    "    \"\"\"\n",
    "    schedule = []\n",
    "\n",
    "    # Give up on mass matrix adaptation when the number of warmup steps is too small.\n",
    "    if num_steps < 20:\n",
    "        schedule += [(0, False)] * num_steps\n",
    "    else:\n",
    "        # When the number of warmup steps is smaller that the sum of the provided (or default)\n",
    "        # window sizes we need to resize the different windows.\n",
    "        if initial_buffer_size + first_window_size + final_buffer_size > num_steps:\n",
    "            initial_buffer_size = int(0.15 * num_steps)\n",
    "            final_buffer_size = int(0.1 * num_steps)\n",
    "            first_window_size = num_steps - initial_buffer_size - final_buffer_size\n",
    "\n",
    "        # First stage: adaptation of fast parameters\n",
    "        schedule += [(0, False)] * (initial_buffer_size - 1)\n",
    "        schedule.append((0, False))\n",
    "        print('First stage (total size): ',initial_buffer_size, len(schedule))\n",
    "        # Second stage: adaptation of slow parameters in successive windows\n",
    "        # doubling in size.\n",
    "        final_buffer_start = num_steps - final_buffer_size\n",
    "        print('2nd stage start : ', final_buffer_start)\n",
    "        next_window_size = first_window_size\n",
    "        next_window_start = initial_buffer_size\n",
    "        while next_window_start < final_buffer_start:\n",
    "            current_start, current_size = next_window_start, next_window_size\n",
    "            if 3 * current_size <= final_buffer_start - current_start:\n",
    "                next_window_size = 2 * current_size\n",
    "            else:\n",
    "                current_size = final_buffer_start - current_start\n",
    "            next_window_start = current_start + current_size\n",
    "            schedule += [(1, False)] * (next_window_start - 1 - current_start)\n",
    "            print((1, True))\n",
    "            schedule.append((1, True))\n",
    "\n",
    "        # Last stage: adaptation of fast parameters\n",
    "        schedule += [(0, False)] * (num_steps - 1 - final_buffer_start)\n",
    "        schedule.append((0, False))\n",
    "\n",
    "    schedule = jnp.array(schedule)\n",
    "\n",
    "    return schedule\n",
    "\n",
    "def window_adaptation(\n",
    "    algorithm,\n",
    "    logdensity_fn: Callable,\n",
    "    is_mass_matrix_diagonal: bool = True,\n",
    "    initial_step_size: float = 1.0,\n",
    "    target_acceptance_rate: float = 0.80,\n",
    "    progress_bar: bool = False,\n",
    "    adaptation_info_fn: Callable = return_all_adapt_info,\n",
    "    integrator=velocity_verlet,\n",
    "    **extra_parameters,\n",
    ") -> AdaptationAlgorithm:\n",
    "\n",
    "    mcmc_kernel = algorithm.build_kernel(integrator)\n",
    "\n",
    "    adapt_init, adapt_step, adapt_final = base(\n",
    "        is_mass_matrix_diagonal,\n",
    "        target_acceptance_rate=target_acceptance_rate,\n",
    "    )\n",
    "\n",
    "    def one_step(carry, xs):\n",
    "        _, rng_key, adaptation_stage = xs\n",
    "        state, adaptation_state = carry\n",
    "\n",
    "        new_state, info = mcmc_kernel(\n",
    "            rng_key,\n",
    "            state,\n",
    "            logdensity_fn,\n",
    "            adaptation_state.step_size,\n",
    "            adaptation_state.inverse_mass_matrix,\n",
    "            **extra_parameters,\n",
    "        )\n",
    "        new_adaptation_state = adapt_step(\n",
    "            adaptation_state,\n",
    "            adaptation_stage,\n",
    "            new_state.position,\n",
    "            info.acceptance_rate,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (new_state, new_adaptation_state),\n",
    "            adaptation_info_fn(new_state, info, new_adaptation_state),\n",
    "        )\n",
    "\n",
    "    def run(rng_key: PRNGKey, position: ArrayLikeTree, num_steps: int = 1000):\n",
    "        init_state = algorithm.init(position, logdensity_fn)\n",
    "        init_adaptation_state = adapt_init(position, initial_step_size)\n",
    "\n",
    "        if progress_bar:\n",
    "            print(\"Running window adaptation\")\n",
    "        scan_fn = gen_scan_fn(num_steps, progress_bar=progress_bar)\n",
    "        start_state = (init_state, init_adaptation_state)\n",
    "        keys = jax.random.split(rng_key, num_steps)\n",
    "        schedule = build_schedule(num_steps)\n",
    "        last_state, info = scan_fn(\n",
    "            one_step,\n",
    "            start_state,\n",
    "            (jnp.arange(num_steps), keys, schedule),\n",
    "        )\n",
    "\n",
    "        last_chain_state, last_warmup_state, *_ = last_state\n",
    "\n",
    "        step_size, inverse_mass_matrix = adapt_final(last_warmup_state)\n",
    "        parameters = {\n",
    "            \"step_size\": step_size,\n",
    "            \"inverse_mass_matrix\": inverse_mass_matrix,\n",
    "            **extra_parameters,\n",
    "        }\n",
    "\n",
    "        return (\n",
    "            AdaptationResults(\n",
    "                last_chain_state,\n",
    "                parameters,\n",
    "            ),\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    return AdaptationAlgorithm(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First stage (total size):  75 75\n",
      "2nd stage start :  150\n",
      "(1, True)\n",
      "(1, True)\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 200\n",
    "schedule = build_schedule(num_steps)\n",
    "print(schedule)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
